<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>clickhouse单机及集群快速部署</title>
    <url>/2020/11/10/clickhouse%E5%8D%95%E6%9C%BA%E5%8F%8A%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p><img src="/2020/11/10/clickhouse%E5%8D%95%E6%9C%BA%E5%8F%8A%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/clickhouse.png"></p>
<p>​    clickhouse列式存储DB由战斗民族俄罗斯程序员开发，由于其超高的查询性能，近来备受关注。</p>
<a id="more"></a>

<p>​    之前的大数据分析，例如 Hadoop 家族由很多技术和框架组合而成，犹如一头大象被拆分后其实所剩下的价值也就是 HDFS、Kafka、Spark ，其他的几乎都没有任何价值。</p>
<p><img src="/2020/11/10/clickhouse%E5%8D%95%E6%9C%BA%E5%8F%8A%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/2.jpg"></p>
<p>​    这些可以用 ClickHouse 一项技术代替。</p>
<p><img src="/2020/11/10/clickhouse%E5%8D%95%E6%9C%BA%E5%8F%8A%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/1.jpg"></p>
<p>​    下面是100M数据集的跑分结果：ClickHouse比Vertia快约5倍，比Hive快279倍，比My SQL 快801倍；虽然对不同的SQL查询，结果不完全一样，但是基本趋势是一致的。ClickHouse跑分有多快？举个例子：ClickHouse 1秒，Vertica 5.42秒，Hive 279秒</p>
<p><img src="/2020/11/10/clickhouse%E5%8D%95%E6%9C%BA%E5%8F%8A%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/3.jpg"></p>
<p>​    clickhouse架构：</p>
<p><a href="http://www.360doc.com/content/20/0719/08/22849536_925250448.shtml">相关资料1</a></p>
<p><a href="https://www.codercto.com/a/27963.html">相关资料2</a></p>
<p><img src="/2020/11/10/clickhouse%E5%8D%95%E6%9C%BA%E5%8F%8A%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/4.jpg"></p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><a href="https://clickhouse.tech/#quick-start">clickhouse官网</a></p>
<p>首先安装工具包和秘钥：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install yum-utils</span><br><span class="line">sudo rpm --import https://repo.clickhouse.tech/CLICKHOUSE-KEY.GPG</span><br></pre></td></tr></table></figure>

<p><em>针对RHEL/CentOS 用户，这里使用清华的镜像</em></p>
<p>新建 <code>/etc/yum.repos.d/clickhouse.repo</code>，内容为</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[repo.yandex.ru_clickhouse_rpm_stable_x86_64]</span><br><span class="line">name=clickhouse stable</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/clickhouse/rpm/stable/x86_64</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p>然后进入安装，保持网络畅通即可：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install clickhouse-server clickhouse-client</span><br></pre></td></tr></table></figure>

<p>出现如下，则安装完成</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">已安装:</span><br><span class="line">  clickhouse-client-20.10.3.30-2.noarch        clickhouse-common-static-20.10.3.30-2.x86_64        clickhouse-server-20.10.3.30-2.noarch       </span><br><span class="line"></span><br><span class="line">完毕！</span><br></pre></td></tr></table></figure>

<p>需要先开启clickhouse服务，然后启动clickhouse客户端，常用命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 关闭clickhouse服务</span></span><br><span class="line">systemctl stop clickhouse-server.service</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启clickhouse服务</span></span><br><span class="line">systemctl start clickhouse-server.service</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看clickhouse服务状态</span></span><br><span class="line">systemctl status clickhouse-server.service</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 登陆客户端</span></span><br><span class="line">clickhouse-client</span><br></pre></td></tr></table></figure>



<h2 id="单机部署"><a href="#单机部署" class="headerlink" title="单机部署"></a>单机部署</h2><p>直接启动clickhouse客户端即可。</p>
<p><a href="https://blog.csdn.net/renyiforever/article/details/89401448">常见错误</a></p>
<h2 id="集群部署"><a href="#集群部署" class="headerlink" title="集群部署"></a>集群部署</h2><p>在其他节点重复以上安装步骤。</p>
<p>新建编辑文件：<code>vim /etc/metrika.xml</code>，并分发至集群各个节点。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">yandex</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">clickhouse_remote_servers</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">perftest_3shards_1replicas</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop01<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop02<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop03<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">perftest_3shards_1replicas</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">clickhouse_remote_servers</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">zookeeper-servers</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">&quot;1&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop01<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">&quot;2&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop02<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">&quot;3&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop03<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">zookeeper-servers</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">macros</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">replica</span>&gt;</span>hadoop01<span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">macros</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">networks</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">ip</span>&gt;</span>::/0<span class="tag">&lt;/<span class="name">ip</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">networks</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">clickhouse_compression</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">case</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">min_part_size</span>&gt;</span>10000000000<span class="tag">&lt;/<span class="name">min_part_size</span>&gt;</span></span><br><span class="line">                                             </span><br><span class="line">  <span class="tag">&lt;<span class="name">min_part_size_ratio</span>&gt;</span>0.01<span class="tag">&lt;/<span class="name">min_part_size_ratio</span>&gt;</span>                                                                                                                                       </span><br><span class="line">  <span class="tag">&lt;<span class="name">method</span>&gt;</span>lz4<span class="tag">&lt;/<span class="name">method</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">case</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">clickhouse_compression</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">yandex</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>在各个节点重新启动clickhouse服务，然后开启客户端，并验证：<code>select * from system.clusters;</code></p>
<p>出现集群各个节点说明部署成功。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> system.clusters</span><br><span class="line"></span><br><span class="line">┌─cluster──────────────────────────────────────┬─shard_num─┬─shard_weight─┬─replica_num─┬─host_name─┬─host_address──┬─port─┬─is_local─┬─<span class="keyword">user</span>────┬─default_database─┬─errors_count─┬─estimated_recovery_time─┐</span><br><span class="line">│ perftest_3shards_1replicas                   │         <span class="number">1</span> │            <span class="number">1</span> │           <span class="number">1</span> │ hadoop01  │ <span class="number">192.168</span><span class="number">.150</span><span class="number">.4</span> │ <span class="number">9000</span> │        <span class="number">1</span> │ <span class="keyword">default</span> │                  │            <span class="number">0</span> │                       <span class="number">0</span> │</span><br><span class="line">│ perftest_3shards_1replicas                   │         <span class="number">2</span> │            <span class="number">1</span> │           <span class="number">1</span> │ hadoop02  │ <span class="number">192.168</span><span class="number">.150</span><span class="number">.5</span> │ <span class="number">9000</span> │        <span class="number">0</span> │ <span class="keyword">default</span> │                  │            <span class="number">0</span> │                       <span class="number">0</span> │</span><br><span class="line">│ perftest_3shards_1replicas                   │         <span class="number">3</span> │            <span class="number">1</span> │           <span class="number">1</span> │ hadoop03  │ <span class="number">192.168</span><span class="number">.150</span><span class="number">.6</span> │ <span class="number">9000</span> │        <span class="number">0</span> │ <span class="keyword">default</span> │</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>OLAP</category>
      </categories>
  </entry>
  <entry>
    <title>Git操作</title>
    <url>/2021/07/13/Git%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<p><img src="https://git-scm.com/images/logo@2x.png" alt="Git"></p>
<a id="more"></a>

<h4 id="git-工作原理"><a href="#git-工作原理" class="headerlink" title="git 工作原理"></a>git 工作原理</h4><p><img src="https://img0.baidu.com/it/u=613996794,3695167693&fm=26&fmt=auto&gp=0.jpg" alt="img"></p>
<h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><p>Cloning from a remote repository to a local repository</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone http://git.skytech.cn/Digger/dolphinscheduler.git</span><br><span class="line"></span><br><span class="line">git init</span><br></pre></td></tr></table></figure>



<h4 id="分支操作"><a href="#分支操作" class="headerlink" title="分支操作"></a>分支操作</h4><p>创建查看并切换分支</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git branch test</span><br><span class="line"></span><br><span class="line">git branch -v</span><br><span class="line"></span><br><span class="line">git checkout test</span><br></pre></td></tr></table></figure>

<p>切换版本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git reset --hard 版本号</span><br></pre></td></tr></table></figure>

<p>拉取合并分支到当前分支=拉取分支并合并</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git fetch origin master</span><br><span class="line">+</span><br><span class="line">git merge dolphinscheduler</span><br><span class="line">||</span><br><span class="line">git pull origin dolphinscheduler:dolphinscheduler</span><br></pre></td></tr></table></figure>

<p>切换分支版本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git reset --hard 版本号</span><br></pre></td></tr></table></figure>



<h4 id="提交"><a href="#提交" class="headerlink" title="提交"></a>提交</h4><p>添加到暂存区-&gt;提交到 local repository-&gt;选择远程库并提交</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git add 文件名</span><br><span class="line"></span><br><span class="line">git commit -am &quot;日志信息&quot; 文件名</span><br><span class="line"></span><br><span class="line">git push 别名 分支</span><br></pre></td></tr></table></figure>

<p>查看远程库</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git remote -v</span><br></pre></td></tr></table></figure>

<p>添加远程库</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git remote add 别名 远程地址</span><br></pre></td></tr></table></figure>

<p>强制push</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git push 别名 分支 -f</span><br></pre></td></tr></table></figure>



<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><p>多人项目，拉取代码后同事又修改代码上传，先拉后推</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git pull origin dolphinscheduler:dolphinscheduler</span><br><span class="line"></span><br><span class="line">git add ./*</span><br><span class="line"></span><br><span class="line">git push origin dolphinscheduler</span><br></pre></td></tr></table></figure>



<h4 id="idea中操作git"><a href="#idea中操作git" class="headerlink" title="idea中操作git"></a>idea中操作git</h4><p><img src="/2021/07/13/Git%E6%93%8D%E4%BD%9C/git.jpg"></p>
]]></content>
      <categories>
        <category>git</category>
      </categories>
  </entry>
  <entry>
    <title>clickhouse作为实时和离线计算引擎从kafka消费数据</title>
    <url>/2020/12/04/clickhouse%E4%BD%9C%E4%B8%BA%E5%AE%9E%E6%97%B6%E5%92%8C%E7%A6%BB%E7%BA%BF%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E%E4%BB%8Ekafka%E6%B6%88%E8%B4%B9%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<p><img src="https://clickhouse.tech/images/index/intro.svg" alt="ClickHouse is capable of generating analytical data reports in real time, with sub-second latencies">    </p>
<p>​    由于clickhouse超高的查询性能，和使用sql开发的低成本，我们不禁想到让clickhouse作为计算引擎和存储介质直接分析存储在其中的海量数据，并配合其MergeTree表引擎特有的TTL功能做实时数据分析。</p>
<a id="more"></a>



<blockquote>
<p>本文使用到的表引擎：</p>
<p>kafka</p>
<p>Distributed</p>
<p>ReplicatedMergeTree</p>
</blockquote>
<p>使用kafka-eagle创建topic：</p>
<p><img src="/2020/12/04/clickhouse%E4%BD%9C%E4%B8%BA%E5%AE%9E%E6%97%B6%E5%92%8C%E7%A6%BB%E7%BA%BF%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E%E4%BB%8Ekafka%E6%B6%88%E8%B4%B9%E6%95%B0%E6%8D%AE/kafka-eagle.jpg"></p>
<p>在控制台创建kafka消费者并指定消费者组与kafka表中的<code>kafka_group_name</code>保持一致：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --topic clickhouse_test01 --bootstrap-server hadoop01:9092,hadoop02:9092,hadoop03:9092 --group clickhouse_group</span><br></pre></td></tr></table></figure>



<p>clickhouse集群副本及其分片配置如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">┌─cluster──────────────────────────────────────┬─shard_num─┬─shard_weight─┬─replica_num─┬─host_name─┬─host_address──┬─port─┬─is_local─┬─user────┬─default_database─┬─errors_count─┬─estimated_recovery_time─┐</span><br><span class="line">│ gmall_cluster                                │         1 │            1 │           1 │ hadoop01  │ 192.168.150.4 │ 9000 │        0 │ default │                  │            0 │                       0 │</span><br><span class="line">│ gmall_cluster                                │         1 │            1 │           2 │ hadoop02  │ 192.168.150.5 │ 9000 │        1 │ default │                  │            0 │                       0 │</span><br><span class="line">│ gmall_cluster                                │         2 │            1 │           1 │ hadoop03  │ 192.168.150.6 │ 9000 │        0 │ default │                  │            0 │                       0 │</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>首先使用kafka表引擎创建集群表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> default.kafka_src_table <span class="keyword">ON</span> CLUSTER gmall_cluster</span><br><span class="line">(</span><br><span class="line">    <span class="string">`id`</span> Int32,</span><br><span class="line">    <span class="string">`age`</span> Int32,</span><br><span class="line">    <span class="string">`msg`</span> <span class="keyword">String</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ENGINE</span> = Kafka()</span><br><span class="line"><span class="keyword">SETTINGS</span> kafka_broker_list = <span class="string">&#x27;hadoop01:9092,hadoop02:9092,hadoop03:9092&#x27;</span>, kafka_topic_list = <span class="string">&#x27;clickhouse_test01&#x27;</span>, kafka_group_name = <span class="string">&#x27;clickhouse_group&#x27;</span>, kafka_format = <span class="string">&#x27;JSONEachRow&#x27;</span></span><br><span class="line"></span><br><span class="line">┌─host─────┬─port─┬─<span class="keyword">status</span>─┬─<span class="keyword">error</span>─┬─num_hosts_remaining─┬─num_hosts_active─┐</span><br><span class="line">│ hadoop02 │ <span class="number">9000</span> │      <span class="number">0</span> │       │                   <span class="number">2</span> │                <span class="number">0</span> │</span><br><span class="line">│ hadoop03 │ <span class="number">9000</span> │      <span class="number">0</span> │       │                   <span class="number">1</span> │                <span class="number">0</span> │</span><br><span class="line">│ hadoop01 │ <span class="number">9000</span> │      <span class="number">0</span> │       │                   <span class="number">0</span> │                <span class="number">0</span> │</span><br><span class="line">└──────────┴──────┴────────┴───────┴─────────────────────┴──────────────────┘</span><br><span class="line"></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> <span class="keyword">in</span> set. Elapsed: <span class="number">0.130</span> sec. </span><br></pre></td></tr></table></figure>



<p>使用ReplacingMergeTree 弱幂等性表引擎创建集群分片表:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE default.kafka_table_local ON CLUSTER gmall_cluster</span><br><span class="line">(</span><br><span class="line">    &#96;id&#96; Int32,</span><br><span class="line">    &#96;age&#96; UInt32,</span><br><span class="line">    &#96;msg&#96; String</span><br><span class="line">)</span><br><span class="line">ENGINE &#x3D; ReplicatedMergeTree(&#39;&#x2F;clickhouse&#x2F;tables&#x2F;kafka_sink_table&#x2F;&#123;shard&#125;&#39;, &#39;&#123;replica&#125;&#39;)</span><br><span class="line">ORDER BY id</span><br><span class="line"></span><br><span class="line">┌─host─────┬─port─┬─status─┬─error─┬─num_hosts_remaining─┬─num_hosts_active─┐</span><br><span class="line">│ hadoop02 │ 9000 │      0 │       │                   2 │                0 │</span><br><span class="line">│ hadoop03 │ 9000 │      0 │       │                   1 │                0 │</span><br><span class="line">│ hadoop01 │ 9000 │      0 │       │                   0 │                0 │</span><br><span class="line">└──────────┴──────┴────────┴───────┴─────────────────────┴──────────────────┘</span><br><span class="line"></span><br><span class="line">3 rows in set. Elapsed: 0.294 sec. </span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>使用distributed引擎创建分布式集群表：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE kafka_table_distributed ON CLUSTER gmall_cluster</span><br><span class="line">(</span><br><span class="line">    &#96;id&#96; Int32,</span><br><span class="line">    &#96;age&#96; UInt32,</span><br><span class="line">    &#96;msg&#96; String</span><br><span class="line">)</span><br><span class="line">ENGINE &#x3D; Distributed(gmall_cluster, default, kafka_table_local, id)</span><br><span class="line"></span><br><span class="line">┌─host─────┬─port─┬─status─┬─error─┬─num_hosts_remaining─┬─num_hosts_active─┐</span><br><span class="line">│ hadoop02 │ 9000 │      0 │       │                   2 │                0 │</span><br><span class="line">│ hadoop03 │ 9000 │      0 │       │                   1 │                0 │</span><br><span class="line">│ hadoop01 │ 9000 │      0 │       │                   0 │                0 │</span><br><span class="line">└──────────┴──────┴────────┴───────┴─────────────────────┴──────────────────┘</span><br><span class="line"></span><br><span class="line">3 rows in set. Elapsed: 0.137 sec. </span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>创建视图从kafka表转储数据到集群表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">MATERIALIZED</span> <span class="keyword">VIEW</span> consumer <span class="keyword">TO</span> kafka_table_distributed <span class="keyword">AS</span> <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> kafka_src_table;</span><br></pre></td></tr></table></figure>



<p>然后开启kafka生产者生产数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --topic clickhouse_test01 --broker-list hadoop01:9092,hadoop02:9092,hadoop03:9092</span><br></pre></td></tr></table></figure>



<p>因为上面指定的kafka表中消费数据格式为json，故这里生产json格式的测试数据：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">3</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">22</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;su&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">1</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">20</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;sunqi&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">2</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">21</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;a&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">3</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">22</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;su&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">4</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">22</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;suqi&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">5</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">203</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;nqi&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">5</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">203</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;nqi&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">5</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">203</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;nqi&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">5</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">203</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;nqi&quot;</span>&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>查询集群表中：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> kafka_table_distributed</span><br><span class="line"></span><br><span class="line">┌─<span class="keyword">id</span>─┬─age─┬─msg──┐</span><br><span class="line">│  <span class="number">2</span> │  <span class="number">21</span> │ a    │</span><br><span class="line">│  <span class="number">4</span> │  <span class="number">22</span> │ suqi │</span><br><span class="line">└────┴─────┴──────┘</span><br><span class="line">┌─<span class="keyword">id</span>─┬─age─┬─msg─┐</span><br><span class="line">│  <span class="number">5</span> │ <span class="number">203</span> │ nqi │</span><br><span class="line">└────┴─────┴─────┘</span><br><span class="line">┌─<span class="keyword">id</span>─┬─age─┬─msg───┐</span><br><span class="line">│  <span class="number">1</span> │  <span class="number">20</span> │ sunqi │</span><br><span class="line">│  <span class="number">3</span> │  <span class="number">22</span> │ su    │</span><br><span class="line">└────┴─────┴───────┘</span><br><span class="line"></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> <span class="keyword">in</span> set. Elapsed: <span class="number">0.012</span> sec. </span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>查询副本1表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> kafka_table_local</span><br><span class="line"></span><br><span class="line">┌─<span class="keyword">id</span>─┬─age─┬─msg──┐</span><br><span class="line">│  <span class="number">2</span> │  <span class="number">21</span> │ a    │</span><br><span class="line">│  <span class="number">4</span> │  <span class="number">22</span> │ suqi │</span><br><span class="line">└────┴─────┴──────┘</span><br><span class="line"></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> <span class="keyword">in</span> set. Elapsed: <span class="number">0.002</span> sec. </span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>OLAP</category>
      </categories>
  </entry>
  <entry>
    <title>elasticsearch原理及常见面试点</title>
    <url>/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/</url>
    <content><![CDATA[<p><img src="https://images.contentstack.io/v3/assets/bltefdd0b53724fa2ce/blt280217a63b82a734/5bbdaacf63ed239936a7dd56/elastic-logo.svg" alt="Elastic logo"></p>
<a id="more"></a>

<h4 id="elasticsearch是基于Lucene-倒排索引结构的一款OLAP数据分析引擎，特点是快速，并且支持模糊查询。"><a href="#elasticsearch是基于Lucene-倒排索引结构的一款OLAP数据分析引擎，特点是快速，并且支持模糊查询。" class="headerlink" title="elasticsearch是基于Lucene 倒排索引结构的一款OLAP数据分析引擎，特点是快速，并且支持模糊查询。"></a>elasticsearch是基于Lucene 倒排索引结构的一款OLAP数据分析引擎，特点是快速，并且支持模糊查询。</h4><p>模糊查询基于倒排索引：</p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/15.jpg"></p>
<p>倒排索引原理：</p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/16.jpg"></p>
<h4 id="图解-ElasticSearch-原理"><a href="#图解-ElasticSearch-原理" class="headerlink" title="图解 ElasticSearch 原理"></a>图解 ElasticSearch 原理</h4><p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/0.jpg"></p>
<ul>
<li>集群cluster中包含很多nodes</li>
<li>一个或多个node中的shard组成index索引=lucene index</li>
<li>lucene index由许多的segments组成</li>
<li>segment内部结构由多个部分组成，最最重要的 <strong>Inverted Index</strong></li>
<li>增加文件可能会使索引所占空间变小，它会引起 Merge，从而可能会有更多的压缩。</li>
</ul>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/1.png"></p>
<p>Inverted Index 主要包括两部分：</p>
<ul>
<li>一个有序的数据字典 Dictionary（包括单词 Term 和它出现的频率）</li>
<li>与单词 Term 对应的 Postings（即存在这个单词的文件）</li>
</ul>
<p>当我们搜索的时候，首先将搜索的内容分解，然后在字典里找到对应 Term，从而查找到与搜索相关的文件内容。</p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/2.jpg"></p>
<p>在 Shard 中搜索</p>
<p>ElasticSearch 从 Shard 中搜索的过程与 Lucene Segment 中搜索的过程类似。</p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/1.jpg"></p>
<p>需要注意的是：1 次搜索查找 2 个 Shard＝2 次分别搜索 Shard</p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/3.jpg"></p>
<p>Shard 不会进行更进一步的拆分，但是 Shard 可能会被转移到不同节点上。</p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/4.jpg"></p>
<p>如果当集群节点压力增长到一定的程度，我们可能会考虑增加新的节点，这就会要求我们对所有数据进行重新索</p>
<p>引，这是我们不太希望看到的。所以我们需要在规划的时候就考虑清楚，如何去平衡足够多的节点与不足节点之间</p>
<p>的关系。</p>
<hr>
<p><strong>一个真实的请求</strong></p>
<p>请求分发：这个请求可能被分发到集群里的任意一个节点，如下图：<br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/5.jpg"><br>上帝节点：</p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/6.png"><br>这时这个节点就成为当前请求的协调者（Coordinator），它决定：<br>根据索引信息，判断请求会被路由到哪个核心节点。<br>以及哪个副本是可用的。<br>等等。</p>
<p>路由：<br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/7.png"><br>在真实搜索之前ElasticSearch 会将 Query 转换成 Lucene Query，如下图：<br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/8.jpg"><br>然后在所有的 Segment 中执行计算，如下图：<br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/9.jpg"><br>对于 Filter 条件本身也会有缓存，如下图：<br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/10.jpg"><br>但 Queries 不会被缓存，所以如果相同的 Query 重复执行，应用程序自己需要做缓存。<br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/17.jpg"><br>所以Filters 可以在任何时候使用。Query 只有在需要 Score 的时候才使用。</p>
<p>搜索结束之后，结果会沿着下行的路径向上逐层返回，如下图：</p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/18.jpg"></p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/11.jpg"><br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/12.jpg"><br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/13.jpg"><br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/14.jpg"></p>
<h3 id="常见面试问题："><a href="#常见面试问题：" class="headerlink" title="常见面试问题："></a>常见面试问题：</h3><p>kafka-flink-es过程中，flink自定义sink可以使用基于哪两种方式发送？</p>
<p>1、http方式</p>
<p>2、jdbc方式</p>
<hr>
<p>字段全值匹配检索如果在关系型数据库Mysql中查询多字段匹配数据（字段检索）一般会执行下面的SQL语句：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> atguigu <span class="keyword">where</span> <span class="keyword">name</span> = <span class="string">&#x27;haha&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>但在Elasticsearch中需要采用特殊的方式：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET atguigu/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;bool&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;filter&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;term&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;about&quot;</span>: <span class="string">&quot;I love to go rock climbing&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>字段分词匹配检索</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET atguigu/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;match&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;about&quot;</span>: <span class="string">&quot;I&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>字段模糊匹配检索如果在关系型数据库Mysql中模糊查询多字段数据一般会执行下面的SQL语句：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> <span class="keyword">name</span> <span class="keyword">like</span> <span class="string">&#x27;%haha%&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>但在Elasticsearch中需要采用特殊的方式，查询出所有文档字段值分词后包含haha的文档：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET  test/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;fuzzy&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;aa&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;value&quot;</span>: <span class="string">&quot;我是程序&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>OLAP</category>
      </categories>
  </entry>
  <entry>
    <title>dolphinscheduler任务调度指南</title>
    <url>/2021/04/08/dolphinscheduler%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%8C%87%E5%8D%97/</url>
    <content><![CDATA[<p><img src="https://dolphinscheduler.apache.org/img/hlogo_white.svg" alt="img"></p>
<a id="more"></a>

<p>GitHub地址：<a href="https://github.com/apache/incubator-dolphinscheduler">https://github.com/apache/incubator-dolphinscheduler</a></p>
<p>dolphinscheduler的集群环境配置有几个要点：</p>
<p>1、元数据库配置及初始化</p>
<p>2、运行环境的配置</p>
<p>3、集群安装参数、上传文件路径及告警配置</p>
<p>其中环境的配置直接影响后面的任务调度。以阿里云E-Mapreduce为例，集群的环境变量及常见类路径部署在/etc/profile.d/*.sh中，如果在任务运行过程中出现命令丢失和jar包缺失，可以去$PATH中去找环境变量。</p>
<p><img src="/2021/04/08/dolphinscheduler%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%8C%87%E5%8D%97/0.jpg"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_HOME=/usr/lib/hadoop-current</span><br><span class="line">export HADOOP_CONF_DIR=/etc/ecm/hadoop-conf</span><br><span class="line">export SPARK_HOME=/usr/lib/spark-current</span><br><span class="line">export HADOOP_CLASSPATH=/usr/lib/hadoop-current/lib/*:/usr/lib/tez-current/*:/usr/lib/tez-current/lib/*:/etc/ecm/tez-conf:/opt/apps/extra-jars/*:/usr/lib/spark-current/yarn/spark-2.4.5-yarn-shuffle.jar</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> SPARK_HOME2=/opt/soft/spark2</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> PYTHON_HOME=/opt/soft/python</span></span><br><span class="line">export JAVA_HOME=/usr/lib/jvm/java-1.8.0</span><br><span class="line">export HIVE_HOME=/usr/lib/hive-current</span><br><span class="line">export FLINK_HOME=/usr/lib/flink-current</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> DATAX_HOME=/opt/soft/datax/bin/datax.py</span></span><br><span class="line">export SQOOP_HOME=/usr/lib/sqoop-current</span><br><span class="line"></span><br><span class="line">export HIVE_CONF_DIR=/etc/ecm/hive-conf</span><br><span class="line">export HCAT_HOME=/usr/lib/hive-current/hcatalog</span><br><span class="line"></span><br><span class="line">export PATH=$SQOOP_HOME/bin:$HADOOP_HOME/bin:$HCAT_HOME/bin:$SPARK_HOME/bin:$JAVA_HOME/bin:$HIVE_HOME/bin:$PATH:$FLINK_HOME/bin:$DATAX_HOME:$PATH</span><br></pre></td></tr></table></figure>

<p>如果这3点中有一点没有做好，将会影响dolphinscheduler的正常工作和任务调度。</p>
<p>在部署好集群后，会在master和worker上共启动5种不同的进程：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MasterServer         ----- master服务    </span><br><span class="line">WorkerServer         ----- worker服务    </span><br><span class="line">LoggerServer         ----- logger服务    </span><br><span class="line">ApiApplicationServer ----- api服务    </span><br><span class="line">AlertServer          ----- alert服务</span><br></pre></td></tr></table></figure>

<p>进入web后首先要进行一些初始化工作，例如租户，用户的创建，数据源的配置等，然后我们可以进行任务调度：</p>
<p><img src="/2021/04/08/dolphinscheduler%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%8C%87%E5%8D%97/1.jpg"></p>
<p>工作台有几种常用的任务例如flink，spark、sql、shell脚本都可以通过托拉拽的方式确定任务调度流程。</p>
<p>在完成任务流程设计后，可以通过定时调度，失败邮箱报警和企业微信报警来题型开发者任务过程出现的问题并及时解决。</p>
<p>在进行任务调度过程中，可能会出现一些程序本身设计问题而导致的bug，</p>
<p>出现任务调度异常，或者任务链卡死，首先去logs下跟踪master和worker的日志，一般出错都会在日志中找出原因，常见的bug如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[ERROR] <span class="number">2021</span>-<span class="number">04</span>-<span class="number">08</span> <span class="number">14</span>:<span class="number">58</span>:<span class="number">51.045</span> org.apache.dolphinscheduler.server.master.processor.queue.TaskResponseService:[<span class="number">169</span>] - worker response master error</span><br><span class="line">org.springframework.dao.DataIntegrityViolationException: </span><br><span class="line">### Error updating database.  Cause: com.mysql.jdbc.MysqlDataTruncation: Data truncation: Data too long for column &#x27;app_link&#x27; at row 1</span><br><span class="line">### The error may exist in org/apache/dolphinscheduler/dao/mapper/TaskInstanceMapper.java (best guess)</span><br><span class="line">### The error may involve org.apache.dolphinscheduler.dao.mapper.TaskInstanceMapper.updateById-Inline</span><br><span class="line">### The error occurred while setting parameters</span><br><span class="line">### SQL: UPDATE t_ds_task_instance  SET flag=?, task_json=?, pid=?, app_link=?, task_type=?, task_instance_priority=?, log_path=?, host=?, start_time=?, worker_group=?, state=?, process_instance_id=?, process_definition_id=?, executor_id=?, alert_flag=?, execute_path=?, max_retry_times=?, retry_times=?, submit_time=?, name=?, retry_interval=?, end_time=?  WHERE id=?</span><br><span class="line">### Cause: com.mysql.jdbc.MysqlDataTruncation: Data truncation: Data too long for column &#x27;app_link&#x27; at row 1</span><br><span class="line">; Data truncation: Data too <span class="keyword">long</span> <span class="keyword">for</span> column <span class="string">&#x27;app_link&#x27;</span> at row <span class="number">1</span>; nested exception is com.mysql.jdbc.MysqlDataTruncation: Data truncation: Data too <span class="keyword">long</span> <span class="keyword">for</span> column <span class="string">&#x27;app_link&#x27;</span> at row <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>这种问题的原因在于dolphinscheduler元数据设计时的局限性，应该将t_ds_task_instance表中的app_link字段修改成text类型，即可解决问题。</p>
<p>dolphinscheduler是一个apache正在孵化中的开源项目，社区建设还不是很成熟，开源社区有GitHub，slack，微信公众号交流群：</p>
<p><a href="https://app.slack.com/client/T01L3LB96V7/C01L3LB9YMP/thread/C01L3LB9YMP-1617861424.034300">https://app.slack.com/client/T01L3LB96V7/C01L3LB9YMP/thread/C01L3LB9YMP-1617861424.034300</a></p>
<p><a href="https://github.com/apache/incubator-dolphinscheduler/issues">https://github.com/apache/incubator-dolphinscheduler/issues</a></p>
<p>微信公众号：dolphin-scheduler</p>
<p>也可以通过我（微信：ycfnsq97）邀请大家加入apache dolphinscheduler社区微信交流群。</p>
]]></content>
  </entry>
  <entry>
    <title>flink checkpoint机制及非barrier对齐</title>
    <url>/2021/02/16/flink%20checkpoint%E6%9C%BA%E5%88%B6%E5%8F%8A%E9%9D%9Ebarrier%E5%AF%B9%E9%BD%90/</url>
    <content><![CDATA[<p><img src="/2021/02/16/flink%20checkpoint%E6%9C%BA%E5%88%B6%E5%8F%8A%E9%9D%9Ebarrier%E5%AF%B9%E9%BD%90/0.jpg"></p>
<p>注：本文部分内容为转载总结</p>
<a id="more"></a>

<p>在flink的世界观里，一切事物都可以视为数据流中的一个个珠子，在算子间不断的流动着，之前的watermark就可以看做数据流中的一个数据珠子，同样checkpoint也不例外。</p>
<p>在多个并行度的数据流经算子内联的barrier(检查点分界线)时，会将自己的位置保存进状态后端(state backend)，生产中一般将状态保存进hdfs，rocksdb中，如果数据源是kafka，那么稳定存储的就是kafka的offset，如下图所示，checkpoint流经keyby算子后保存了当前的位置，后面经过map算子时，保存了checkpoint流过算子时的数据状态，同样作为稳定存储进状态后端，当任务出现异常时，就可以从状态后端中将数据流还原到未出错前的样子：</p>
<p><img src="/2021/02/16/flink%20checkpoint%E6%9C%BA%E5%88%B6%E5%8F%8A%E9%9D%9Ebarrier%E5%AF%B9%E9%BD%90/1.jpg"></p>
<p>但是，屏障对齐是阻塞式的，在作业出现反压时可能会成为不定时炸弹。我们知道，检查点屏障是从Source端产生并源源不断地向下游流动的。如果作业出现反压（哪怕整个DAG中的一条链路反压），数据流动的速度减慢，屏障到达下游算子的延迟就会变大，进而影响到检查点完成的延时（变大甚至超时失败）。如果反压长久不能得到解决，快照数据与实际数据之间的差距就越来越明显，一旦作业failover，势必丢失较多的处理进度。另一方面，作业恢复后需要重新处理的数据又会积压，加重反压，造成恶性循环。</p>
<p>为了规避风险，Flink 1.11引入了非对齐检查点（unaligned checkpoint）的feature。</p>
<p>非对齐检查点取消了屏障对齐操作：</p>
<ol>
<li>当算子的所有输入流中的第一个屏障到达算子的输入缓冲区时，立即将这个屏障发往下游（输出缓冲区）。</li>
<li>由于第一个屏障没有被阻塞，它的步调会比较快，超过一部分缓冲区中的数据。算子会标记两部分数据：一是屏障首先到达的那条流中被超过的数据，二是其他流中位于当前检查点屏障之前的所有数据（当然也包括进入了输入缓冲区的数据），如下图中标黄的部分所示。</li>
<li>将上述两部分数据连同算子的状态一起做异步快照。</li>
</ol>
<p><img src="/2021/02/16/flink%20checkpoint%E6%9C%BA%E5%88%B6%E5%8F%8A%E9%9D%9Ebarrier%E5%AF%B9%E9%BD%90/2.png"></p>
<p>不同检查点的数据都混在一起，非对齐检查点还是能保证exactly once。当任务从非对齐检查点恢复时，除了对齐检查点也会涉及到的Source端重放和算子的计算状态恢复之外，未对齐的流数据也会被恢复到各个链路，三者合并起来就是能够保证exactly once的完整现场了。</p>
<p><img src="/2021/02/16/flink%20checkpoint%E6%9C%BA%E5%88%B6%E5%8F%8A%E9%9D%9Ebarrier%E5%AF%B9%E9%BD%90/3.png"></p>
<p>非对齐检查点主要缺点有二：</p>
<ul>
<li><p>需要额外保存数据流的现场，总的状态大小可能会有比较明显的膨胀（文档中说可能会达到a couple of GB per task），磁盘压力大。当集群本身就具有I/O bound的特点时，该缺点的影响更明显。</p>
</li>
<li><p>从状态恢复时也需要额外恢复数据流的现场，作业重新拉起的耗时可能会很长。特别地，如果第一次恢复失败，有可能触发death spiral（死亡螺旋）使得作业永远无法恢复。</p>
</li>
</ul>
<p>官方当前推荐仅将它应用于那些容易产生反压且I/O压力较小（比如原始状态不太大）的作业中。随着后续版本的打磨，非对齐检查点肯定会更加好用。</p>
]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title>flink watermark传递机制</title>
    <url>/2021/02/15/flink%20watermark%E4%BC%A0%E9%80%92%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.12/fig/parallel_kafka_watermarks.svg" alt="Generating Watermarks with awareness for Kafka-partitions"></p>
<a id="more"></a>

<p>watermark是衡量eventtime进展机制的时间，经常用来处理乱序数据，常常结合window使用</p>
<p>生产环境一般使用周期性生成watermark，系统默认的周期是200ms，可以在代码中自定义：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.getConfig.setAutoWatermarkInterval(<span class="number">5000</span>);</span><br></pre></td></tr></table></figure>



<p>对于有eventtime的数据流，可以分为两大类：时间单调递增，乱序时间数据流，一般在生产环境中不能保证数据的准确性，所以使用BoundedOutOfOrdernessTimestampExtractor允许2s内的数据迟到：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;String&gt; input=env.socketTextStream(<span class="string">&quot;hadoop01&quot;</span>,<span class="number">7777</span>).assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;String&gt;(Time.seconds(<span class="number">2</span>)) &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(String element)</span> </span>&#123;</span><br><span class="line">                String[] fields = element.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> Long.parseLong(fields[<span class="number">1</span>]) * <span class="number">1000L</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure>



<p>在这之后要对数据进行开窗口计算，对于窗口时间和2s允许迟到的时间，以及allowedLateness内的时间也就是加起来4s的时间，会在主流输出。</p>
<p>对于超出允许迟到4s的数据，则会被送往侧输出流sideOutputLateData每来一条就进行侧输出。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">WindowedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple, TimeWindow&gt; windowedStream = keyedStream</span><br><span class="line">                .timeWindow(Time.seconds(<span class="number">5</span>))</span><br><span class="line">                .allowedLateness(Time.seconds(<span class="number">2</span>))</span><br><span class="line">                .sideOutputLateData(<span class="keyword">new</span> OutputTag&lt;Tuple2&lt;String, Integer&gt;&gt;(<span class="string">&quot;sideOutPut&quot;</span>) &#123;</span><br><span class="line">                &#125;);</span><br></pre></td></tr></table></figure>

<p><img src="/2021/02/15/flink%20watermark%E4%BC%A0%E9%80%92%E6%9C%BA%E5%88%B6/4.jpg"></p>
<p>在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/event_timestamps_watermarks.html#%E5%8F%AF%E4%BB%A5%E5%BC%83%E7%94%A8-assignerwithperiodicwatermarks-%E5%92%8C-assignerwithpunctuatedwatermarks-%E4%BA%86">新发布的flink 1.12</a>中，官方对生成watermark的api进行了升级，旧版本的api预计在不久的未来会被淘汰，但是原理是通用的。</p>
<p><img src="/2021/02/15/flink%20watermark%E4%BC%A0%E9%80%92%E6%9C%BA%E5%88%B6/1.jpg"></p>
<p>在多并行度时，算子向下游发送watermark是同时发送的，且同一个算子如果有多个watermark取最小。在生成watermark之前则是轮询发送。</p>
<p>下图是在map算子之前生成watermark：</p>
<p><img src="/2021/02/15/flink%20watermark%E4%BC%A0%E9%80%92%E6%9C%BA%E5%88%B6/2.jpg"></p>
<p>在map算子之后生成watermark：</p>
<p><img src="/2021/02/15/flink%20watermark%E4%BC%A0%E9%80%92%E6%9C%BA%E5%88%B6/3.jpg"></p>
]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title>flink yarn模式下的 HA 和 故障重启</title>
    <url>/2020/12/07/flink-yarn%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84-HA-%E5%92%8C-%E6%95%85%E9%9A%9C%E9%87%8D%E5%90%AF/</url>
    <content><![CDATA[<p><img src="https://flink.apache.org/img/flink-header-logo.svg" alt="Apache Flink"></p>
<p>​    </p>
<a id="more"></a>

<h1 id="HA"><a href="#HA" class="headerlink" title="HA"></a>HA</h1><p>​        JobManager协调每个Flink部署。它负责调度和资源管理。默认情况下，每个Flink集群只有一个JobManager实例。这会创建一个单点故障(SPOF):如果JobManager崩溃，就不能提交新的程序，运行中的程序也会失败。使用JobManager高可用性，您可以从JobManager故障中恢复，从而消除SPOF。您可以为独立集群和纱线集群配置高可用性。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/jobmanager_ha_overview.png" alt="img"></p>
<p>​    </p>
<p>​    由于生产环境下通常使用yarn模式，所以这里介绍yarn模式下的高可用配置，并测试杀死jobmanager进程后重启的过程。通常高可用需要配置zookeeper参数，所以这里分别对<code>yarn-site.xml</code>和<code>flink-conf.yaml</code>进行配置：</p>
<p>yarn-site.xml增加jobmanageer失败后最大的重试次数</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.am.max-attempts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    The maximum number of application master execution attempts.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p>flink-conf.yaml中设置zookeeper参数：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">high-availability:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.quorum:</span> <span class="string">hadoop01:2181,hadoop02:2181,hadoop03:2181</span></span><br><span class="line"><span class="attr">high-availability.storageDir:</span> <span class="string">hdfs:///flink/ha</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.path.root:</span> <span class="string">/flink</span></span><br><span class="line"><span class="attr">yarn.application-attempts:</span> <span class="number">10</span></span><br></pre></td></tr></table></figure>



<blockquote>
<p>注意<code>yarn.resourcemanager.am.max-attempts</code>应用程序重新启动的上限。因此，Flink中设置的应用程序尝试次数不能超过启动YARN的YARN群集设置。(yarn.resourcemanager.am.max-attemps)。</p>
<p>yarn.application-attempts  &lt;=  yarn.resourcemanager.am.max-attempts</p>
</blockquote>
<p>配置成功并分发至各节点，然后分别启动zookeeper，yarn，并开启session cluster：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/yarn-session.sh -n 2 -s 2 -jm 1024 -tm 1024 -nm test -d</span><br></pre></td></tr></table></figure>

<p>​    查看控制台：</p>
<p><img src="/2020/12/07/flink-yarn%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84-HA-%E5%92%8C-%E6%95%85%E9%9A%9C%E9%87%8D%E5%90%AF/be.jpg"></p>
<p>​    查看job manager信息，可以看到zookeeper中注册了flink节点信息：</p>
<p><img src="/2020/12/07/flink-yarn%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84-HA-%E5%92%8C-%E6%95%85%E9%9A%9C%E9%87%8D%E5%90%AF/ha.jpg"></p>
<p>在控制台杀死<code>YarnSessionClusterEntrypoint</code>进程，flink UI界面无法访问</p>
<p><img src="/2020/12/07/flink-yarn%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84-HA-%E5%92%8C-%E6%95%85%E9%9A%9C%E9%87%8D%E5%90%AF/404.jpg"></p>
<p>查看控制台发现在hadoop03又出现一个<code>YarnSessionClusterEntrypoint</code>进程：</p>
<p><img src="/2020/12/07/flink-yarn%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84-HA-%E5%92%8C-%E6%95%85%E9%9A%9C%E9%87%8D%E5%90%AF/la.jpg"></p>
<p>通过yarn application发现又出现一个application，打开：</p>
<p><img src="/2020/12/07/flink-yarn%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84-HA-%E5%92%8C-%E6%95%85%E9%9A%9C%E9%87%8D%E5%90%AF/ha02.jpg"></p>
<h1 id="故障重启"><a href="#故障重启" class="headerlink" title="故障重启"></a>故障重启</h1><p>flink在消费kafka源源不断产生的消息时，难免会遇到程序挂掉的情况，这时候如果无人值守，那么kafka生产的消息会一直积压无法消费，所以需要开启故障重启来自动恢复任务。</p>
<p>flink重启策略有四种：下面是在<code>flink-conf.yaml</code>配置和代码中进行配置：</p>
<h4 id="固定延迟重启策略"><a href="#固定延迟重启策略" class="headerlink" title="固定延迟重启策略"></a>固定延迟重启策略</h4><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fixed-delay:固定延迟策略</span></span><br><span class="line"><span class="attr">restart-strategy:</span> <span class="string">fixed-delay</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 尝试5次，默认Integer.MAX_VALUE</span></span><br><span class="line"><span class="attr">restart-strategy.fixed-delay.attempts:</span> <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置延迟时间10s，默认为 akka.ask.timeout时间</span></span><br><span class="line"><span class="attr">restart-strategy.fixed-delay.delay:</span> <span class="string">10s</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5表示最大重试次数为5次，10s为延迟时间</span></span><br><span class="line">env.setRestartStrategy(RestartStrategies.fixedDelayRestart(<span class="number">5</span>,Time.of(<span class="number">10</span>, TimeUnit.SECONDS)));</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="故障率重启策略"><a href="#故障率重启策略" class="headerlink" title="故障率重启策略"></a>故障率重启策略</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置重启策略为failure-rate</span></span><br><span class="line"><span class="attr">restart-strategy:</span> <span class="string">failure-rate</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 失败作业之前的给定时间间隔内的最大重启次数，默认1</span></span><br><span class="line"><span class="attr">restart-strategy.failure-rate.max-failures-per-interval:</span> <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测量故障率的时间间隔。默认1min</span></span><br><span class="line"><span class="attr">restart-strategy.failure-rate.failure-rate-interval:</span> <span class="string">5min</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 两次连续重启尝试之间的延迟，默认akka.ask.timeout时间</span></span><br><span class="line"><span class="attr">restart-strategy.failure-rate.delay:</span> <span class="string">10s</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3为最大失败次数；5min为测量的故障时间；10s为2次间的延迟时间</span></span><br><span class="line">env.setRestartStrategy(RestartStrategies.failureRateRestart(<span class="number">3</span>,Time.of(<span class="number">5</span>, TimeUnit.MINUTES),Time.of(<span class="number">10</span>, TimeUnit.SECONDS)));</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="后备重启策略"><a href="#后备重启策略" class="headerlink" title="后备重启策略"></a>后备重启策略</h4><p>​    使用群集定义的重新启动策略。这对于启用检查点的流式传输程序很有帮助。默认情况下，如果没有定义其他重启策略，则选择固定延迟重启策略。</p>
<h4 id="无重启策略"><a href="#无重启策略" class="headerlink" title="无重启策略"></a>无重启策略</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">restart-strategy:</span> <span class="string">none</span></span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setRestartStrategy(RestartStrategies.noRestart());</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>/2021/05/04/flink-sql%20cdc%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<hr>
<h2 id="flink-sql-cdc初探"><a href="#flink-sql-cdc初探" class="headerlink" title="flink-sql cdc初探"></a>flink-sql cdc初探</h2><p><img src="/2021/05/04/flink-sql%20cdc%E5%88%9D%E6%8E%A2/0.jpg"></p>
<a id="more"></a>

<p>技术架构：</p>
<p><img src="/2021/05/04/flink-sql%20cdc%E5%88%9D%E6%8E%A2/1.jpg"></p>
<p>首先，创建测试用例mysql表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> mydb;</span><br><span class="line"><span class="keyword">USE</span> mydb;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> mydb.products;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> mydb.orders;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> mydb.shipments;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">--产品表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> products (</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">INTEGER</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT PRIMARY <span class="keyword">KEY</span>,</span><br><span class="line">  <span class="keyword">name</span> <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  description <span class="built_in">VARCHAR</span>(<span class="number">512</span>)</span><br><span class="line">);</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> products AUTO_INCREMENT = <span class="number">101</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> products</span><br><span class="line"><span class="keyword">VALUES</span> (<span class="keyword">default</span>,<span class="string">&quot;scooter&quot;</span>,<span class="string">&quot;Small 2-wheel scooter&quot;</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="string">&quot;car battery&quot;</span>,<span class="string">&quot;12V car battery&quot;</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="string">&quot;12-pack drill bits&quot;</span>,<span class="string">&quot;12-pack of drill bits with sizes ranging from #40 to #3&quot;</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="string">&quot;hammer&quot;</span>,<span class="string">&quot;12oz carpenter&#x27;s hammer&quot;</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="string">&quot;hammer&quot;</span>,<span class="string">&quot;14oz carpenter&#x27;s hammer&quot;</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="string">&quot;hammer&quot;</span>,<span class="string">&quot;16oz carpenter&#x27;s hammer&quot;</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="string">&quot;rocks&quot;</span>,<span class="string">&quot;box of assorted rocks&quot;</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="string">&quot;jacket&quot;</span>,<span class="string">&quot;water resistent black wind breaker&quot;</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="string">&quot;spare tire&quot;</span>,<span class="string">&quot;24 inch spare tire&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">--订单表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> orders (</span><br><span class="line">  order_id <span class="built_in">INTEGER</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT PRIMARY <span class="keyword">KEY</span>,</span><br><span class="line">  order_date DATETIME <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  customer_name <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  price <span class="built_in">DECIMAL</span>(<span class="number">10</span>, <span class="number">5</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  product_id <span class="built_in">INTEGER</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  order_status <span class="built_in">BOOLEAN</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="comment">-- 是否下单</span></span><br><span class="line">) AUTO_INCREMENT = <span class="number">10001</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> orders</span><br><span class="line"><span class="keyword">VALUES</span> (<span class="keyword">default</span>, <span class="string">&#x27;2020-07-30 10:08:22&#x27;</span>, <span class="string">&#x27;Jark&#x27;</span>, <span class="number">50.50</span>, <span class="number">102</span>, <span class="literal">false</span>),</span><br><span class="line">       (<span class="keyword">default</span>, <span class="string">&#x27;2020-07-30 10:11:09&#x27;</span>, <span class="string">&#x27;Sally&#x27;</span>, <span class="number">15.00</span>, <span class="number">105</span>, <span class="literal">false</span>),</span><br><span class="line">       (<span class="keyword">default</span>, <span class="string">&#x27;2020-07-30 12:00:30&#x27;</span>, <span class="string">&#x27;Edward&#x27;</span>, <span class="number">25.25</span>, <span class="number">106</span>, <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 物流表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> shipments (</span><br><span class="line">  shipment_id <span class="built_in">bigint</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> auto_increment PRIMARY <span class="keyword">KEY</span>,</span><br><span class="line">  order_id <span class="built_in">bigint</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  origin <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  destination <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  is_arrived <span class="built_in">BOOLEAN</span> <span class="keyword">NOT</span> <span class="literal">NULL</span></span><br><span class="line">)auto_increment=<span class="number">1001</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> shipments</span><br><span class="line"><span class="keyword">VALUES</span> (<span class="keyword">default</span>,<span class="number">10001</span>,<span class="string">&#x27;Beijing&#x27;</span>,<span class="string">&#x27;Shanghai&#x27;</span>,<span class="literal">false</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="number">10002</span>,<span class="string">&#x27;Hangzhou&#x27;</span>,<span class="string">&#x27;Shanghai&#x27;</span>,<span class="literal">false</span>),</span><br><span class="line">       (<span class="keyword">default</span>,<span class="number">10003</span>,<span class="string">&#x27;Shanghai&#x27;</span>,<span class="string">&#x27;Hangzhou&#x27;</span>,<span class="literal">false</span>);</span><br></pre></td></tr></table></figure>

<p>本文使用以下两个cdc jar包，需要放置到flink/lib中：</p>
<ul>
<li><a href="%5Bhttps://archiva-maven-storage-prod.oss-cn-beijing.aliyuncs.com/repository/jcenter/org/apache/flink/flink-sql-connector-elasticsearch7_2.12/1.12.1/flink-sql-connector-elasticsearch7_2.12-1.12.1.jar?Expires=1620101383&OSSAccessKeyId=LTAIfU51SusnnfCC&Signature=qIaVuaZ2ebVg1uO3uBgCckaUl28=%5D(https://archiva-maven-storage-prod.oss-cn-beijing.aliyuncs.com/repository/jcenter/org/apache/flink/flink-sql-connector-elasticsearch7_2.12/1.12.1/flink-sql-connector-elasticsearch7_2.12-1.12.1.jar?spm=a2c40.maven_devops2020_goldlog_.0.0.43643054RWXSkd&Expires=1620101383&OSSAccessKeyId=LTAIfU51SusnnfCC&Signature=qIaVuaZ2ebVg1uO3uBgCckaUl28=)">flink-sql-connector-elasticsearch7_2.11-1.12.1.jar</a></li>
<li><a href="https://repo1.maven.org/maven2/com/alibaba/ververica/flink-sql-connector-mysql-cdc/1.0.0/flink-sql-connector-mysql-cdc-1.0.0.jar">flink-sql-connector-mysql-cdc-1.0.0.jar</a></li>
</ul>
<p>紧接着，开启flink集群，打开flink sql客户端，并创建cdc表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[root@hadoop04 flink-1.12]<span class="comment"># bin/sql-client.sh embedded</span></span><br><span class="line">No default environment specified.</span><br><span class="line">Searching for &#x27;/home/module/flink-1.12/conf/sql-client-defaults.yaml&#x27;...found.</span><br><span class="line">Reading default environment from: file:/home/module/flink-1.12/conf/sql-client-defaults.yaml</span><br><span class="line">No session environment specified.</span><br><span class="line"></span><br><span class="line">Command history file path: /root/.flink-sql-history</span><br><span class="line">                                   ▒▓██▓██▒</span><br><span class="line">                               ▓████▒▒█▓▒▓███▓▒</span><br><span class="line">                            ▓███▓░░        ▒▒▒▓██▒  ▒</span><br><span class="line">                          ░██▒   ▒▒▓▓█▓▓▒░      ▒████</span><br><span class="line">                          ██▒         ░▒▓███▒    ▒█▒█▒</span><br><span class="line">                            ░▓█            ███   ▓░▒██</span><br><span class="line">                              ▓█       ▒▒▒▒▒▓██▓░▒░▓▓█</span><br><span class="line">                            █░ █   ▒▒░       ███▓▓█ ▒█▒▒▒</span><br><span class="line">                            ████░   ▒▓█▓      ██▒▒▒ ▓███▒</span><br><span class="line">                         ░▒█▓▓██       ▓█▒    ▓█▒▓██▓ ░█░</span><br><span class="line">                   ▓░▒▓████▒ ██         ▒█    █▓░▒█▒░▒█▒</span><br><span class="line">                  ███▓░██▓  ▓█           █   █▓ ▒▓█▓▓█▒</span><br><span class="line">                ░██▓  ░█░            █  █▒ ▒█████▓▒ ██▓░▒</span><br><span class="line">               ███░ ░ █░          ▓ ░█ █████▒░░    ░█░▓  ▓░</span><br><span class="line">              ██▓█ ▒▒▓▒          ▓███████▓░       ▒█▒ ▒▓ ▓██▓</span><br><span class="line">           ▒██▓ ▓█ █▓█       ░▒█████▓▓▒░         ██▒▒  █ ▒  ▓█▒</span><br><span class="line">           ▓█▓  ▓█ ██▓ ░▓▓▓▓▓▓▓▒              ▒██▓           ░█▒</span><br><span class="line">           ▓█    █ ▓███▓▒░              ░▓▓▓███▓          ░▒░ ▓█</span><br><span class="line">           ██▓    ██▒    ░▒▓▓███▓▓▓▓▓██████▓▒            ▓███  █</span><br><span class="line">          ▓███▒ ███   ░▓▓▒░░   ░▓████▓░                  ░▒▓▒  █▓</span><br><span class="line">          █▓▒▒▓▓██  ░▒▒░░░▒▒▒▒▓██▓░                            █▓</span><br><span class="line">          ██ ▓░▒█   ▓▓▓▓▒░░  ▒█▓       ▒▓▓██▓    ▓▒          ▒▒▓</span><br><span class="line">          ▓█▓ ▓▒█  █▓░  ░▒▓▓██▒            ░▓█▒   ▒▒▒░▒▒▓█████▒</span><br><span class="line">           ██░ ▓█▒█▒  ▒▓▓▒  ▓█                █░      ░░░░   ░█▒</span><br><span class="line">           ▓█   ▒█▓   ░     █░                ▒█              █▓</span><br><span class="line">            █▓   ██         █░                 ▓▓        ▒█▓▓▓▒█░</span><br><span class="line">             █▓ ░▓██░       ▓▒                  ▓█▓▒░░░▒▓█░    ▒█</span><br><span class="line">              ██   ▓█▓░      ▒                    ░▒█▒██▒      ▓▓</span><br><span class="line">               ▓█▒   ▒█▓▒░                         ▒▒ █▒█▓▒▒░░▒██</span><br><span class="line">                ░██▒    ▒▓▓▒                     ▓██▓▒█▒ ░▓▓▓▓▒█▓</span><br><span class="line">                  ░▓██▒                          ▓░  ▒█▓█  ░░▒▒▒</span><br><span class="line">                      ▒▓▓▓▓▓▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒░░▓▓  ▓░▒█░</span><br><span class="line">          </span><br><span class="line">    ______ _ _       _       _____  ____  _         _____ _ _            _  BETA   </span><br><span class="line">   |  ____| (_)     | |     / ____|/ __ \| |       / ____| (_)          | |  </span><br><span class="line">   | |__  | |_ _ __ | | __ | (___ | |  | | |      | |    | |_  ___ _ __ | |_ </span><br><span class="line">   |  __| | | | &#x27;_ \| |/ /  \___ \| |  | | |      | |    | | |/ _ \ &#x27;_ \| __|</span><br><span class="line">   | |    | | | | | |   &lt;   ____) | |__| | |____  | |____| | |  __/ | | | |_ </span><br><span class="line">   |_|    |_|_|_| |_|_|\_\ |_____/ \___\_\______|  \_____|_|_|\___|_| |_|\__|</span><br><span class="line">          </span><br><span class="line">        Welcome! Enter &#x27;<span class="keyword">HELP</span>;&#x27; to list all available commands. &#x27;QUIT;&#x27; to exit.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Flink SQL&gt; CREATE TABLE products (</span><br><span class="line">&gt;   id INT,</span><br><span class="line">&gt;   name STRING,</span><br><span class="line">&gt;   description STRING</span><br><span class="line">&gt; ) WITH (</span><br><span class="line">&gt;   &#x27;connector&#x27; = &#x27;mysql-cdc&#x27;,</span><br><span class="line">&gt;   &#x27;hostname&#x27; = &#x27;localhost&#x27;,</span><br><span class="line">&gt;   &#x27;port&#x27; = &#x27;3306&#x27;,</span><br><span class="line">&gt;   &#x27;username&#x27; = &#x27;root&#x27;,</span><br><span class="line">&gt;   &#x27;password&#x27; = &#x27;143382&#x27;,</span><br><span class="line">&gt;   &#x27;database-name&#x27; = &#x27;mydb&#x27;,</span><br><span class="line">&gt;   &#x27;table-name&#x27; = &#x27;products&#x27;</span><br><span class="line">&gt; );</span><br><span class="line">[INFO] Table has been created.</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; </span><br><span class="line">&gt; CREATE TABLE orders (</span><br><span class="line">&gt;   order_id INT,</span><br><span class="line">&gt;   order_date TIMESTAMP(0),</span><br><span class="line">&gt;   customer_name STRING,</span><br><span class="line">&gt;   price DECIMAL(10, 5),</span><br><span class="line">&gt;   product_id INT,</span><br><span class="line">&gt;   order_status BOOLEAN</span><br><span class="line">&gt; ) WITH (</span><br><span class="line">&gt;   &#x27;connector&#x27; = &#x27;mysql-cdc&#x27;,</span><br><span class="line">&gt;   &#x27;hostname&#x27; = &#x27;localhost&#x27;,</span><br><span class="line">&gt;   &#x27;port&#x27; = &#x27;3306&#x27;,</span><br><span class="line">&gt;   &#x27;username&#x27; = &#x27;root&#x27;,</span><br><span class="line">&gt;   &#x27;password&#x27; = &#x27;143382&#x27;,</span><br><span class="line">&gt;   &#x27;database-name&#x27; = &#x27;mydb&#x27;,</span><br><span class="line">&gt;   &#x27;table-name&#x27; = &#x27;orders&#x27;</span><br><span class="line">&gt; );</span><br><span class="line">[INFO] Table has been created.</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; </span><br><span class="line">&gt; CREATE TABLE shipments (</span><br><span class="line">&gt;   shipment_id INT,</span><br><span class="line">&gt;   order_id INT,</span><br><span class="line">&gt;   origin STRING,</span><br><span class="line">&gt;   destination STRING,</span><br><span class="line">&gt;   is_arrived BOOLEAN</span><br><span class="line">&gt; ) WITH (</span><br><span class="line">&gt;   &#x27;connector&#x27; = &#x27;mysql-cdc&#x27;,</span><br><span class="line">&gt;   &#x27;hostname&#x27; = &#x27;localhost&#x27;,</span><br><span class="line">&gt;   &#x27;port&#x27; = &#x27;3306&#x27;,</span><br><span class="line">&gt;   &#x27;username&#x27; = &#x27;root&#x27;,</span><br><span class="line">&gt;   &#x27;password&#x27; = &#x27;143382&#x27;,</span><br><span class="line">&gt;   &#x27;database-name&#x27; = &#x27;mydb&#x27;,</span><br><span class="line">&gt;   &#x27;table-name&#x27; = &#x27;shipments&#x27;</span><br><span class="line">&gt; );</span><br><span class="line">[INFO] Table has been created.</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; </span><br><span class="line">&gt; CREATE TABLE enriched_orders (</span><br><span class="line">&gt;   order_id INT,</span><br><span class="line">&gt;   order_date TIMESTAMP(0),</span><br><span class="line">&gt;   customer_name STRING,</span><br><span class="line">&gt;   price DECIMAL(10, 5),</span><br><span class="line">&gt;   product_id INT,</span><br><span class="line">&gt;   order_status BOOLEAN,</span><br><span class="line">&gt;   product_name STRING,</span><br><span class="line">&gt;   product_description STRING,</span><br><span class="line">&gt;   shipment_id INT,</span><br><span class="line">&gt;   origin STRING,</span><br><span class="line">&gt;   destination STRING,</span><br><span class="line">&gt;   is_arrived BOOLEAN,</span><br><span class="line">&gt;   PRIMARY KEY (order_id) NOT ENFORCED</span><br><span class="line">&gt; ) WITH (</span><br><span class="line">&gt;     &#x27;connector&#x27; = &#x27;elasticsearch-7&#x27;,</span><br><span class="line">&gt;     &#x27;hosts&#x27; = &#x27;http://hadoop04:9200&#x27;,</span><br><span class="line">&gt;     &#x27;index&#x27; = &#x27;enriched_orders&#x27;</span><br><span class="line">&gt; );</span><br><span class="line">[INFO] Table has been created.</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; </span><br><span class="line">&gt; INSERT INTO enriched_orders</span><br><span class="line">&gt; SELECT o.*, p.name, p.description, s.shipment_id, s.origin, s.destination, s.is_arrived</span><br><span class="line">&gt; FROM orders AS o</span><br><span class="line">&gt; LEFT JOIN products AS p ON o.product_id = p.id</span><br><span class="line">&gt; LEFT JOIN shipments AS s ON o.order_id = s.order_id;</span><br><span class="line">[INFO] Submitting SQL <span class="keyword">update</span> <span class="keyword">statement</span> <span class="keyword">to</span> the cluster...</span><br><span class="line">[INFO] <span class="keyword">Table</span> <span class="keyword">update</span> <span class="keyword">statement</span> has been successfully submitted <span class="keyword">to</span> the cluster:</span><br><span class="line">Job <span class="keyword">ID</span>: bfe9e15269924fec5bb9028e175efdab</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>查看flink web端，发现正在运行的flink-sql cdc任务：</p>
<p><img src="/2021/05/04/flink-sql%20cdc%E5%88%9D%E6%8E%A2/2.jpg"></p>
<p>查看kibana中数据条数：3</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;took&quot;</span> : <span class="number">0</span>,</span><br><span class="line">  <span class="attr">&quot;timed_out&quot;</span> : <span class="literal">false</span>,</span><br><span class="line">  <span class="attr">&quot;_shards&quot;</span> : &#123;</span><br><span class="line">    <span class="attr">&quot;total&quot;</span> : <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;successful&quot;</span> : <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;skipped&quot;</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="attr">&quot;failed&quot;</span> : <span class="number">0</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;hits&quot;</span> : &#123;</span><br><span class="line">    <span class="attr">&quot;total&quot;</span> : &#123;</span><br><span class="line">      <span class="attr">&quot;value&quot;</span> : <span class="number">3</span>,</span><br><span class="line">      <span class="attr">&quot;relation&quot;</span> : <span class="string">&quot;eq&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;max_score&quot;</span> : <span class="number">1.0</span>,</span><br><span class="line">    <span class="attr">&quot;hits&quot;</span> : [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;_index&quot;</span> : <span class="string">&quot;enriched_orders&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_type&quot;</span> : <span class="string">&quot;_doc&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_id&quot;</span> : <span class="string">&quot;10001&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_score&quot;</span> : <span class="number">1.0</span>,</span><br><span class="line">        <span class="attr">&quot;_source&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;order_id&quot;</span> : <span class="number">10001</span>,</span><br><span class="line">          <span class="attr">&quot;order_date&quot;</span> : <span class="string">&quot;2020-07-30 10:08:22&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;customer_name&quot;</span> : <span class="string">&quot;Jark&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;price&quot;</span> : <span class="number">50.5</span>,</span><br><span class="line">          <span class="attr">&quot;product_id&quot;</span> : <span class="number">102</span>,</span><br><span class="line">          <span class="attr">&quot;order_status&quot;</span> : <span class="literal">false</span>,</span><br><span class="line">          <span class="attr">&quot;product_name&quot;</span> : <span class="string">&quot;car battery&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;product_description&quot;</span> : <span class="string">&quot;12V car battery&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;shipment_id&quot;</span> : <span class="number">1001</span>,</span><br><span class="line">          <span class="attr">&quot;origin&quot;</span> : <span class="string">&quot;Beijing&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;destination&quot;</span> : <span class="string">&quot;Shanghai&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;is_arrived&quot;</span> : <span class="literal">false</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;_index&quot;</span> : <span class="string">&quot;enriched_orders&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_type&quot;</span> : <span class="string">&quot;_doc&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_id&quot;</span> : <span class="string">&quot;10002&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_score&quot;</span> : <span class="number">1.0</span>,</span><br><span class="line">        <span class="attr">&quot;_source&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;order_id&quot;</span> : <span class="number">10002</span>,</span><br><span class="line">          <span class="attr">&quot;order_date&quot;</span> : <span class="string">&quot;2020-07-30 10:11:09&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;customer_name&quot;</span> : <span class="string">&quot;Sally&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;price&quot;</span> : <span class="number">15</span>,</span><br><span class="line">          <span class="attr">&quot;product_id&quot;</span> : <span class="number">105</span>,</span><br><span class="line">          <span class="attr">&quot;order_status&quot;</span> : <span class="literal">false</span>,</span><br><span class="line">          <span class="attr">&quot;product_name&quot;</span> : <span class="string">&quot;hammer&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;product_description&quot;</span> : <span class="string">&quot;14oz carpenter&#x27;s hammer&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;shipment_id&quot;</span> : <span class="number">1002</span>,</span><br><span class="line">          <span class="attr">&quot;origin&quot;</span> : <span class="string">&quot;Hangzhou&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;destination&quot;</span> : <span class="string">&quot;Shanghai&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;is_arrived&quot;</span> : <span class="literal">false</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;_index&quot;</span> : <span class="string">&quot;enriched_orders&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_type&quot;</span> : <span class="string">&quot;_doc&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_id&quot;</span> : <span class="string">&quot;10003&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_score&quot;</span> : <span class="number">1.0</span>,</span><br><span class="line">        <span class="attr">&quot;_source&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;order_id&quot;</span> : <span class="number">10003</span>,</span><br><span class="line">          <span class="attr">&quot;order_date&quot;</span> : <span class="string">&quot;2020-07-30 12:00:30&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;customer_name&quot;</span> : <span class="string">&quot;Edward&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;price&quot;</span> : <span class="number">25.25</span>,</span><br><span class="line">          <span class="attr">&quot;product_id&quot;</span> : <span class="number">106</span>,</span><br><span class="line">          <span class="attr">&quot;order_status&quot;</span> : <span class="literal">false</span>,</span><br><span class="line">          <span class="attr">&quot;product_name&quot;</span> : <span class="string">&quot;hammer&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;product_description&quot;</span> : <span class="string">&quot;16oz carpenter&#x27;s hammer&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;shipment_id&quot;</span> : <span class="number">1003</span>,</span><br><span class="line">          <span class="attr">&quot;origin&quot;</span> : <span class="string">&quot;Shanghai&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;destination&quot;</span> : <span class="string">&quot;Hangzhou&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;is_arrived&quot;</span> : <span class="literal">false</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>下面在flink-sql客户端中进行插入更新，并观察数据条数和内容的变化：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--MySQL</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> orders</span><br><span class="line"><span class="keyword">VALUES</span> (<span class="keyword">default</span>, <span class="string">&#x27;2020-07-30 15:22:00&#x27;</span>, <span class="string">&#x27;Jark&#x27;</span>, <span class="number">29.71</span>, <span class="number">104</span>, <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">--MySQL</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> shipments</span><br><span class="line"><span class="keyword">VALUES</span> (<span class="keyword">default</span>,<span class="number">10004</span>,<span class="string">&#x27;Shanghai&#x27;</span>,<span class="string">&#x27;Beijing&#x27;</span>,<span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">--MySQL</span></span><br><span class="line"><span class="keyword">UPDATE</span> orders <span class="keyword">SET</span> order_status = <span class="literal">true</span> <span class="keyword">WHERE</span> order_id = <span class="number">10004</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--MySQL</span></span><br><span class="line"><span class="keyword">UPDATE</span> shipments <span class="keyword">SET</span> is_arrived = <span class="literal">true</span> <span class="keyword">WHERE</span> shipment_id = <span class="number">1004</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--MySQL</span></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> orders <span class="keyword">WHERE</span> order_id = <span class="number">10004</span>;</span><br></pre></td></tr></table></figure>



<p>观察得到，第一条数据插入后，订单数据多出了一条变成了4条，随后物流信息更新，订单状态更新，最后删除订单，es中数据又变成3条。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;took&quot;</span> : <span class="number">993</span>,</span><br><span class="line">  <span class="attr">&quot;timed_out&quot;</span> : <span class="literal">false</span>,</span><br><span class="line">  <span class="attr">&quot;_shards&quot;</span> : &#123;</span><br><span class="line">    <span class="attr">&quot;total&quot;</span> : <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;successful&quot;</span> : <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;skipped&quot;</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="attr">&quot;failed&quot;</span> : <span class="number">0</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;hits&quot;</span> : &#123;</span><br><span class="line">    <span class="attr">&quot;total&quot;</span> : &#123;</span><br><span class="line">      <span class="attr">&quot;value&quot;</span> : <span class="number">4</span>,</span><br><span class="line">      <span class="attr">&quot;relation&quot;</span> : <span class="string">&quot;eq&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;max_score&quot;</span> : <span class="number">1.0</span>,</span><br><span class="line">    <span class="attr">&quot;hits&quot;</span> : [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;_index&quot;</span> : <span class="string">&quot;enriched_orders&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_type&quot;</span> : <span class="string">&quot;_doc&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_id&quot;</span> : <span class="string">&quot;10001&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_score&quot;</span> : <span class="number">1.0</span>,</span><br><span class="line">        <span class="attr">&quot;_source&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;order_id&quot;</span> : <span class="number">10001</span>,</span><br><span class="line">          <span class="attr">&quot;order_date&quot;</span> : <span class="string">&quot;2020-07-30 10:08:22&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;customer_name&quot;</span> : <span class="string">&quot;Jark&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;price&quot;</span> : <span class="number">50.5</span>,</span><br><span class="line">          <span class="attr">&quot;product_id&quot;</span> : <span class="number">102</span>,</span><br><span class="line">          <span class="attr">&quot;order_status&quot;</span> : <span class="literal">false</span>,</span><br><span class="line">          <span class="attr">&quot;product_name&quot;</span> : <span class="string">&quot;car battery&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;product_description&quot;</span> : <span class="string">&quot;12V car battery&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;shipment_id&quot;</span> : <span class="number">1001</span>,</span><br><span class="line">          <span class="attr">&quot;origin&quot;</span> : <span class="string">&quot;Beijing&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;destination&quot;</span> : <span class="string">&quot;Shanghai&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;is_arrived&quot;</span> : <span class="literal">false</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;_index&quot;</span> : <span class="string">&quot;enriched_orders&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_type&quot;</span> : <span class="string">&quot;_doc&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_id&quot;</span> : <span class="string">&quot;10002&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_score&quot;</span> : <span class="number">1.0</span>,</span><br><span class="line">        <span class="attr">&quot;_source&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;order_id&quot;</span> : <span class="number">10002</span>,</span><br><span class="line">          <span class="attr">&quot;order_date&quot;</span> : <span class="string">&quot;2020-07-30 10:11:09&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;customer_name&quot;</span> : <span class="string">&quot;Sally&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;price&quot;</span> : <span class="number">15</span>,</span><br><span class="line">          <span class="attr">&quot;product_id&quot;</span> : <span class="number">105</span>,</span><br><span class="line">          <span class="attr">&quot;order_status&quot;</span> : <span class="literal">false</span>,</span><br><span class="line">          <span class="attr">&quot;product_name&quot;</span> : <span class="string">&quot;hammer&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;product_description&quot;</span> : <span class="string">&quot;14oz carpenter&#x27;s hammer&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;shipment_id&quot;</span> : <span class="number">1002</span>,</span><br><span class="line">          <span class="attr">&quot;origin&quot;</span> : <span class="string">&quot;Hangzhou&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;destination&quot;</span> : <span class="string">&quot;Shanghai&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;is_arrived&quot;</span> : <span class="literal">false</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;_index&quot;</span> : <span class="string">&quot;enriched_orders&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_type&quot;</span> : <span class="string">&quot;_doc&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_id&quot;</span> : <span class="string">&quot;10003&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_score&quot;</span> : <span class="number">1.0</span>,</span><br><span class="line">        <span class="attr">&quot;_source&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;order_id&quot;</span> : <span class="number">10003</span>,</span><br><span class="line">          <span class="attr">&quot;order_date&quot;</span> : <span class="string">&quot;2020-07-30 12:00:30&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;customer_name&quot;</span> : <span class="string">&quot;Edward&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;price&quot;</span> : <span class="number">25.25</span>,</span><br><span class="line">          <span class="attr">&quot;product_id&quot;</span> : <span class="number">106</span>,</span><br><span class="line">          <span class="attr">&quot;order_status&quot;</span> : <span class="literal">false</span>,</span><br><span class="line">          <span class="attr">&quot;product_name&quot;</span> : <span class="string">&quot;hammer&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;product_description&quot;</span> : <span class="string">&quot;16oz carpenter&#x27;s hammer&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;shipment_id&quot;</span> : <span class="number">1003</span>,</span><br><span class="line">          <span class="attr">&quot;origin&quot;</span> : <span class="string">&quot;Shanghai&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;destination&quot;</span> : <span class="string">&quot;Hangzhou&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;is_arrived&quot;</span> : <span class="literal">false</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;_index&quot;</span> : <span class="string">&quot;enriched_orders&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_type&quot;</span> : <span class="string">&quot;_doc&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_id&quot;</span> : <span class="string">&quot;10004&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;_score&quot;</span> : <span class="number">1.0</span>,</span><br><span class="line">        <span class="attr">&quot;_source&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;order_id&quot;</span> : <span class="number">10004</span>,</span><br><span class="line">          <span class="attr">&quot;order_date&quot;</span> : <span class="string">&quot;2020-07-30 15:22:00&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;customer_name&quot;</span> : <span class="string">&quot;Jark&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;price&quot;</span> : <span class="number">29.71</span>,</span><br><span class="line">          <span class="attr">&quot;product_id&quot;</span> : <span class="number">104</span>,</span><br><span class="line">          <span class="attr">&quot;order_status&quot;</span> : <span class="literal">true</span>,</span><br><span class="line">          <span class="attr">&quot;product_name&quot;</span> : <span class="string">&quot;hammer&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;product_description&quot;</span> : <span class="string">&quot;12oz carpenter&#x27;s hammer&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;shipment_id&quot;</span> : <span class="number">1004</span>,</span><br><span class="line">          <span class="attr">&quot;origin&quot;</span> : <span class="string">&quot;Shanghai&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;destination&quot;</span> : <span class="string">&quot;Beijing&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;is_arrived&quot;</span> : <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>cdc sink可以是es，同样也可以是kafka等，使用sql cdc简化了代码开发的繁琐流程，真好用。</p>
<p><img src="/2021/05/04/flink-sql%20cdc%E5%88%9D%E6%8E%A2/3.jpg"></p>
<p><img src="/2021/05/04/flink-sql%20cdc%E5%88%9D%E6%8E%A2/4.jpg"></p>
]]></content>
  </entry>
  <entry>
    <title>flink实时数仓-dwd层维表关联</title>
    <url>/2020/12/14/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dwd%E5%B1%82%E7%BB%B4%E8%A1%A8%E5%85%B3%E8%81%94/</url>
    <content><![CDATA[<p><img src="https://flink.apache.org/img/flink-home-graphic.png" alt="img"></p>
<a id="more"></a>

<p>​        在实时业务场景中，mysql中的业务数据通过maxwell传到kafka后，我们可以将来自mysql的不同表传来的数据通过flink回传到不同的topic，然后再开启另外flink消费这些来自不同topic的数据并存储到hbase中，这样就可以方便维度表和事实表的关联。OLAP分析引擎往往不善于关联不同表的数据，即使是clickhouse的join也会损耗性能，所以在flink可以将表进行关联，也可以拓宽下游即席查询的广度。</p>
<p><img src="/2020/12/14/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dwd%E5%B1%82%E7%BB%B4%E8%A1%A8%E5%85%B3%E8%81%94/2.jpg"></p>
<p>​        在监控mysqlbinlog过程中，maxwell具有canal所没有的优势，那就是bootstrap功能，可以将mysql中的历史数据传输到kafka，在需要历史数据关联的时候就比canal有优势。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/maxwell-bootstrap --user root  --password 143382 --host hadoop01  --database gmall --table base_province --client_id maxwell_1</span><br></pre></td></tr></table></figure>

<p><img src="/2020/12/14/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dwd%E5%B1%82%E7%BB%B4%E8%A1%A8%E5%85%B3%E8%81%94/3.jpg"></p>
<p>需求整体架构：</p>
<p><img src="/2020/12/14/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dwd%E5%B1%82%E7%BB%B4%E8%A1%A8%E5%85%B3%E8%81%94/4.jpg"></p>
<p>flink-java实现：<a href="https://github.com/ycfn97/flink-utils.git">GitHub</a></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> dwd;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> bean.OrderInfo01;</span><br><span class="line"><span class="keyword">import</span> bean.dim.ProvinceInfo;</span><br><span class="line"><span class="keyword">import</span> bean.dim.UserInfo;</span><br><span class="line"><span class="keyword">import</span> bean.dim.UserState;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FilterFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.RichMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.RuntimeContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.ConfigConstants;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.RestOptions;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.memory.MemoryStateBackend;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch.util.RetryRejectedExecutionFailureHandler;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.util.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.http.HttpHost;</span><br><span class="line"><span class="keyword">import</span> org.apache.jute.Index;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.ActionRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.index.IndexRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.Requests;</span><br><span class="line"><span class="keyword">import</span> util.MyKafkaSink02;</span><br><span class="line"><span class="keyword">import</span> util.PhoenixUtil;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Copyright(c) 2020-2021 sparrow All Rights Reserved</span></span><br><span class="line"><span class="comment"> * Project: gmall2020-parent</span></span><br><span class="line"><span class="comment"> * Package: dwd</span></span><br><span class="line"><span class="comment"> * ClassName: OrderInfoApp01</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> 18729 created on date: 2020/12/10 12:22</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@version</span> 1.0</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> JDK 1.8</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderInfoApp01</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String kafkaBrokers = <span class="string">&quot;hadoop01:9092&quot;</span>;</span><br><span class="line">        String zkBrokers = <span class="string">&quot;hadoop01:2181,hadoop02:2181,hadoop03:2181&quot;</span>;</span><br><span class="line">        String topic = <span class="string">&quot;ODS_ORDER_INFO&quot;</span>;</span><br><span class="line">        String groupId = <span class="string">&quot;order_info_group&quot;</span>;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;===============》 flink任务开始  ==============》&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.setBoolean(ConfigConstants.LOCAL_START_WEBSERVER, <span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">//自定义端口</span></span><br><span class="line">        conf.setInteger(RestOptions.PORT, <span class="number">8060</span>);</span><br><span class="line">        <span class="comment">//本地env</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(conf);</span><br><span class="line">        env.setParallelism(<span class="number">4</span>);</span><br><span class="line">        <span class="comment">//生产env</span></span><br><span class="line">        <span class="comment">//val env = StreamExecutionEnvironment.getExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span></span><br><span class="line">        <span class="comment">//设置kafka连接参数</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, kafkaBrokers);</span><br><span class="line">        properties.setProperty(<span class="string">&quot;zookeeper.connect&quot;</span>, zkBrokers);</span><br><span class="line">        properties.setProperty(<span class="string">&quot;group.id&quot;</span>, groupId);</span><br><span class="line">        <span class="comment">//设置时间类型</span></span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">        <span class="comment">//设置检查点时间间隔</span></span><br><span class="line"><span class="comment">//        env.enableCheckpointing(5000);</span></span><br><span class="line"><span class="comment">//        env.setStateBackend( new MemoryStateBackend());</span></span><br><span class="line">        <span class="comment">//创建kafak消费者，获取kafak中的数据</span></span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; kafkaConsumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(topic, <span class="keyword">new</span> SimpleStringSchema(), properties);</span><br><span class="line">        DataStreamSource&lt;String&gt; kafkaData = env.addSource(kafkaConsumer);</span><br><span class="line">        DataStream&lt;String&gt; userData = kafkaData.map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line"><span class="comment">//                System.out.println(&quot;&gt;&gt;&gt;&gt;&gt;&gt;接收topic报文:&quot;+s+&quot;&lt;&lt;&lt;&lt;&lt;&quot;);</span></span><br><span class="line">                <span class="keyword">return</span> s;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;OrderInfo01&gt; map = userData.map(<span class="keyword">new</span> RichMapFunction&lt;String, OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> OrderInfo01 <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                OrderInfo01 o = (OrderInfo01) JSON.parseObject(value, Class.forName(<span class="string">&quot;bean.OrderInfo01&quot;</span>));</span><br><span class="line">                String[] s = o.getCreate_time().split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">                o.setCreate_date(s[<span class="number">0</span>]);</span><br><span class="line">                String[] split = s[<span class="number">1</span>].split(<span class="string">&quot;:&quot;</span>);</span><br><span class="line">                o.setCreate_hour(split[<span class="number">0</span>]);</span><br><span class="line">                <span class="keyword">return</span> o;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;OrderInfo01&gt; map1 = map.map(<span class="keyword">new</span> RichMapFunction&lt;OrderInfo01, OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> OrderInfo01 <span class="title">map</span><span class="params">(OrderInfo01 value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                value.setIf_first_order(<span class="string">&quot;1&quot;</span>);</span><br><span class="line">                Long user_id = value.getUser_id();</span><br><span class="line">                String s = <span class="string">&quot;select user_id,if_consumed from USER_STATE2020 where user_id = &#x27;&quot;</span>+user_id+<span class="string">&quot;&#x27;&quot;</span>;</span><br><span class="line">                List&lt;JSONObject&gt; jsonObjects = <span class="keyword">new</span> PhoenixUtil().queryList(s);</span><br><span class="line"><span class="comment">//                System.out.println(jsonObjects);</span></span><br><span class="line">                <span class="keyword">for</span> (JSONObject jsonObject:jsonObjects) &#123;</span><br><span class="line"><span class="comment">//                    System.out.println(jsonObject.getString(&quot;USER_ID&quot;));</span></span><br><span class="line"><span class="comment">//                    System.out.println(jsonObject.getString(&quot;IF_CONSUMED&quot;));</span></span><br><span class="line">                    <span class="keyword">if</span> (jsonObject.getString(<span class="string">&quot;USER_ID&quot;</span>).equals(value.getUser_id().toString())&amp;&amp;<span class="string">&quot;1&quot;</span>.equals(jsonObject.getString(<span class="string">&quot;IF_CONSUMED&quot;</span>)))&#123;</span><br><span class="line"><span class="comment">//                        System.out.println(111111111);</span></span><br><span class="line">                        value.setIf_first_order(<span class="string">&quot;0&quot;</span>);</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"><span class="comment">//                System.out.println(value);</span></span><br><span class="line">                <span class="keyword">return</span> value;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;OrderInfo01&gt; map2 = map1.map(<span class="keyword">new</span> RichMapFunction&lt;OrderInfo01, OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> OrderInfo01 <span class="title">map</span><span class="params">(OrderInfo01 value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String sql = <span class="string">&quot;select  id,name,region_id,area_code,iso_code,iso_3166_2 from GMALL2020_PROVINCE_INFO &quot;</span>;</span><br><span class="line">                List&lt;JSONObject&gt; jsonObjects = <span class="keyword">new</span> PhoenixUtil().queryList(sql);</span><br><span class="line">                HashMap&lt;String, ProvinceInfo&gt; provinceInfoHashMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">                <span class="keyword">for</span> (JSONObject object : jsonObjects) &#123;</span><br><span class="line">                    String id = object.getString(<span class="string">&quot;ID&quot;</span>);</span><br><span class="line">                    ProvinceInfo provinceInfo = <span class="keyword">new</span> ProvinceInfo(object.getString(<span class="string">&quot;ID&quot;</span>), object.getString(<span class="string">&quot;NAME&quot;</span>), object.getString(<span class="string">&quot;REGION_ID&quot;</span>), object.getString(<span class="string">&quot;AREA_CODE&quot;</span>), object.getString(<span class="string">&quot;ISO_CODE&quot;</span>), object.getString(<span class="string">&quot;ISO_3166_2&quot;</span>));</span><br><span class="line">                    provinceInfoHashMap.put(id, provinceInfo);</span><br><span class="line">                &#125;</span><br><span class="line"><span class="comment">//                System.out.println(provinceInfoHashMap.get(value.getProvince_id().toString()));</span></span><br><span class="line">                value.setProvince_name(provinceInfoHashMap.get(value.getProvince_id().toString()).getName());</span><br><span class="line">                value.setProvince_area_code(provinceInfoHashMap.get(value.getProvince_id().toString()).getArea_code());</span><br><span class="line">                value.setProvince_iso_code(provinceInfoHashMap.get(value.getProvince_id().toString()).getIso_code());</span><br><span class="line">                value.setProvince_iso_3166_2(provinceInfoHashMap.get(value.getProvince_id().toString()).getIso_3166_2());</span><br><span class="line">                <span class="keyword">return</span> value;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        map2.filter(<span class="keyword">new</span> FilterFunction&lt;OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(OrderInfo01 value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value!=<span class="keyword">null</span>&amp;&amp;value.getUser_id()!=<span class="keyword">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;OrderInfo01&gt; map3 = map2.map(<span class="keyword">new</span> RichMapFunction&lt;OrderInfo01, OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> OrderInfo01 <span class="title">map</span><span class="params">(OrderInfo01 value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String sql = <span class="string">&quot;select id,user_level,birthday,gender,age_group,gender_name from GMALL_USER_INFO &quot;</span>;</span><br><span class="line">                List&lt;JSONObject&gt; jsonObjects = <span class="keyword">new</span> PhoenixUtil().queryList(sql);</span><br><span class="line">                HashMap&lt;String, UserInfo&gt; userInfoHashMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">                <span class="keyword">for</span> (JSONObject object : jsonObjects) &#123;</span><br><span class="line">                    String id = object.getString(<span class="string">&quot;ID&quot;</span>);</span><br><span class="line">                    UserInfo userInfo = <span class="keyword">new</span> UserInfo(object.getString(<span class="string">&quot;ID&quot;</span>), object.getString(<span class="string">&quot;USER_LEVEL&quot;</span>), object.getString(<span class="string">&quot;BIRTHDAY&quot;</span>), object.getString(<span class="string">&quot;GENDER&quot;</span>), object.getString(<span class="string">&quot;AGE_GROUP&quot;</span>), object.getString(<span class="string">&quot;GENDER_NAME&quot;</span>));</span><br><span class="line">                    userInfoHashMap.put(id, userInfo);</span><br><span class="line">                &#125;</span><br><span class="line"><span class="comment">//                System.out.println(value.getUser_id());</span></span><br><span class="line"><span class="comment">//                System.out.println(userInfoHashMap.get(value.getUser_id().toString()));</span></span><br><span class="line"><span class="comment">//                System.out.println(userInfoHashMap.get(value.getUser_id().toString()).getAge_group());</span></span><br><span class="line">                value.setUser_age_group(userInfoHashMap.get(value.getUser_id().toString()).getAge_group());</span><br><span class="line">                value.setUser_gender(userInfoHashMap.get(value.getUser_id().toString()).getGender_name());</span><br><span class="line">                <span class="keyword">return</span> value;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        map3.filter(<span class="keyword">new</span> FilterFunction&lt;OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(OrderInfo01 value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value.getIf_first_order()==<span class="string">&quot;1&quot;</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).map(<span class="keyword">new</span> RichMapFunction&lt;OrderInfo01, OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> OrderInfo01 <span class="title">map</span><span class="params">(OrderInfo01 value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">new</span> PhoenixUtil().update(<span class="string">&quot;upsert into USER_STATE2020 values(&#x27;&quot;</span> +</span><br><span class="line">                        value.getUser_id()+<span class="string">&quot;&#x27;,&#x27;&quot;</span>+value.getIf_first_order()+<span class="string">&quot;&#x27;)&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> value;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        map3.addSink(EsUtils().build());</span><br><span class="line">        map3.addSink(<span class="keyword">new</span> MyKafkaSink02());</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> ElasticsearchSink.<span class="function">Builder&lt;OrderInfo01&gt; <span class="title">EsUtils</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        String format = <span class="keyword">new</span> SimpleDateFormat(<span class="string">&quot;yyyy-MM-dd&quot;</span>).format(<span class="keyword">new</span> Date());</span><br><span class="line">        List&lt;HttpHost&gt; httpHosts = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        httpHosts.add(<span class="keyword">new</span> HttpHost(<span class="string">&quot;hadoop01&quot;</span>, <span class="number">9200</span>, <span class="string">&quot;http&quot;</span>));</span><br><span class="line"></span><br><span class="line">        ElasticsearchSink.Builder&lt;OrderInfo01&gt; esBuilder = <span class="keyword">new</span> ElasticsearchSink.Builder&lt;&gt;(httpHosts, <span class="keyword">new</span> ElasticsearchSinkFunction&lt;OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(OrderInfo01 orderInfo01, RuntimeContext runtimeContext, RequestIndexer requestIndexer)</span> </span>&#123;</span><br><span class="line">                requestIndexer.add(createIndexRequest(orderInfo01));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="function"><span class="keyword">private</span> IndexRequest <span class="title">createIndexRequest</span><span class="params">(OrderInfo01 orderInfo01)</span> </span>&#123;</span><br><span class="line">                HashMap&lt;String, Object&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">                map.put(<span class="string">&quot;id&quot;</span>, orderInfo01.getId());</span><br><span class="line">                map.put(<span class="string">&quot;province_id&quot;</span>, orderInfo01.getProvince_id());</span><br><span class="line">                map.put(<span class="string">&quot;order_status&quot;</span>, orderInfo01.getOrder_status());</span><br><span class="line">                map.put(<span class="string">&quot;user_id&quot;</span>, orderInfo01.getUser_id());</span><br><span class="line">                map.put(<span class="string">&quot;final_total_amount&quot;</span>, orderInfo01.getFinal_total_amount());</span><br><span class="line">                map.put(<span class="string">&quot;benefit_reduce_amount&quot;</span>, orderInfo01.getBenefit_reduce_amount());</span><br><span class="line">                map.put(<span class="string">&quot;original_total_amount&quot;</span>, orderInfo01.getOriginal_total_amount());</span><br><span class="line">                map.put(<span class="string">&quot;feight_fee&quot;</span>, orderInfo01.getFeight_fee());</span><br><span class="line">                map.put(<span class="string">&quot;expire_time&quot;</span>, orderInfo01.getExpire_time());</span><br><span class="line">                map.put(<span class="string">&quot;create_time&quot;</span>, orderInfo01.getCreate_time());</span><br><span class="line">                map.put(<span class="string">&quot;create_hour&quot;</span>, orderInfo01.getCreate_hour());</span><br><span class="line">                map.put(<span class="string">&quot;if_first_order&quot;</span>, orderInfo01.getIf_first_order());</span><br><span class="line">                map.put(<span class="string">&quot;province_name&quot;</span>, orderInfo01.getProvince_name());</span><br><span class="line">                map.put(<span class="string">&quot;province_area_code&quot;</span>, orderInfo01.getProvince_iso_3166_2());</span><br><span class="line">                map.put(<span class="string">&quot;user_age_group&quot;</span>, orderInfo01.getUser_age_group());</span><br><span class="line">                map.put(<span class="string">&quot;user_gender&quot;</span>, orderInfo01.getUser_gender());</span><br><span class="line">                System.out.println(<span class="string">&quot;data:&quot;</span> + orderInfo01);</span><br><span class="line">                <span class="keyword">return</span> Requests.indexRequest()</span><br><span class="line">                        .index(<span class="string">&quot;gmall2020_order_info_&quot;</span> + format)</span><br><span class="line">                        .type(<span class="string">&quot;_doc&quot;</span>)</span><br><span class="line">                        .id(orderInfo01.getUser_id().toString())</span><br><span class="line">                        .source(map);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        esBuilder.setBulkFlushMaxActions(<span class="number">1</span>);</span><br><span class="line">        esBuilder.setRestClientFactory(<span class="keyword">new</span> util.RestClientFactoryImpl());</span><br><span class="line">        esBuilder.setFailureHandler(<span class="keyword">new</span> RetryRejectedExecutionFailureHandler());</span><br><span class="line">        <span class="keyword">return</span> esBuilder;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>​        在sparkstreaming中，要保持精准一次性消费需要依靠redis来读取和保存kafka偏移量，而flink-kafka端到端exactly once的优势，不言自明。</p>
]]></content>
      <categories>
        <category>实时数仓</category>
      </categories>
  </entry>
  <entry>
    <title>flink实时数仓-dws层双流join</title>
    <url>/2020/12/15/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dws%E5%B1%82%E5%8F%8C%E6%B5%81join/</url>
    <content><![CDATA[<p><img src="/2020/12/15/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dws%E5%B1%82%E5%8F%8C%E6%B5%81join/0.jpg"></p>
<a id="more"></a>

<p>​        在ods进行维度表和事实表关联形成dwd层宽表后，分别可以得到orderinfo和orderdetail两张宽表，这时可以将订单信息和订单详情表从kafka消费并使用flink intervalJoin和ProcessJoinFunction再次进行合流并去重形成订单总表，这样就可以将聚合好的数据写入OLAP例如clickhouse中，方便后续其他实时需求的实现SQL化，同时也可以将合流后的数据再次写入kafka形成dws层宽表方便数据的再次加工，最后实现ads层实时需求。</p>
<p>整体架构：</p>
<p><img src="/2020/12/15/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dws%E5%B1%82%E5%8F%8C%E6%B5%81join/1.jpg"></p>
<p>程序流图：</p>
<p><img src="/2020/12/15/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dws%E5%B1%82%E5%8F%8C%E6%B5%81join/2.jpg"></p>
<p>代码实现  java版：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> dws;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> bean.OrderDetail;</span><br><span class="line"><span class="keyword">import</span> bean.OrderDetailWide;</span><br><span class="line"><span class="keyword">import</span> bean.OrderInfo01;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> lombok.SneakyThrows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FilterFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.RichMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.ConfigConstants;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.RestOptions;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.co.ProcessJoinFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.util.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Copyright(c) 2020-2021 sparrow All Rights Reserved</span></span><br><span class="line"><span class="comment"> * Project: gmall2020-parent</span></span><br><span class="line"><span class="comment"> * Package: dws</span></span><br><span class="line"><span class="comment"> * ClassName: OrderDetailWideApp</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> 18729 created on date: 2020/12/14 16:29</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@version</span> 1.0</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> JDK 1.8</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderDetailWideApp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String kafkaBrokers = <span class="string">&quot;hadoop01:9092&quot;</span>;</span><br><span class="line">        String zkBrokers = <span class="string">&quot;hadoop01:2181,hadoop02:2181,hadoop03:2181&quot;</span>;</span><br><span class="line">        String topic = <span class="string">&quot;DWD_ORDER_INFO&quot;</span>;</span><br><span class="line">        String groupId = <span class="string">&quot;dws_order_info_group&quot;</span>;</span><br><span class="line"></span><br><span class="line">        String topic01=<span class="string">&quot;DWD_ORDER_DETAIL&quot;</span>;</span><br><span class="line">        String groupId01=<span class="string">&quot;dws_order_detail_group&quot;</span>;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;===============》 flink任务开始  ==============》&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.setBoolean(ConfigConstants.LOCAL_START_WEBSERVER, <span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">//自定义端口</span></span><br><span class="line">        conf.setInteger(RestOptions.PORT, <span class="number">4550</span>);</span><br><span class="line">        <span class="comment">//本地env</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(conf);</span><br><span class="line">        env.setParallelism(<span class="number">4</span>);</span><br><span class="line">        <span class="comment">//生产env</span></span><br><span class="line">        <span class="comment">//val env = StreamExecutionEnvironment.getExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span></span><br><span class="line">        <span class="comment">//设置kafka连接参数</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, kafkaBrokers);</span><br><span class="line">        properties.setProperty(<span class="string">&quot;zookeeper.connect&quot;</span>, zkBrokers);</span><br><span class="line">        properties.setProperty(<span class="string">&quot;group.id&quot;</span>, groupId);</span><br><span class="line"></span><br><span class="line">        Properties properties01 = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties01.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, kafkaBrokers);</span><br><span class="line">        properties01.setProperty(<span class="string">&quot;zookeeper.connect&quot;</span>, zkBrokers);</span><br><span class="line">        properties01.setProperty(<span class="string">&quot;group.id&quot;</span>, groupId01);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置时间类型</span></span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">        <span class="comment">//设置检查点时间间隔</span></span><br><span class="line"><span class="comment">//        env.enableCheckpointing(5000);</span></span><br><span class="line"><span class="comment">//        env.setStateBackend( new MemoryStateBackend());</span></span><br><span class="line">        <span class="comment">//创建kafak消费者，获取kafak中的数据</span></span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; kafkaConsumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(topic, <span class="keyword">new</span> SimpleStringSchema(), properties);</span><br><span class="line">        DataStreamSource&lt;String&gt; kafkaData = env.addSource(kafkaConsumer);</span><br><span class="line">        DataStream&lt;String&gt; orderInfo = kafkaData.map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line"><span class="comment">//                System.out.println(&quot;&gt;&gt;&gt;&gt;&gt;&gt;接收topic报文:&quot;+s+&quot;&lt;&lt;&lt;&lt;&lt;&quot;);</span></span><br><span class="line">                <span class="keyword">return</span> s;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; kafkaConsumer01 = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(topic01, <span class="keyword">new</span> SimpleStringSchema(), properties01);</span><br><span class="line">        DataStreamSource&lt;String&gt; kafkaData01 = env.addSource(kafkaConsumer01);</span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; orderDetail = kafkaData01.map(<span class="keyword">new</span> RichMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">//                System.out.println(&quot;&gt;&gt;&gt;&gt;&gt;&gt;接收topic报文:&quot; + value + &quot;&lt;&lt;&lt;&lt;&lt;&quot;);</span></span><br><span class="line">                <span class="keyword">return</span> value;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//        双流join</span></span><br><span class="line">        SingleOutputStreamOperator&lt;OrderInfo01&gt; orderInfo01 = orderInfo.map(<span class="keyword">new</span> RichMapFunction&lt;String, OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> OrderInfo01 <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                OrderInfo01 orderInfo = (OrderInfo01) JSON.parseObject(value, Class.forName(<span class="string">&quot;bean.OrderInfo01&quot;</span>));</span><br><span class="line">                <span class="keyword">return</span> orderInfo;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@SneakyThrows</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(OrderInfo01 element)</span> </span>&#123;</span><br><span class="line">                SimpleDateFormat simpleDateFormat = <span class="keyword">new</span> SimpleDateFormat(<span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>);</span><br><span class="line">                Date date = simpleDateFormat.parse(element.getCreate_time());</span><br><span class="line">                <span class="keyword">long</span> time = date.getTime();</span><br><span class="line">                <span class="keyword">return</span> time * <span class="number">1000L</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).filter(<span class="keyword">new</span> FilterFunction&lt;OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(OrderInfo01 value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value.getOrder_status().equals(<span class="string">&quot;1004&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;OrderDetail&gt; orderDetail01 = (SingleOutputStreamOperator&lt;OrderDetail&gt;) orderDetail.map(<span class="keyword">new</span> RichMapFunction&lt;String, OrderDetail&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> OrderDetail <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                OrderDetail orderDetail = (OrderDetail) JSON.parseObject(value, Class.forName(<span class="string">&quot;bean.OrderDetail&quot;</span>));</span><br><span class="line">                <span class="keyword">return</span> orderDetail;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;OrderDetail&gt;() &#123;</span><br><span class="line">            <span class="meta">@SneakyThrows</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(OrderDetail element)</span> </span>&#123;</span><br><span class="line">                SimpleDateFormat simpleDateFormat = <span class="keyword">new</span> SimpleDateFormat(<span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>);</span><br><span class="line">                Date date = simpleDateFormat.parse(element.getCreate_time());</span><br><span class="line">                <span class="keyword">long</span> time = date.getTime();</span><br><span class="line">                <span class="keyword">return</span> time * <span class="number">1000L</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        orderInfo01</span><br><span class="line">                .keyBy(OrderInfo01::getId)</span><br><span class="line">                .intervalJoin(orderDetail01.keyBy(OrderDetail::getOrder_id))</span><br><span class="line">                .between(Time.minutes(-<span class="number">5</span>), Time.minutes(<span class="number">5</span>))</span><br><span class="line">                .process(<span class="keyword">new</span> TimeJoinFunction())</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="comment">//        订单金额分摊 略</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//        写入clickhouse 略</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//        .print();</span></span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeJoinFunction</span> <span class="keyword">extends</span> <span class="title">ProcessJoinFunction</span>&lt;<span class="title">OrderInfo01</span>,<span class="title">OrderDetail</span>,<span class="title">OrderDetailWide</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(OrderInfo01 left, OrderDetail right, Context ctx, Collector&lt;OrderDetailWide&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            out.collect(<span class="keyword">new</span> OrderDetailWide(left,right));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>webUI反压监控正常：</p>
<p><img src="/2020/12/15/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dws%E5%B1%82%E5%8F%8C%E6%B5%81join/3.jpg"></p>
<p>dws层写入clickhouse</p>
]]></content>
      <categories>
        <category>实时数仓</category>
      </categories>
  </entry>
  <entry>
    <title>flink读取mq数据实时流式写入hive并合并hdfs分区小文件</title>
    <url>/2021/07/08/flink%E8%AF%BB%E5%8F%96mq%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E6%B5%81%E5%BC%8F%E5%86%99%E5%85%A5hive%E5%B9%B6%E5%90%88%E5%B9%B6hdfs%E5%88%86%E5%8C%BA%E5%B0%8F%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<p><img src="/2021/07/08/flink%E8%AF%BB%E5%8F%96mq%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E6%B5%81%E5%BC%8F%E5%86%99%E5%85%A5hive%E5%B9%B6%E5%90%88%E5%B9%B6hdfs%E5%88%86%E5%8C%BA%E5%B0%8F%E6%96%87%E4%BB%B6/00.jpg"></p>
<a id="more"></a>

<p>source以及sink连接参考flink官网不在赘述。</p>
<p>数据从mq读入flink然后定义hive sink流式写入，最后也在hdfs查到了写入的分区文件，但是到hive却查不到数据，观察源码：<code>org.apache.flink.table.filesystem.stream.PartitionTimeCommitTrigger </code>中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;String&gt; <span class="title">committablePartitions</span><span class="params">(<span class="keyword">long</span> checkpointId)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!watermarks.containsKey(checkpointId)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(</span><br><span class="line">                String.format(</span><br><span class="line">                        <span class="string">&quot;Checkpoint(%d) has not been snapshot. The watermark information is: %s.&quot;</span>,</span><br><span class="line">                        checkpointId, watermarks));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> watermark = watermarks.get(checkpointId);</span><br><span class="line">    watermarks.headMap(checkpointId, <span class="keyword">true</span>).clear();</span><br><span class="line"></span><br><span class="line">    List&lt;String&gt; needCommit = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    Iterator&lt;String&gt; iter = pendingPartitions.iterator();</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">        String partition = iter.next();</span><br><span class="line">        LocalDateTime partTime =</span><br><span class="line">                extractor.extract(partitionKeys, extractPartitionValues(<span class="keyword">new</span> Path(partition)));</span><br><span class="line">        <span class="keyword">if</span> (watermark &gt; toMills(partTime) + commitDelay) &#123;</span><br><span class="line">            needCommit.add(partition);</span><br><span class="line">            iter.remove();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> needCommit;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>if (watermark &gt; toMills(partTime) + commitDelay)</code>这里点进toMills方法，发现并没有进行时区处理，所以我们重写这个类，在extract方法抽取分区的时候减去8小时，并在创建hive表时使用自定义分区提交抽取，这样才能提交分区。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> LocalDateTime <span class="title">extract</span><span class="params">(List&lt;String&gt; partitionKeys, List&lt;String&gt; partitionValues)</span> </span>&#123;</span><br><span class="line">    String timestampString;</span><br><span class="line">    <span class="keyword">if</span> (pattern == <span class="keyword">null</span>) &#123;</span><br><span class="line">        timestampString = partitionValues.get(<span class="number">0</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        timestampString = pattern;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; partitionKeys.size(); i++) &#123;</span><br><span class="line">            timestampString =</span><br><span class="line">                    timestampString.replaceAll(</span><br><span class="line">                            <span class="string">&quot;\\$&quot;</span> + partitionKeys.get(i), partitionValues.get(i));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> toLocalDateTime(timestampString);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> LocalDateTime <span class="title">toLocalDateTime</span><span class="params">(String timestampString)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> LocalDateTime.parse(timestampString, TIMESTAMP_FORMATTER);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (DateTimeParseException e) &#123;</span><br><span class="line">        <span class="keyword">return</span> LocalDateTime.of(</span><br><span class="line">                LocalDate.parse(timestampString, DATE_FORMATTER), LocalTime.MIDNIGHT);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">long</span> <span class="title">toMills</span><span class="params">(LocalDateTime dateTime)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> TimestampData.fromLocalDateTime(dateTime).getMillisecond();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">long</span> <span class="title">toMills</span><span class="params">(String timestampString)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> toMills(toLocalDateTime(timestampString));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>重写类的extract方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Licensed to the Apache Software Foundation (ASF) under one</span></span><br><span class="line"><span class="comment"> * or more contributor license agreements.  See the NOTICE file</span></span><br><span class="line"><span class="comment"> * distributed with this work for additional information</span></span><br><span class="line"><span class="comment"> * regarding copyright ownership.  The ASF licenses this file</span></span><br><span class="line"><span class="comment"> * to you under the Apache License, Version 2.0 (the</span></span><br><span class="line"><span class="comment"> * &quot;License&quot;); you may not use this file except in compliance</span></span><br><span class="line"><span class="comment"> * with the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"> * See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"> * limitations under the License.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> org.apache.flink.table.filesystem;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.data.TimestampData;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.annotation.Nullable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.LocalDate;</span><br><span class="line"><span class="keyword">import</span> java.time.LocalDateTime;</span><br><span class="line"><span class="keyword">import</span> java.time.LocalTime;</span><br><span class="line"><span class="keyword">import</span> java.time.format.DateTimeFormatter;</span><br><span class="line"><span class="keyword">import</span> java.time.format.DateTimeFormatterBuilder;</span><br><span class="line"><span class="keyword">import</span> java.time.format.DateTimeParseException;</span><br><span class="line"><span class="keyword">import</span> java.time.format.ResolverStyle;</span><br><span class="line"><span class="keyword">import</span> java.time.format.SignStyle;</span><br><span class="line"><span class="keyword">import</span> java.time.temporal.ChronoField;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> java.time.temporal.ChronoField.DAY_OF_MONTH;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> java.time.temporal.ChronoField.HOUR_OF_DAY;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> java.time.temporal.ChronoField.MINUTE_OF_HOUR;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> java.time.temporal.ChronoField.MONTH_OF_YEAR;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> java.time.temporal.ChronoField.SECOND_OF_MINUTE;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> java.time.temporal.ChronoField.YEAR;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Default &#123;<span class="doctag">@link</span> PartitionTimeExtractor&#125;. See &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment"> * FileSystemOptions#PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN&#125;.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DefaultPartTimeExtractor</span> <span class="keyword">implements</span> <span class="title">PartitionTimeExtractor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> DateTimeFormatter TIMESTAMP_FORMATTER =</span><br><span class="line">            <span class="keyword">new</span> DateTimeFormatterBuilder()</span><br><span class="line">                    .appendValue(YEAR, <span class="number">1</span>, <span class="number">10</span>, SignStyle.NORMAL)</span><br><span class="line">                    .appendLiteral(<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">                    .appendValue(MONTH_OF_YEAR, <span class="number">1</span>, <span class="number">2</span>, SignStyle.NORMAL)</span><br><span class="line">                    .appendLiteral(<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">                    .appendValue(DAY_OF_MONTH, <span class="number">1</span>, <span class="number">2</span>, SignStyle.NORMAL)</span><br><span class="line">                    .optionalStart()</span><br><span class="line">                    .appendLiteral(<span class="string">&quot; &quot;</span>)</span><br><span class="line">                    .appendValue(HOUR_OF_DAY, <span class="number">1</span>, <span class="number">2</span>, SignStyle.NORMAL)</span><br><span class="line">                    .appendLiteral(<span class="string">&#x27;:&#x27;</span>)</span><br><span class="line">                    .appendValue(MINUTE_OF_HOUR, <span class="number">1</span>, <span class="number">2</span>, SignStyle.NORMAL)</span><br><span class="line">                    .appendLiteral(<span class="string">&#x27;:&#x27;</span>)</span><br><span class="line">                    .appendValue(SECOND_OF_MINUTE, <span class="number">1</span>, <span class="number">2</span>, SignStyle.NORMAL)</span><br><span class="line">                    .optionalStart()</span><br><span class="line">                    .appendFraction(ChronoField.NANO_OF_SECOND, <span class="number">1</span>, <span class="number">9</span>, <span class="keyword">true</span>)</span><br><span class="line">                    .optionalEnd()</span><br><span class="line">                    .optionalEnd()</span><br><span class="line">                    .toFormatter()</span><br><span class="line">                    .withResolverStyle(ResolverStyle.LENIENT);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> DateTimeFormatter DATE_FORMATTER =</span><br><span class="line">            <span class="keyword">new</span> DateTimeFormatterBuilder()</span><br><span class="line">                    .appendValue(YEAR, <span class="number">1</span>, <span class="number">10</span>, SignStyle.NORMAL)</span><br><span class="line">                    .appendLiteral(<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">                    .appendValue(MONTH_OF_YEAR, <span class="number">1</span>, <span class="number">2</span>, SignStyle.NORMAL)</span><br><span class="line">                    .appendLiteral(<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">                    .appendValue(DAY_OF_MONTH, <span class="number">1</span>, <span class="number">2</span>, SignStyle.NORMAL)</span><br><span class="line">                    .toFormatter()</span><br><span class="line">                    .withResolverStyle(ResolverStyle.LENIENT);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Nullable</span> <span class="keyword">private</span> <span class="keyword">final</span> String pattern;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DefaultPartTimeExtractor</span><span class="params">(<span class="meta">@Nullable</span> String pattern)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.pattern = pattern;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> LocalDateTime <span class="title">extract</span><span class="params">(List&lt;String&gt; partitionKeys, List&lt;String&gt; partitionValues)</span> </span>&#123;</span><br><span class="line">        String timestampString;</span><br><span class="line">        <span class="keyword">if</span> (pattern == <span class="keyword">null</span>) &#123;</span><br><span class="line">            timestampString = partitionValues.get(<span class="number">0</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            timestampString = pattern;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; partitionKeys.size(); i++) &#123;</span><br><span class="line">                timestampString =</span><br><span class="line">                        timestampString.replaceAll(</span><br><span class="line">                                <span class="string">&quot;\\$&quot;</span> + partitionKeys.get(i), partitionValues.get(i));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> toLocalDateTime(timestampString).plusHours(-<span class="number">8</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> LocalDateTime <span class="title">toLocalDateTime</span><span class="params">(String timestampString)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> LocalDateTime.parse(timestampString, TIMESTAMP_FORMATTER);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (DateTimeParseException e) &#123;</span><br><span class="line">            <span class="keyword">return</span> LocalDateTime.of(</span><br><span class="line">                    LocalDate.parse(timestampString, DATE_FORMATTER), LocalTime.MIDNIGHT);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">long</span> <span class="title">toMills</span><span class="params">(LocalDateTime dateTime)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TimestampData.fromLocalDateTime(dateTime).getMillisecond();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">long</span> <span class="title">toMills</span><span class="params">(String timestampString)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> toMills(toLocalDateTime(timestampString));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>我们查看官网也可以发现时区的问题：</p>
<p><img src="/2021/07/08/flink%E8%AF%BB%E5%8F%96mq%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E6%B5%81%E5%BC%8F%E5%86%99%E5%85%A5hive%E5%B9%B6%E5%90%88%E5%B9%B6hdfs%E5%88%86%E5%8C%BA%E5%B0%8F%E6%96%87%E4%BB%B6/0.jpg"></p>
<p>创建三级/二级/一级分区hive表:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">external</span> <span class="keyword">TABLE</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> awsLog</span><br><span class="line">(</span><br><span class="line">    actionsExecuted        <span class="keyword">STRING</span>,</span><br><span class="line">    chosenCertArn          <span class="keyword">STRING</span>,</span><br><span class="line">    classification         <span class="keyword">STRING</span>,</span><br><span class="line">    classificationReasonf  <span class="keyword">STRING</span>,</span><br><span class="line">    clientPort             <span class="keyword">STRING</span>,</span><br><span class="line">    domainName             <span class="keyword">STRING</span>,</span><br><span class="line">    elb                    <span class="keyword">STRING</span>,</span><br><span class="line">    elbStatusCode          <span class="keyword">STRING</span>,</span><br><span class="line">    errorReason            <span class="keyword">STRING</span>,</span><br><span class="line">    matchedRulePriority    <span class="keyword">STRING</span>,</span><br><span class="line">    receivedBytes          <span class="keyword">STRING</span>,</span><br><span class="line">    redirectUrl            <span class="keyword">STRING</span>,</span><br><span class="line">    request                <span class="keyword">STRING</span>,</span><br><span class="line">    requestCreationTime    <span class="keyword">STRING</span>,</span><br><span class="line">    requestProcessingTime  <span class="keyword">STRING</span>,</span><br><span class="line">    responseProcessingTime <span class="keyword">STRING</span>,</span><br><span class="line">    sentBytes              <span class="keyword">STRING</span>,</span><br><span class="line">    sslCipher              <span class="keyword">STRING</span>,</span><br><span class="line">    sslProtocol            <span class="keyword">STRING</span>,</span><br><span class="line">    targetGroupArn         <span class="keyword">STRING</span>,</span><br><span class="line">    targetPort             <span class="keyword">STRING</span>,</span><br><span class="line">    targetPortList         <span class="keyword">STRING</span>,</span><br><span class="line">    targetProcessingTime   <span class="keyword">STRING</span>,</span><br><span class="line">    targetStatusCode       <span class="keyword">STRING</span>,</span><br><span class="line">    targetStatusCodeList   <span class="keyword">STRING</span>,</span><br><span class="line">    <span class="string">`time`</span>                 <span class="keyword">STRING</span>,</span><br><span class="line">    traceId                <span class="keyword">STRING</span>,</span><br><span class="line">    <span class="keyword">type</span>                   <span class="keyword">STRING</span>,</span><br><span class="line">    userAgent              <span class="keyword">STRING</span></span><br><span class="line">) partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>,hr <span class="keyword">string</span>,mm <span class="keyword">string</span>)</span><br><span class="line">    <span class="keyword">stored</span> <span class="keyword">as</span> PARQUET</span><br><span class="line">    location <span class="string">&#x27;/sunqi/awsLog/&#x27;</span></span><br><span class="line">    TBLPROPERTIES (</span><br><span class="line">        <span class="string">&#x27;partition.time-extractor.kind&#x27;</span> = <span class="string">&#x27;custom&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;partition.time-extractor.timestamp-pattern&#x27;</span> = <span class="string">&#x27;$dt $hr:$mm:00&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;partition.time-extractor.class&#x27;</span> = <span class="string">&#x27;com.topsky.aws.userlogin.functions.MyPartTimeExtractor&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;sink.partition-commit.policy.kind&#x27;</span> = <span class="string">&#x27;metastore,success-file,custom&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;sink.partition-commit.policy.class&#x27;</span> = <span class="string">&#x27;com.topsky.aws.userlogin.functions.ParquetFileMergingCommitPolicy&#x27;</span></span><br><span class="line">        );</span><br></pre></td></tr></table></figure>

<p>hive查到数据后，问题又来了，数据量很大时，发现hive每个分区出现了大量的小文件：</p>
<p><img src="/2021/07/08/flink%E8%AF%BB%E5%8F%96mq%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E6%B5%81%E5%BC%8F%E5%86%99%E5%85%A5hive%E5%B9%B6%E5%90%88%E5%B9%B6hdfs%E5%88%86%E5%8C%BA%E5%B0%8F%E6%96%87%E4%BB%B6/1.jpg"></p>
<p>所以这里需要自定义分区一个分区提交策略，将textfile手动合并为parquet列式存储文件，这里参考网上一位大佬的demo：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> me.lmagics.flinkexp.hiveintegration.util;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> org.apache.flink.hive.shaded.parquet.example.data.Group;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.hive.shaded.parquet.hadoop.ParquetFileReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.hive.shaded.parquet.hadoop.ParquetFileWriter.Mode;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.hive.shaded.parquet.hadoop.ParquetReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.hive.shaded.parquet.hadoop.ParquetWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.hive.shaded.parquet.hadoop.example.ExampleParquetWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.hive.shaded.parquet.hadoop.example.GroupReadSupport;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.hive.shaded.parquet.hadoop.metadata.CompressionCodecName;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.hive.shaded.parquet.hadoop.metadata.ParquetMetadata;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.hive.shaded.parquet.hadoop.util.HadoopInputFile;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.hive.shaded.parquet.schema.MessageType;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.filesystem.PartitionCommitPolicy;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.LocatedFileStatus;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ParquetFileMergingCommitPolicy</span> <span class="keyword">implements</span> <span class="title">PartitionCommitPolicy</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(ParquetFileMergingCommitPolicy.class);</span><br><span class="line"> </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commit</span><span class="params">(Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">    FileSystem fs = FileSystem.get(conf);</span><br><span class="line">    String partitionPath = context.partitionPath().getPath();</span><br><span class="line"> </span><br><span class="line">    List&lt;Path&gt; files = listAllFiles(fs, <span class="keyword">new</span> Path(partitionPath), <span class="string">&quot;part-&quot;</span>);</span><br><span class="line">    LOGGER.info(<span class="string">&quot;&#123;&#125; files in path &#123;&#125;&quot;</span>, files.size(), partitionPath);</span><br><span class="line"> </span><br><span class="line">    MessageType schema = getParquetSchema(files, conf);</span><br><span class="line">    <span class="keyword">if</span> (schema == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    LOGGER.info(<span class="string">&quot;Fetched parquet schema: &#123;&#125;&quot;</span>, schema.toString());</span><br><span class="line"> </span><br><span class="line">    Path result = merge(partitionPath, schema, files, fs);</span><br><span class="line">    LOGGER.info(<span class="string">&quot;Files merged into &#123;&#125;&quot;</span>, result.toString());</span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="function"><span class="keyword">private</span> List&lt;Path&gt; <span class="title">listAllFiles</span><span class="params">(FileSystem fs, Path dir, String prefix)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    List&lt;Path&gt; result = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"> </span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; dirIterator = fs.listFiles(dir, <span class="keyword">false</span>);</span><br><span class="line">    <span class="keyword">while</span> (dirIterator.hasNext()) &#123;</span><br><span class="line">      LocatedFileStatus fileStatus = dirIterator.next();</span><br><span class="line">      Path filePath = fileStatus.getPath();</span><br><span class="line">      <span class="keyword">if</span> (fileStatus.isFile() &amp;&amp; filePath.getName().startsWith(prefix)) &#123;</span><br><span class="line">        result.add(filePath);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="function"><span class="keyword">private</span> MessageType <span class="title">getParquetSchema</span><span class="params">(List&lt;Path&gt; files, Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (files.size() == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    HadoopInputFile inputFile = HadoopInputFile.fromPath(files.get(<span class="number">0</span>), conf);</span><br><span class="line">    ParquetFileReader reader = ParquetFileReader.open(inputFile);</span><br><span class="line">    ParquetMetadata metadata = reader.getFooter();</span><br><span class="line">    MessageType schema = metadata.getFileMetaData().getSchema();</span><br><span class="line"> </span><br><span class="line">    reader.close();</span><br><span class="line">    <span class="keyword">return</span> schema;</span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="function"><span class="keyword">private</span> Path <span class="title">merge</span><span class="params">(String partitionPath, MessageType schema, List&lt;Path&gt; files, FileSystem fs)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Path mergeDest = <span class="keyword">new</span> Path(partitionPath + <span class="string">&quot;/result-&quot;</span> + System.currentTimeMillis() + <span class="string">&quot;.parquet&quot;</span>);</span><br><span class="line">    ParquetWriter&lt;Group&gt; writer = ExampleParquetWriter.builder(mergeDest)</span><br><span class="line">      .withType(schema)</span><br><span class="line">      .withConf(fs.getConf())</span><br><span class="line">      .withWriteMode(Mode.CREATE)</span><br><span class="line">      .withCompressionCodec(CompressionCodecName.SNAPPY)</span><br><span class="line">      .build();</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span> (Path file : files) &#123;</span><br><span class="line">      ParquetReader&lt;Group&gt; reader = ParquetReader.builder(<span class="keyword">new</span> GroupReadSupport(), file)</span><br><span class="line">        .withConf(fs.getConf())</span><br><span class="line">        .build();</span><br><span class="line">      Group data;</span><br><span class="line">      <span class="keyword">while</span>((data = reader.read()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">        writer.write(data);</span><br><span class="line">      &#125;</span><br><span class="line">      reader.close();</span><br><span class="line">    &#125;</span><br><span class="line">    writer.close();</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span> (Path file : files) &#123;</span><br><span class="line">      fs.delete(file, <span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> mergeDest;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>再次写入，问题得到解决：</p>
<p><img src="/2021/07/08/flink%E8%AF%BB%E5%8F%96mq%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E6%B5%81%E5%BC%8F%E5%86%99%E5%85%A5hive%E5%B9%B6%E5%90%88%E5%B9%B6hdfs%E5%88%86%E5%8C%BA%E5%B0%8F%E6%96%87%E4%BB%B6/2.jpg"></p>
<p>分区提交策略说明：</p>
<p><img src="/2021/07/08/flink%E8%AF%BB%E5%8F%96mq%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E6%B5%81%E5%BC%8F%E5%86%99%E5%85%A5hive%E5%B9%B6%E5%90%88%E5%B9%B6hdfs%E5%88%86%E5%8C%BA%E5%B0%8F%E6%96%87%E4%BB%B6/3.jpg"></p>
<p>参考官网：</p>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/table/hive/overview/">https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/table/hive/overview/</a></p>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/table/filesystem/">https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/table/filesystem/</a></p>
<p>参考文章：</p>
<p><a href="https://blog.csdn.net/nazeniwaresakini/article/details/107811860">https://blog.csdn.net/nazeniwaresakini/article/details/107811860</a></p>
<p><a href="https://blog.csdn.net/m0_37592814/article/details/108044830">https://blog.csdn.net/m0_37592814/article/details/108044830</a></p>
]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title>hdfs故障恢复及原理</title>
    <url>/2021/02/21/hdfs%E6%95%85%E9%9A%9C%E6%81%A2%E5%A4%8D%E5%8F%8A%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p><img src="http://hadoop.apache.org/docs/r1.0.4/cn/images/hadoop-logo.jpg" alt="Hadoop"></p>
<a id="more"></a>

<h3 id="2NN工作机制"><a href="#2NN工作机制" class="headerlink" title="2NN工作机制"></a>2NN工作机制</h3><p>Fsimage：hdfs文件系统元数据的一个永久性检查点。</p>
<p>Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。</p>
<p><img src="/2021/02/21/hdfs%E6%95%85%E9%9A%9C%E6%81%A2%E5%A4%8D%E5%8F%8A%E5%8E%9F%E7%90%86/1.jpg"></p>
<ol>
<li>NameNode启动，滚动Edits并生成一个空的edits.inprogress，并加载Edits和Fsimage到内存中，持有最新的元数据信息。</li>
<li>Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询操作不会被记录，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息并在内存中执行元数据的增删改的操作。</li>
<li>SecondaryNameNode询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。</li>
<li>SecondaryNameNode执行CheckPoint，NameNode滚动Edits并生成一个新的edits.inprogress并将以后所有新的操作都写入edits.inprogress</li>
<li>其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint</li>
<li>然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。</li>
<li>NameNode在启动时就只需要加载新的edits.inprogress和新Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。</li>
</ol>
<h3 id="CheckPoint时间设置"><a href="#CheckPoint时间设置" class="headerlink" title="CheckPoint时间设置"></a>CheckPoint时间设置</h3><ul>
<li><p>通常情况下，SecondaryNameNode每隔一小时执行一次 [hdfs-default.xml] 。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span> &gt;</span></span><br></pre></td></tr></table></figure>



</li>
</ul>
<h3 id="NameNode故障处理"><a href="#NameNode故障处理" class="headerlink" title="NameNode故障处理"></a>NameNode故障处理</h3><p>NameNode故障后，可以采用如下两种方法恢复数据。</p>
<ul>
<li>将SecondaryNameNode中数据拷贝到NameNode存储数据的目录</li>
<li>使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。</li>
</ul>
<h3 id="Hadoop宕机"><a href="#Hadoop宕机" class="headerlink" title="Hadoop宕机"></a>Hadoop宕机</h3><ul>
<li>如果MR造成系统宕机。此时要控制Yarn同时运行的任务数，和每个任务申请的最大内存。调整参数：yarn.scheduler.maximum-allocation-mb（单个任务可申请的最多物理内存量，默认是8192MB）</li>
<li>如果写入文件过快造成NameNode宕机。那么调高Kafka的存储大小，控制从Kafka到HDFS的写入速度。例如，可以调整Flume每批次拉取数据量的大小参数batchsize。</li>
</ul>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/09/23/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<a id="more"></a>

<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>mysql炸裂函数实例</title>
    <url>/2021/03/20/mysql%E7%82%B8%E8%A3%82%E5%87%BD%E6%95%B0%E5%AE%9E%E4%BE%8B/</url>
    <content><![CDATA[<p><img src="/2021/03/20/mysql%E7%82%B8%E8%A3%82%E5%87%BD%E6%95%B0%E5%AE%9E%E4%BE%8B/1.jpg"></p>
<a id="more"></a>

<p>首先需要了解一个函数：</p>
<p><code>substring_index（“待截取字符串”，“截取规则”，截取前n个字符），如果第三个参数为负数，则倒过来取前n个</code></p>
<p>举例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 取第一个:==&gt;得到结果为： 15</span></span><br><span class="line"><span class="keyword">SELECT</span> SUBSTRING_INDEX(<span class="string">&#x27;15,151,152,16&#x27;</span>,<span class="string">&#x27;,&#x27;</span>,<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># N可以为负数，表示倒数前N个字符串。（有负号的时候，可以将整个字符倒过来看）==&gt;得到结果为： 16</span></span><br><span class="line"><span class="keyword">SELECT</span> SUBSTRING_INDEX(<span class="string">&#x27;15,151,152,16&#x27;</span>,<span class="string">&#x27;,&#x27;</span>,<span class="number">-1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取前2个字符串，然后倒过来取第一个 ==&gt;得到结果为： 151</span></span><br><span class="line"><span class="keyword">SELECT</span> SUBSTRING_INDEX(SUBSTRING_INDEX(<span class="string">&#x27;15,151,152,16&#x27;</span>,<span class="string">&#x27;,&#x27;</span>,<span class="number">2</span>),<span class="string">&#x27;,&#x27;</span>,<span class="number">-1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 倒过来取前两个字符串，再正数第一个:==&gt; 得到结果为：152</span></span><br><span class="line"><span class="keyword">SELECT</span> SUBSTRING_INDEX(SUBSTRING_INDEX(<span class="string">&#x27;15,151,152,16&#x27;</span>,<span class="string">&#x27;,&#x27;</span>,<span class="number">-2</span>),<span class="string">&#x27;,&#x27;</span>,<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<p>如果有一个字符数组，我们想分别取第一个，第二个，第三个字符：</p>
<p><code>user1,user2,user3</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">substring_index(substring_index(processed_data,&#x27;,&#x27;,1),&#x27;,&#x27;,-1),</span><br><span class="line">substring_index(substring_index(processed_data,&#x27;,&#x27;,2),&#x27;,&#x27;,-1),</span><br><span class="line">substring_index(substring_index(processed_data,&#x27;,&#x27;,3),&#x27;,&#x27;,-1)</span><br></pre></td></tr></table></figure>

<p>以此类推第n个字符：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">substring_index(substring_index(processed_data,&#x27;,&#x27;,n),&#x27;,&#x27;,-1)</span><br></pre></td></tr></table></figure>

<p>如果我们需要把前n个字符都取出来，那就要借助一个足够长从0单调递增的数列，把&lt;=n的所有字符都提取出来</p>
<p>创建一个只有一列的单调递增表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">procedure</span> test_200w;</span><br><span class="line"></span><br><span class="line">DELIMITER $$</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">PROCEDURE</span> test_200w(<span class="keyword">IN</span> in_count <span class="built_in">INT</span>) <span class="comment"># 创建存储过程</span></span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">    <span class="keyword">DECLARE</span> <span class="keyword">COUNT</span> <span class="built_in">INT</span> <span class="keyword">DEFAULT</span> <span class="number">1</span>;</span><br><span class="line"><span class="comment"># DECLARE SUM INT DEFAULT 0;</span></span><br><span class="line">    WHILE COUNT &lt;= in_count</span><br><span class="line">        <span class="keyword">DO</span></span><br><span class="line">            <span class="comment"># SET SUM = SUM + COUNT;</span></span><br><span class="line">            <span class="keyword">insert</span> <span class="keyword">into</span> test_200w <span class="keyword">values</span> (<span class="keyword">COUNT</span>);</span><br><span class="line">            <span class="keyword">SET</span> <span class="keyword">COUNT</span> = <span class="keyword">COUNT</span> + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">END</span> <span class="keyword">WHILE</span>;</span><br><span class="line"><span class="keyword">END</span> $$</span><br><span class="line">DELIMITER ;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> test_200w;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_200w</span><br><span class="line">(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">bigint</span> primary <span class="keyword">key</span> <span class="keyword">default</span> <span class="number">0</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">call</span> test_200w(<span class="number">200000</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> *</span><br><span class="line"><span class="keyword">from</span> test_200w;</span><br></pre></td></tr></table></figure>

<p>下面就来利用这个表进行炸裂：</p>
<p>需求1：炸裂email</p>
<p>示例数据中的email字段示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[</span><br><span class="line">  &quot;last@roechling.com&quot;,</span><br><span class="line">  &quot;rep@roechling.com.sg&quot;,</span><br><span class="line">  &quot;info@roechling.com&quot;,</span><br><span class="line">  &quot;bewerbung@roechling.com&quot;,</span><br><span class="line">  &quot;brown@roechling.com&quot;,</span><br><span class="line">  &quot;bkoennecker@roechling.com&quot;,</span><br><span class="line">  &quot;info-xan@roechling.com&quot;,</span><br><span class="line">  &quot;ikhammarit@roechling.com&quot;,</span><br><span class="line">  &quot;dwalulik@roechling.com&quot;,</span><br><span class="line">  &quot;reg_thermo@roechling.com.sg&quot;,</span><br><span class="line">  &quot;ina.breitsprecher@roechling.com&quot;,</span><br><span class="line">  &quot;bewerbung.debre@roechling.com&quot;,</span><br><span class="line">  &quot;robaproducts@roechling.com&quot;,</span><br><span class="line">  &quot;reckerstorfer@roechling.com&quot;,</span><br><span class="line">  &quot;aluger@roechling.com&quot;,</span><br><span class="line">  &quot;datenschutz@roechling.com&quot;,</span><br><span class="line">  &quot;info.deneu@roechling.com&quot;,</span><br><span class="line">  &quot;rep@roechling.com&quot;,</span><br><span class="line">  &quot;zentrale.deneu@roechling.com&quot;,</span><br><span class="line">  &quot;hr-oep@roechling.com&quot;,</span><br><span class="line">  &quot;apueschel@roechling.com&quot;,</span><br><span class="line">  &quot;seminar-management@roechling.com&quot;,</span><br><span class="line">  &quot;admin@roechling.com.sg&quot;,</span><br><span class="line">  &quot;ibaumgaertel@roechling.com&quot;,</span><br><span class="line">  &quot;personal.waldachtal@roechling.com&quot;,</span><br><span class="line">  &quot;kbiedebach@roechling.com&quot;,</span><br><span class="line">  &quot;mfritzmann@roechling.com&quot;,</span><br><span class="line">  &quot;sales.deneu@roechling.com&quot;,</span><br><span class="line">  &quot;nikolaus_niepon@roechling.com&quot;,</span><br><span class="line">  &quot;lsieber@roechling.com&quot;,</span><br><span class="line">  &quot;d-jane@roechling.com&quot;,</span><br><span class="line">  &quot;rep@roechling.com.cn&quot;,</span><br><span class="line">  &quot;pstraub@roechling.com&quot;,</span><br><span class="line">  &quot;jane_d@roechling.com&quot;,</span><br><span class="line">  &quot;info@roechling.com.cn&quot;,</span><br><span class="line">  &quot;peter.walsh@roechling.com.email&quot;,</span><br><span class="line">  &quot;breitsprecher@roechling.com&quot;,</span><br><span class="line">  &quot;susanne.salomon@roechling.com&quot;,</span><br><span class="line">  &quot;johannes.salmuth@roechling.com&quot;,</span><br><span class="line">  &quot;angela.lin@roechling.com.sg&quot;,</span><br><span class="line">  &quot;gabriele.stegher@roechling.com&quot;,</span><br><span class="line">  &quot;jermel.jones@roechling.com&quot;,</span><br><span class="line">  &quot;rega@roechling.com&quot;,</span><br><span class="line">  &quot;lewis.carter@roechling.com&quot;,</span><br><span class="line">  &quot;kgina.breitsprecher@roechling.com&quot;,</span><br><span class="line">  &quot;jane.doe@roechling.com&quot;,</span><br><span class="line">  &quot;martina.anschuetz@roechling.com&quot;,</span><br><span class="line">  &quot;jtoepfer-linss@roechling.com&quot;,</span><br><span class="line">  &quot;peter.walsh@roechling.com&quot;,</span><br><span class="line">  &quot;witherspoon@roechling.com&quot;,</span><br><span class="line">  &quot;cachedkeegan@roechling.com&quot;,</span><br><span class="line">  &quot;chris@roechling.com&quot;,</span><br><span class="line">  &quot;chris_krebs@roechling.com&quot;,</span><br><span class="line">  &quot;heidi@roechling.com.email&quot;,</span><br><span class="line">  &quot;abernathy@roechling.com&quot;,</span><br><span class="line">  &quot;ludger@roechling.com&quot;,</span><br><span class="line">  &quot;liangtong@roechling.com.sg&quot;,</span><br><span class="line">  &quot;jtoepfer-linss@roechling.com.mit&quot;,</span><br><span class="line">  &quot;rep@roechling.com.company&quot;,</span><br><span class="line">  &quot;plastics.rep@roechling.com.sg&quot;,</span><br><span class="line">  &quot;bewerbung.deneu@roechling.com&quot;,</span><br><span class="line">  &quot;jtoepfer-linss@roechling.com.sv&quot;,</span><br><span class="line">  &quot;ludger.bartels@roechling.com&quot;,</span><br><span class="line">  &quot;admin@roechling.com&quot;,</span><br><span class="line">  &quot;james_andolina@roechling.com&quot;,</span><br><span class="line">  &quot;darren@roechling.com&quot;,</span><br><span class="line">  &quot;linlin.ng@roechling.com.sg&quot;,</span><br><span class="line">  &quot;cachedrep@roechling.com.sg&quot;,</span><br><span class="line">  &quot;georg.duffner@roechling.com&quot;,</span><br><span class="line">  &quot;km.pang@roechling.com.sg&quot;,</span><br><span class="line">  &quot;j-d@roechling.com&quot;,</span><br><span class="line">  &quot;djane@roechling.com&quot;,</span><br><span class="line">  &quot;odell@roechling.com&quot;,</span><br><span class="line">  &quot;info.debre@roechling.com&quot;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>这里首先要对数据进行清洗：清洗掉’[‘,’]’,’”‘,然后在join单调表，取id&lt;=字符条数的所有字符进行炸裂</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 公司名称,</span><br><span class="line">               地址,</span><br><span class="line">               网址,</span><br><span class="line">               电话,</span><br><span class="line">               主要管理人,</span><br><span class="line">               雇员人数,</span><br><span class="line">               成立时间,</span><br><span class="line">               substring_index(substring_index(processed_data, <span class="string">&#x27;,&#x27;</span>, b.id), <span class="string">&#x27;,&#x27;</span>, - <span class="number">1</span>) 邮箱</span><br><span class="line">        <span class="keyword">FROM</span> (</span><br><span class="line">            <span class="keyword">SELECT</span> 公司名称,</span><br><span class="line">            地址,</span><br><span class="line">            网址,</span><br><span class="line">            电话,</span><br><span class="line">            主要管理人,</span><br><span class="line">            雇员人数,</span><br><span class="line">            成立时间,</span><br><span class="line">            <span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(邮箱, <span class="string">&#x27;[&#x27;</span>, <span class="string">&#x27;&#x27;</span>), <span class="string">&#x27;]&#x27;</span>, <span class="string">&#x27;&#x27;</span>), <span class="string">&#x27;&quot;&#x27;</span>, <span class="string">&#x27;&#x27;</span>), <span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;&#x27;</span>),</span><br><span class="line">                            <span class="built_in">CHAR</span>(<span class="number">10</span>), <span class="string">&#x27;&#x27;</span>), <span class="built_in">CHAR</span>(<span class="number">13</span>), <span class="string">&#x27;&#x27;</span>) processed_data</span><br><span class="line">            <span class="keyword">FROM</span> company_all</span><br><span class="line">        ) temp</span><br><span class="line">                 <span class="keyword">JOIN</span> test_200w b</span><br><span class="line">                      <span class="keyword">ON</span> b.id &lt;= (<span class="keyword">length</span>(temp.processed_data) - <span class="keyword">length</span>(<span class="keyword">REPLACE</span>(temp.processed_data, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;&#x27;</span>)) + <span class="number">1</span>)</span><br><span class="line">       <span class="keyword">union</span> <span class="keyword">all</span> <span class="keyword">select</span> * <span class="keyword">from</span> company_all <span class="keyword">where</span> 邮箱 <span class="keyword">is</span> <span class="literal">null</span>;</span><br></pre></td></tr></table></figure>

<p>这里有一点需要特别注意：如果有的列email为null，那么这些列的length也是null，长度是无法作差的，所以会丢失数据，这里需要加上。结果如下：</p>
<p><img src="/2021/03/20/mysql%E7%82%B8%E8%A3%82%E5%87%BD%E6%95%B0%E5%AE%9E%E4%BE%8B/2.jpg"></p>
<p>需求2：炸裂email并取前5</p>
<p>方案一：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">     <span class="keyword">SELECT</span> 公司名称,</span><br><span class="line">            地址,</span><br><span class="line">            网址,</span><br><span class="line">            电话,</span><br><span class="line">            主要管理人,</span><br><span class="line">            雇员人数,</span><br><span class="line">            成立时间,</span><br><span class="line">            substring_index(substring_index(processed_data, <span class="string">&#x27;,&#x27;</span>, b.id), <span class="string">&#x27;,&#x27;</span>, <span class="number">-1</span>) 邮箱</span><br><span class="line">     <span class="keyword">FROM</span> (</span><br><span class="line">              <span class="keyword">SELECT</span> 公司名称,</span><br><span class="line">                     地址,</span><br><span class="line">                     网址,</span><br><span class="line">                     电话,</span><br><span class="line">                     主要管理人,</span><br><span class="line">                     雇员人数,</span><br><span class="line">                     成立时间,</span><br><span class="line">                     substring_index(<span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(邮箱, <span class="string">&#x27;[&#x27;</span>, <span class="string">&#x27;&#x27;</span>), <span class="string">&#x27;]&#x27;</span>, <span class="string">&#x27;&#x27;</span>), <span class="string">&#x27;&quot;&#x27;</span>, <span class="string">&#x27;&#x27;</span>),<span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;&#x27;</span>), <span class="built_in">CHAR</span>(<span class="number">10</span>), <span class="string">&#x27;&#x27;</span>), <span class="built_in">CHAR</span>(<span class="number">13</span>), <span class="string">&#x27;&#x27;</span>), <span class="string">&#x27;,&#x27;</span>,<span class="number">5</span>) processed_data</span><br><span class="line">              <span class="keyword">FROM</span> company_all</span><br><span class="line">          ) temp</span><br><span class="line">              <span class="keyword">JOIN</span> test_200w b</span><br><span class="line">                   <span class="keyword">ON</span> b.id &lt;= (<span class="keyword">length</span>(temp.processed_data) - <span class="keyword">length</span>(<span class="keyword">REPLACE</span>(temp.processed_data, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;&#x27;</span>))+<span class="number">1</span>)</span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span> <span class="keyword">select</span> * <span class="keyword">from</span> company_all <span class="keyword">where</span> 邮箱 <span class="keyword">is</span> <span class="literal">null</span>;</span><br></pre></td></tr></table></figure>

<p>方案二：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 公司名称,</span><br><span class="line">       地址,</span><br><span class="line">       网址,</span><br><span class="line">       电话,</span><br><span class="line">       主要管理人,</span><br><span class="line">       雇员人数,</span><br><span class="line">       成立时间,</span><br><span class="line">       substring_index(substring_index(processed_data, <span class="string">&#x27;,&#x27;</span>, b.id), <span class="string">&#x27;,&#x27;</span>, - <span class="number">1</span>) 邮箱</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">         <span class="keyword">SELECT</span> 公司名称,</span><br><span class="line">                地址,</span><br><span class="line">                网址,</span><br><span class="line">                电话,</span><br><span class="line">                主要管理人,</span><br><span class="line">                雇员人数,</span><br><span class="line">                成立时间,</span><br><span class="line">                <span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(<span class="keyword">REPLACE</span>(邮箱, <span class="string">&#x27;[&#x27;</span>, <span class="string">&#x27;&#x27;</span>), <span class="string">&#x27;]&#x27;</span>, <span class="string">&#x27;&#x27;</span>), <span class="string">&#x27;&quot;&#x27;</span>, <span class="string">&#x27;&#x27;</span>), <span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;&#x27;</span>),</span><br><span class="line">                                <span class="built_in">CHAR</span>(<span class="number">10</span>), <span class="string">&#x27;&#x27;</span>), <span class="built_in">CHAR</span>(<span class="number">13</span>), <span class="string">&#x27;&#x27;</span>) processed_data</span><br><span class="line">         <span class="keyword">FROM</span> company_all</span><br><span class="line">     ) temp</span><br><span class="line">         <span class="keyword">JOIN</span> test_200w b <span class="keyword">ON</span> b.id &lt;= (<span class="number">5</span>)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> 公司名称, 地址, 网址, 电话, 主要管理人, 雇员人数, 成立时间,</span><br><span class="line">         substring_index(substring_index(processed_data, <span class="string">&#x27;,&#x27;</span>, b.id), <span class="string">&#x27;,&#x27;</span>, - <span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<p>效果如下：</p>
<p><img src="/2021/03/20/mysql%E7%82%B8%E8%A3%82%E5%87%BD%E6%95%B0%E5%AE%9E%E4%BE%8B/3.jpg"></p>
]]></content>
  </entry>
  <entry>
    <title>spark RDD原理</title>
    <url>/2020/09/26/spark%20RDD%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p><img src="/2020/09/26/spark%20RDD%E5%8E%9F%E7%90%86/2.png"></p>
<a id="more"></a>

<p>​            首先我觉得原理性的东西，重要的在于了解整个业务的运行流程。虽然在面试中狠问底层这是区分面试者的重要手段，但在实际工作中，最重要的还是提高业务运行的效率。当然知道框架运行的原理越细说明面试者对底层运行接触的时间就越久。然而人的精力是有限的。深入源码固然能提升能力。这里只对原理做一个简单的阐述。</p>
<p><img src="/2020/09/26/spark%20RDD%E5%8E%9F%E7%90%86/1.png"></p>
<p>​            整个spark RDD运行在资源调度系统yarn上，运行代码由spark-submit提交到rm，rm将业务交给nm上的driver，driver会把业务交由executors具体去执行，executors上进行RDD弹性分布式计算，相较于MapReduce，shufflemapRDDs之间一般省去了shuffle落盘的过程，这也是spark速度比mr引擎快的原因。    </p>
<p><img src="/2020/09/26/spark%20RDD%E5%8E%9F%E7%90%86/1.jpg"></p>
<p><img src="/2020/09/26/spark%20RDD%E5%8E%9F%E7%90%86/2.jpg">    </p>
<p>​        我们知道spark RDD算子包括transformation算子和action算子，运算过程主要发生map阶段，经过reduce聚合后action算子进行提交job任务，而transformation算子是懒加载的，也就是说，前面的运算过程并不会立即执行，application需要action算子进行触发，action算子的数量决定了job的个数，action算子中的DAGScheduler会对job任务进行切分，并寻找依赖关系，根据是否是宽依赖来确定stag个数。job的task个数由一个stage阶段最后一个RDD的partition个数决定。</p>
<p>​        spark application的运行效率取决于数据是否均衡，可以通过合理的分区来确保各个分区数据的均衡，常用的分区方案有：hash分区，range分区，自定义分区，也可以在初始化sc时指定分区。</p>
<p>​        数据均衡可以使transformation算子在各个分区完成时间相差减小以提高运行效率，常见的转换算子分为value型，双value交互，key-value型，在不同的业务场景下选择不同的运算组合以达到最佳的效率是设计者的初衷。</p>
<p>​        未完待续。。。</p>
]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title>spark core-top10热门品类</title>
    <url>/2020/09/28/spark-core-top10%E7%83%AD%E9%97%A8%E5%93%81%E7%B1%BB/</url>
    <content><![CDATA[<p>需求说明：品类是指产品的分类，大型电商网站品类分多级，咱们的项目中品类只有一级，不同的公司可能对热门的定义不一样。我们按照每个品类的点击、下单、支付的量来统计热门品类。</p>
<p>鞋            点击数 下单数  支付数</p>
<p>衣服        点击数 下单数  支付数</p>
<p>电脑        点击数 下单数  支付数</p>
<p>本项目需求优化为：先按照点击数排名，靠前的就排名高；如果点击数相同，再比较下单数；下单数再相同，就比较支付数。</p>
<a id="more"></a>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//用户访问动作表</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UserVisitAction</span>(<span class="params">date: <span class="type">String</span>,//用户点击行为的日期</span></span></span><br><span class="line"><span class="class"><span class="params">                           user_id: <span class="type">Long</span>,//用户的<span class="type">ID</span></span></span></span><br><span class="line"><span class="class"><span class="params">                        <span class="type">UserVisitAction</span>   session_id: <span class="type">String</span>,//<span class="type">Session</span>的<span class="type">ID</span></span></span></span><br><span class="line"><span class="class"><span class="params">                           page_id: <span class="type">Long</span>,//某个页面的<span class="type">ID</span></span></span></span><br><span class="line"><span class="class"><span class="params">                           action_time: <span class="type">String</span>,//动作的时间点</span></span></span><br><span class="line"><span class="class"><span class="params">                           search_keyword: <span class="type">String</span>,//用户搜索的关键词</span></span></span><br><span class="line"><span class="class"><span class="params">                           click_category_id: <span class="type">Long</span>,//某一个商品品类的<span class="type">ID</span></span></span></span><br><span class="line"><span class="class"><span class="params">                           click_product_id: <span class="type">Long</span>,//某一个商品的<span class="type">ID</span></span></span></span><br><span class="line"><span class="class"><span class="params">                           order_category_ids: <span class="type">String</span>,//一次订单中所有品类的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="class"><span class="params">                           order_product_ids: <span class="type">String</span>,//一次订单中所有商品的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="class"><span class="params">                           pay_category_ids: <span class="type">String</span>,//一次支付中所有品类的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="class"><span class="params">                           pay_product_ids: <span class="type">String</span>,//一次支付中所有商品的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="class"><span class="params">                           city_id: <span class="type">Long</span></span>)<span class="title">//城市</span> <span class="title">id</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">输出结果表</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">CategoryCountInfo</span>(<span class="params">var categoryId: <span class="type">String</span>,//品类id</span></span></span><br><span class="line"><span class="class"><span class="params">                             var clickCount: <span class="type">Long</span>,//点击次数</span></span></span><br><span class="line"><span class="class"><span class="params">                             var orderCount: <span class="type">Long</span>,//订单次数</span></span></span><br><span class="line"><span class="class"><span class="params">                             var payCount: <span class="type">Long</span></span>)<span class="title">//支付次数</span></span></span><br></pre></td></tr></table></figure>



<p>数据（局部）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2019-07-17_95_26070e87-1ad7-49a3-8fb3-cc741facaddf_6_2019-07-17 00:00:17_null_19_85_null_null_null_null_7</span><br><span class="line">2019-07-17_38_6502cdc9-cf95-4b08-8854-f03a25baa917_29_2019-07-17 00:00:19_null_12_36_null_null_null_null_5</span><br><span class="line">2019-07-17_38_6502cdc9-cf95-4b08-8854-f03a25baa917_22_2019-07-17 00:00:28_null_-1_-1_null_null_15,1,20,6,4_15,88,75_9</span><br><span class="line">2019-07-17_38_6502cdc9-cf95-4b08-8854-f03a25baa917_11_2019-07-17 00:00:29_苹果_-1_-1_null_null_null_null_7</span><br><span class="line">2019-07-17_38_6502cdc9-cf95-4b08-8854-f03a25baa917_24_2019-07-17 00:00:38_null_-1_-1_15,13,5,11,8_99,2_null_null_10</span><br><span class="line">2019-07-17_38_6502cdc9-cf95-4b08-8854-f03a25baa917_24_2019-07-17 00:00:48_null_19_44_null_null_null_null_4</span><br><span class="line">2019-07-17_38_6502cdc9-cf95-4b08-8854-f03a25baa917_47_2019-07-17 00:00:54_null_14_79_null_null_null_null_2</span><br></pre></td></tr></table></figure>



<p>根据上面提供的类，很显然是将各个字段封装的上面提供的两个类中，然后进行运算调用。</p>
<p>思路：</p>
<p>1，首先要将数据按照_切分，得到各个字段对应的值并封装到UserVisitAction对象中</p>
<p>2，对封装好的UserVisitAction对象进行进一步封装，得到CategoryCountInfo对象并打散分布</p>
<p>3，根据商品id对数据进行聚合，最后排序并取前十输出</p>
<p>伪代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.textFile(<span class="string">&quot;input02&quot;</span>)</span><br><span class="line">  .map(line=&gt;&#123;</span><br><span class="line">    <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">    <span class="type">UserVisitAction</span>(</span><br><span class="line">      datas(<span class="number">0</span>),</span><br><span class="line">      datas(<span class="number">1</span>).toLong,</span><br><span class="line">      datas(<span class="number">2</span>),</span><br><span class="line">      datas(<span class="number">3</span>).toLong,</span><br><span class="line">      datas(<span class="number">4</span>),</span><br><span class="line">      datas(<span class="number">5</span>),</span><br><span class="line">      datas(<span class="number">6</span>).toLong,</span><br><span class="line">      datas(<span class="number">7</span>).toLong,</span><br><span class="line">      datas(<span class="number">8</span>),</span><br><span class="line">      datas(<span class="number">9</span>),</span><br><span class="line">      datas(<span class="number">10</span>),</span><br><span class="line">      datas(<span class="number">11</span>),</span><br><span class="line">      datas(<span class="number">12</span>).toLong</span><br><span class="line">    )</span><br><span class="line">  &#125;)</span><br><span class="line">  .flatMap(a=&gt;(</span><br><span class="line">    <span class="keyword">if</span> (a.click_category_id != <span class="number">-1</span>)&#123;</span><br><span class="line">      <span class="type">List</span>(<span class="type">CategoryCountInfo</span>(a.click_category_id.toString,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    &#125;<span class="keyword">else</span> <span class="keyword">if</span>(a.order_category_ids!=<span class="string">&quot;null&quot;</span>)&#123;</span><br><span class="line">      <span class="keyword">val</span> infoesToInfoes: <span class="type">ListBuffer</span>[<span class="type">CategoryCountInfo</span>] =<span class="keyword">new</span>  <span class="type">ListBuffer</span>[<span class="type">CategoryCountInfo</span>]</span><br><span class="line">      <span class="keyword">val</span> strings: <span class="type">Array</span>[<span class="type">String</span>] = a.order_category_ids.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">      <span class="keyword">for</span> (id&lt;-strings)&#123;</span><br><span class="line">        infoesToInfoes.append(<span class="type">CategoryCountInfo</span>(id,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>))</span><br><span class="line">      &#125;</span><br><span class="line">      infoesToInfoes</span><br><span class="line">    &#125;<span class="keyword">else</span> <span class="keyword">if</span> (a.pay_category_ids!=<span class="string">&quot;null&quot;</span>)&#123;</span><br><span class="line">      <span class="keyword">val</span> infoesToInfoes: <span class="type">ListBuffer</span>[<span class="type">CategoryCountInfo</span>] =<span class="keyword">new</span>  <span class="type">ListBuffer</span>[<span class="type">CategoryCountInfo</span>]</span><br><span class="line">      <span class="keyword">val</span> strings: <span class="type">Array</span>[<span class="type">String</span>] = a.pay_category_ids.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">      <span class="keyword">for</span> (id&lt;-strings)&#123;</span><br><span class="line">        infoesToInfoes.append(<span class="type">CategoryCountInfo</span>(id,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">      &#125;</span><br><span class="line">      infoesToInfoes</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">      <span class="type">Nil</span></span><br><span class="line">    &#125;</span><br><span class="line">  ))</span><br><span class="line">  .groupBy(info=&gt;info.categoryId)</span><br><span class="line">  .mapValues(a=&gt;a.reduce(</span><br><span class="line">    (a,b)=&gt;&#123;</span><br><span class="line">      a.orderCount=a.orderCount+b.orderCount</span><br><span class="line">      a.clickCount=a.clickCount+b.clickCount</span><br><span class="line">      a.payCount=a.payCount+b.payCount</span><br><span class="line">      a</span><br><span class="line">    &#125;</span><br><span class="line">  ))</span><br><span class="line">  .map(_._2)</span><br><span class="line">  .sortBy(a=&gt;(a.clickCount,a.orderCount,a.payCount),<span class="literal">false</span>)</span><br><span class="line">  .take(<span class="number">10</span>).foreach(println)</span><br></pre></td></tr></table></figure>



<p>输出结果：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">6</span>,<span class="number">5912</span>,<span class="number">1768</span>,<span class="number">1197</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">16</span>,<span class="number">5928</span>,<span class="number">1782</span>,<span class="number">1233</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">4</span>,<span class="number">5961</span>,<span class="number">1760</span>,<span class="number">1271</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">14</span>,<span class="number">5964</span>,<span class="number">1773</span>,<span class="number">1171</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">8</span>,<span class="number">5974</span>,<span class="number">1736</span>,<span class="number">1238</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">3</span>,<span class="number">5975</span>,<span class="number">1749</span>,<span class="number">1192</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">1</span>,<span class="number">5976</span>,<span class="number">1766</span>,<span class="number">1191</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">10</span>,<span class="number">5991</span>,<span class="number">1757</span>,<span class="number">1174</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">5</span>,<span class="number">6011</span>,<span class="number">1820</span>,<span class="number">1132</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">18</span>,<span class="number">6024</span>,<span class="number">1754</span>,<span class="number">1197</span>)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/09/28/spark-core-top10%E7%83%AD%E9%97%A8%E5%93%81%E7%B1%BB/1.png" alt="DAG Visualization"></p>
]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title>spark核心函数实例-广告点击Top3</title>
    <url>/2020/09/24/spark-core-%E5%B9%BF%E5%91%8A%E7%82%B9%E5%87%BBTop3/</url>
    <content><![CDATA[<p>案例需求：统计每个省份广告被点击次数的top3</p>
<a id="more"></a>

<p><img src="/2020/09/24/spark-core-%E5%B9%BF%E5%91%8A%E7%82%B9%E5%87%BBTop3/1.png"></p>
<p>最终希望输出效果：</p>
<p><img src="/2020/09/24/spark-core-%E5%B9%BF%E5%91%8A%E7%82%B9%E5%87%BBTop3/2.jpg"></p>
<p>思考：样本中需要提取出省份，广告，还有统计出每个省份每个广告种类点击的次数。然后聚合出广告出现的次数并做排名截取前三名。</p>
<p>代码实现1：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.textFile(<span class="string">&quot;input/agent.log&quot;</span>)</span><br><span class="line">  .map(line=&gt;(line.split(<span class="string">&quot; &quot;</span>)(<span class="number">1</span>)+<span class="string">&quot;-&quot;</span>+line.split(<span class="string">&quot; &quot;</span>)(<span class="number">4</span>),<span class="number">1</span>))</span><br><span class="line">  .reduceByKey(_+_)</span><br><span class="line">  .map(a=&gt;(a._1.split(<span class="string">&quot;-&quot;</span>)(<span class="number">0</span>),(a._1.split(<span class="string">&quot;-&quot;</span>)(<span class="number">1</span>),a._2)))</span><br><span class="line">  .groupByKey()</span><br><span class="line">  .mapValues(data=&gt;data.toList.sortWith((a,b)=&gt;(a._2&gt;b._2)).take(<span class="number">3</span>))</span><br><span class="line">  .collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p><img src="/2020/09/24/spark-core-%E5%B9%BF%E5%91%8A%E7%82%B9%E5%87%BBTop3/4.jpg" alt="DAG Visualization"></p>
<p>代码实现2：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.textFile(<span class="string">&quot;input/agent.log&quot;</span>)</span><br><span class="line">  .map(line=&gt;((line.split(<span class="string">&quot; &quot;</span>)(<span class="number">1</span>),line.split(<span class="string">&quot; &quot;</span>)(<span class="number">4</span>)),<span class="number">1</span>))</span><br><span class="line">  .reduceByKey((a,b)=&gt;(a+b))</span><br><span class="line">  .groupBy(_._1._1)</span><br><span class="line">  .mapValues(a=&gt;a.toList.sortWith(_._2&gt;_._2).take(<span class="number">3</span>))</span><br><span class="line">  .collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p><img src="/2020/09/24/spark-core-%E5%B9%BF%E5%91%8A%E7%82%B9%E5%87%BBTop3/5.jpg" alt="DAG Visualization"></p>
<p>分析实现：</p>
<p><img src="/2020/09/24/spark-core-%E5%B9%BF%E5%91%8A%E7%82%B9%E5%87%BBTop3/3.jpg"></p>
]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title>sqoop在实际生产中的使用经验总结</title>
    <url>/2021/05/26/sqoop%E5%9C%A8%E5%AE%9E%E9%99%85%E7%94%9F%E4%BA%A7%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p><img src="http://sqoop.apache.org/images/sqoop-logo.png" alt="Sqoop"></p>
<a id="more"></a>

<h4 id="sqoop原理："><a href="#sqoop原理：" class="headerlink" title="sqoop原理："></a>sqoop原理：</h4><p><img src="/2021/05/26/sqoop%E5%9C%A8%E5%AE%9E%E9%99%85%E7%94%9F%E4%BA%A7%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/1.jpg"></p>
<p>详细介绍：</p>
<p><a href="http://sqoop.apache.org/">http://sqoop.apache.org/</a></p>
<h2 id="1、数据安全"><a href="#1、数据安全" class="headerlink" title="1、数据安全"></a>1、数据安全</h2><blockquote>
<p>获取帮助：hadoop credential</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop credential create mysql.digger.prod.r.alias [-value password] -provider jceks://hdfs/mysql/password/mysql.digger.prod.r.jceks</span><br></pre></td></tr></table></figure>

<p><a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_importing_data_into_hive">http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_importing_data_into_hive</a></p>
<blockquote>
<p><strong>Protecting password from preying eyes.</strong> Hadoop 2.6.0 provides an API to separate password storage from applications. This API is called the credential provided API and there is a new <code>credential</code> command line tool to manage passwords and their aliases. The passwords are stored with their aliases in a keystore that is password protected. The keystore password can be the provided to a password prompt on the command line, via an environment variable or defaulted to a software defined constant. Please check the Hadoop documentation on the usage of this facility.</p>
<p>Once the password is stored using the Credential Provider facility and the Hadoop configuration has been suitably updated, all applications can optionally use the alias in place of the actual password and at runtime resolve the alias for the password to use.</p>
<p>Since the keystore or similar technology used for storing the credential provider is shared across components, passwords for various applications, various database and other passwords can be securely stored in them and only the alias needs to be exposed in configuration files, protecting the password from being visible.</p>
<p>Sqoop has been enhanced to allow usage of this funcionality if it is available in the underlying Hadoop version being used. One new option has been introduced to provide the alias on the command line instead of the actual password (–password-alias). The argument value this option is the alias on the storage associated with the actual password. Example usage is as sqoop脚本：</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-Dhadoop.security.credential.provider.path=jceks://hdfs/mysql/password/mysql.digger.prod.r.jceks \</span><br><span class="line">--password-alias mysql.digger.prod.r.alias \</span><br></pre></td></tr></table></figure>





<h2 id="2、数据核对"><a href="#2、数据核对" class="headerlink" title="2、数据核对"></a>2、数据核对</h2><p><strong><em>presto</em></strong>：跨数据源join</p>
<p>java代码自动生成核对sql：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">presto_for_iglobalwin</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;String&gt; array = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        InputStream in = Test01.class.getClassLoader().getResourceAsStream(<span class="string">&quot;druid1.properties&quot;</span>);</span><br><span class="line">        properties.load(in);</span><br><span class="line">        in.close();</span><br><span class="line">        DataSource dataSource = DruidDataSourceFactory.createDataSource(properties);</span><br><span class="line">        Connection connection = dataSource.getConnection();</span><br><span class="line">        System.out.println(connection);</span><br><span class="line">        String sql = <span class="string">&quot;show tables&quot;</span>;</span><br><span class="line">        PreparedStatement preparedStatement = connection.prepareStatement(sql);</span><br><span class="line">        ResultSet resultSet = preparedStatement.executeQuery();</span><br><span class="line">        <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">            array.add(resultSet.getString(<span class="string">&quot;Tables_in_skytree_test&quot;</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        connection.close();</span><br><span class="line"></span><br><span class="line">        String s3=<span class="string">&quot;hive.skytree.&quot;</span>;</span><br><span class="line">        String s4=<span class="string">&quot;connector2.skytree_test.&quot;</span>;</span><br><span class="line"></span><br><span class="line">        Iterator&lt;String&gt; iterator = array.iterator();</span><br><span class="line">        <span class="keyword">while</span> (iterator.hasNext())&#123;</span><br><span class="line">            String next = iterator.next();</span><br><span class="line">            System.out.println(<span class="string">&quot;select &quot;</span>+<span class="string">&quot;&#x27;&quot;</span>+next+<span class="string">&quot;&#x27;&quot;</span>+<span class="string">&quot;,t1.c1,t2.c1,if(t1.c1=t2.c1,1,0),(t1.c1-t2.c1)/(t2.c1+0.0001) from (select count(*) as c1 from ( select  * from &quot;</span>+s3+<span class="string">&quot;ods_&quot;</span>+next+<span class="string">&quot; union select *,&#x27;2021-05-07&#x27; from &quot;</span>+s4+next+<span class="string">&quot; )t0) t1, (select count(*) as c1 from &quot;</span>+s4+next+<span class="string">&quot;)t2 union all&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="数据不一致："><a href="#数据不一致：" class="headerlink" title="数据不一致："></a>数据不一致：</h3><h4 id="1、tinyint（1）自动转化为-bit类型"><a href="#1、tinyint（1）自动转化为-bit类型" class="headerlink" title="1、tinyint（1）自动转化为 bit类型"></a>1、tinyint（1）自动转化为 bit类型</h4><p>解决方案：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">jdbc:mysql://rm-xxx.mysql.rds.aliyuncs.com:3306/digger?zeroDateTimeBehavior=convertToNull&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&amp;tinyInt1isBit=false</span><br></pre></td></tr></table></figure>

<p><a href="https://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html#_mysql_import_of_tinyint_1_from_mysql_behaves_strangely">https://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html#_mysql_import_of_tinyint_1_from_mysql_behaves_strangely</a></p>
<blockquote>
<p>官网原文：<br>27.2.5. MySQL: Import of TINYINT(1) from MySQL behaves strangely<br>Problem: Sqoop is treating TINYINT(1) columns as booleans, which is for example causing issues with HIVE import. This is because by default the MySQL JDBC connector maps the TINYINT(1) to java.sql.Types.BIT, which Sqoop by default maps to Boolean.</p>
<p>Solution: A more clean solution is to force MySQL JDBC Connector to stop converting TINYINT(1) to java.sql.Types.BIT by adding tinyInt1isBit=false into your JDBC path (to create something like jdbc:mysql://localhost/test?tinyInt1isBit=false). Another solution would be to explicitly override the column mapping for the datatype TINYINT(1) column. For example, if the column name is foo, then pass the following option to Sqoop during import: –map-column-hive foo=tinyint. In the case of non-Hive imports to HDFS, use –map-column-java foo=integer.</p>
</blockquote>
<h4 id="2、字段中出现回车，换行"><a href="#2、字段中出现回车，换行" class="headerlink" title="2、字段中出现回车，换行"></a>2、字段中出现回车，换行</h4><p>导致同步到hive后数据行数不对，且内容错乱。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--hive-drop-import-delims \</span><br></pre></td></tr></table></figure>

<p><a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_importing_data_into_hive">http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_importing_data_into_hive</a></p>
<table>
<thead>
<tr>
<th align="center">Argument</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>--hive-partition-value &lt;v&gt;</code></td>
<td align="center">String-value that serves as partition key for this imported into hive in this job.</td>
</tr>
<tr>
<td align="center"><code>--map-column-hive &lt;map&gt;</code></td>
<td align="center">Override default mapping from SQL type to Hive type for configured columns. If specify commas in this argument, use URL encoded keys and values, for example, use DECIMAL(1%2C%201) instead of DECIMAL(1, 1).</td>
</tr>
</tbody></table>
<h4 id="3、null一致性问题"><a href="#3、null一致性问题" class="headerlink" title="3、null一致性问题"></a>3、null一致性问题</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sqoop import  ... --null-string <span class="string">&#x27;\\N&#x27;</span> --null-non-string <span class="string">&#x27;\\N&#x27;</span></span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>Sqoop will by default import NULL values as string <code>null</code>. Hive is however using string <code>\N</code> to denote <code>NULL</code> values and therefore predicates dealing with <code>NULL</code> (like <code>IS NULL</code>) will not work correctly. You should append parameters <code>--null-string</code> and <code>--null-non-string</code> in case of import job or <code>--input-null-string</code> and <code>--input-null-non-string</code> in case of an export job if you wish to properly preserve <code>NULL</code> values. Because sqoop is using those parameters in generated code, you need to properly escape value <code>\N</code> to <code>\\N</code>:</p>
</blockquote>
<h2 id="3、数据压缩"><a href="#3、数据压缩" class="headerlink" title="3、数据压缩"></a>3、数据压缩</h2><p>–compression-codec Snappy \</p>
<p><a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_importing_data_into_hive">http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_importing_data_into_hive</a></p>
<blockquote>
<p>7.2.11. File Formats</p>
<p>By default, data is not compressed. You can compress your data by using the deflate (gzip) algorithm with the <code>-z</code> or <code>--compress</code> argument, or specify any Hadoop compression codec using the <code>--compression-codec</code> argument. This applies to SequenceFile, text, and Avro files.</p>
</blockquote>
<h2 id="4、同步策略"><a href="#4、同步策略" class="headerlink" title="4、同步策略"></a>4、同步策略</h2><p>sql语句实现：全量，增量，新增及变化，特殊，与后面hive中周期型快照事实表/全量同步维表，事务性事实表/拉链表/累计型快照事实表、特殊表相对应。</p>
<h5 id="新增及变化："><a href="#新增及变化：" class="headerlink" title="新增及变化："></a>新增及变化：</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,</span><br><span class="line">       final_total_amount,</span><br><span class="line">       order_status,</span><br><span class="line">       user_id,</span><br><span class="line">       out_trade_no,</span><br><span class="line">       create_time,</span><br><span class="line">       operate_time,</span><br><span class="line">       province_id,</span><br><span class="line">       benefit_reduce_amount,</span><br><span class="line">       original_total_amount,</span><br><span class="line">       feight_fee</span><br><span class="line"><span class="keyword">from</span> order_info</span><br><span class="line"><span class="keyword">where</span> (<span class="keyword">date_format</span>(create_time, <span class="string">&#x27;%Y-%m-%d&#x27;</span>) = <span class="string">&#x27;$do_date&#x27;</span></span><br><span class="line">    <span class="keyword">or</span> <span class="keyword">date_format</span>(operate_time, <span class="string">&#x27;%Y-%m-%d&#x27;</span>) = <span class="string">&#x27;$do_date&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h5 id="增量："><a href="#增量：" class="headerlink" title="增量："></a>增量：</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> business_ads_report <span class="keyword">where</span> <span class="built_in">date</span>=$do_date</span><br></pre></td></tr></table></figure>

<h5 id="全量："><a href="#全量：" class="headerlink" title="全量："></a>全量：</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> web_site_analytics_report_daily <span class="keyword">where</span> <span class="number">1</span>=<span class="number">1</span></span><br></pre></td></tr></table></figure>

<h5 id="特殊："><a href="#特殊：" class="headerlink" title="特殊："></a>特殊：</h5><p>同步sql和全量一样，shell/python同步函数只在第一次调用</p>
<h2 id="5、提高并行度和高并发"><a href="#5、提高并行度和高并发" class="headerlink" title="5、提高并行度和高并发"></a>5、提高并行度和高并发</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-Dmapreduce.job.queuename=spark \</span><br><span class="line">-m,--num-mappers &lt;n&gt; \</span><br><span class="line">--split-by [int] \</span><br></pre></td></tr></table></figure>

<blockquote>
<h4 id="7-2-4-Controlling-Parallelism"><a href="#7-2-4-Controlling-Parallelism" class="headerlink" title="7.2.4. Controlling Parallelism"></a>7.2.4. Controlling Parallelism</h4><p>Sqoop imports data in parallel from most database sources. You can specify the number of map tasks (parallel processes) to use to perform the import by using the <code>-m</code> or <code>--num-mappers</code> argument. Each of these arguments takes an integer value which corresponds to the degree of parallelism to employ. By default, four tasks are used. Some databases may see improved performance by increasing this value to 8 or 16. Do not increase the degree of parallelism greater than that available within your MapReduce cluster; tasks will run serially and will likely increase the amount of time required to perform the import. Likewise, do not increase the degree of parallism higher than that which your database can reasonably support. Connecting 100 concurrent clients to your database may increase the load on the database server to a point where performance suffers as a result.</p>
<p>When performing parallel imports, Sqoop needs a criterion by which it can split the workload. Sqoop uses a <em>splitting column</em> to split the workload. By default, Sqoop will identify the primary key column (if present) in a table and use it as the splitting column. The low and high values for the splitting column are retrieved from the database, and the map tasks operate on evenly-sized components of the total range. For example, if you had a table with a primary key column of <code>id</code> whose minimum value was 0 and maximum value was 1000, and Sqoop was directed to use 4 tasks, Sqoop would run four processes which each execute SQL statements of the form <code>SELECT * FROM sometable WHERE id &gt;= lo AND id &lt; hi</code>, with <code>(lo, hi)</code> set to (0, 250), (250, 500), (500, 750), and (750, 1001) in the different tasks.</p>
<p>If the actual values for the primary key are not uniformly distributed across its range, then this can result in unbalanced tasks. You should explicitly choose a different column with the <code>--split-by</code> argument. For example, <code>--split-by employee_id</code>. Sqoop cannot currently split on multi-column indices. If your table has no index column, or has a multi-column key, then you must also manually choose a splitting column.</p>
<p>User can override the <code>--num-mapers</code> by using <code>--split-limit</code> option. Using the <code>--split-limit</code> parameter places a limit on the size of the split section created. If the size of the split created is larger than the size specified in this parameter, then the splits would be resized to fit within this limit, and the number of splits will change according to that.This affects actual number of mappers. If size of a split calculated based on provided <code>--num-mappers</code> parameter exceeds <code>--split-limit</code> parameter then actual number of mappers will be increased.If the value specified in <code>--split-limit</code> parameter is 0 or negative, the parameter will be ignored altogether and the split size will be calculated according to the number of mappers.</p>
<p>If a table does not have a primary key defined and the <code>--split-by &lt;col&gt;</code> is not provided, then import will fail unless the number of mappers is explicitly set to one with the <code>--num-mappers 1</code> option or the <code>--autoreset-to-one-mapper</code> option is used. The option <code>--autoreset-to-one-mapper</code> is typically used with the import-all-tables tool to automatically handle tables without a primary key in a schema.</p>
</blockquote>
<p><img src="/2021/05/26/sqoop%E5%9C%A8%E5%AE%9E%E9%99%85%E7%94%9F%E4%BA%A7%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/3.jpg"></p>
<h2 id="样例shell函数："><a href="#样例shell函数：" class="headerlink" title="样例shell函数："></a>样例shell函数：</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">import_data()&#123;</span><br><span class="line">sqoop1 import \</span><br><span class="line">-Dmapreduce.job.queuename=storm \</span><br><span class="line">-Dhadoop.security.credential.provider.path=jceks://hdfs/mysql/password/mysql.digger.prod.r.jceks \</span><br><span class="line">--connect &quot;jdbc:mysql://rm-xxx.mysql.rds.aliyuncs.com:3306/digger?zeroDateTimeBehavior=convertToNull&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&amp;tinyInt1isBit=false&quot; \</span><br><span class="line">--username hadoop_read \</span><br><span class="line">--password-alias mysql.digger.prod.r.alias \</span><br><span class="line">--target-dir /sqoop/db/digger/$1/$do_date \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--query &quot;$2 and  \$CONDITIONS&quot; \</span><br><span class="line">--num-mappers 32 \</span><br><span class="line">--split-by id \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27; \</span><br><span class="line">--compress \</span><br><span class="line">--compression-codec Snappy \</span><br><span class="line">--hive-drop-import-delims \</span><br><span class="line">--null-string &#x27;\\N&#x27; \</span><br><span class="line">--null-non-string &#x27;\\N&#x27;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><img src="/2021/05/26/sqoop%E5%9C%A8%E5%AE%9E%E9%99%85%E7%94%9F%E4%BA%A7%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/2.jpg"></p>
]]></content>
      <categories>
        <category>sqoop</category>
      </categories>
  </entry>
  <entry>
    <title>一次意外断电导致mysql数据库损坏的数据恢复过程</title>
    <url>/2020/11/15/%E4%B8%80%E6%AC%A1%E6%84%8F%E5%A4%96%E6%96%AD%E7%94%B5%E5%AF%BC%E8%87%B4mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8D%9F%E5%9D%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<p>​    <img src="/2020/11/15/%E4%B8%80%E6%AC%A1%E6%84%8F%E5%A4%96%E6%96%AD%E7%94%B5%E5%AF%BC%E8%87%B4mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8D%9F%E5%9D%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D%E8%BF%87%E7%A8%8B/1.jpg"></p>
<p>​    mysql在企业开发中不仅存放着大量用户数据表，还存放着其他服务的元数据，这些数据一但无法访问，不仅影响其他业务的进行，同时用户也无法查询自己的数据信息，众所周知，数据是无价的，弄丢了数据的后果极其严重。</p>
<a id="more"></a>

<p>​    昨天就遇到了一次意外断电mysql索引损坏导致mysqld服务无法正常启动的麻烦，断电后重启hadoop集群，开启hiveservice2和matestore元数据服务时发现无法启动，打开hive.log发现是mysql连接失败：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Caused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure</span><br><span class="line">......</span><br><span class="line">Caused by: java.net.ConnectException: 拒绝连接 (Connection refused)</span><br></pre></td></tr></table></figure>

<p>随即检查了一下mysql服务，发现mysql服务被意外关闭了，于是又重新启动mysql服务:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart mysqld;</span><br></pre></td></tr></table></figure>

<p>但是服务并没有顺利启动成功，而是卡住不动了，尝试了几次都是一样卡住，我有点不淡定了，平时mysql服务断开重新开启一下就可以了，于是进入<code>/etc/my.cnf</code>中找到mysql的错误日志并监控日志输出：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tail -f /var/log/mysql.log</span><br></pre></td></tr></table></figure>

<p>在另外一个控制台又启动一次服务，发现了从来没有遇到过得报错：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2020-11-15T09:08:52.532504Z 0 [ERROR] [MY-012712] [InnoDB] Dir slot does not point to right rec 99</span><br><span class="line"> len 16384; hex 64c7c9230000023b0000023affffffff00000000292dd74545bf00000000000000000000024400231a58808c000000001a2d00020089008a0000000000000000000000000000000004700000000000000000000000000000000000000000010002001a696e66696d756d0007000b000073757072656d756d000010003000000018338b000000100f1d81000000c601100000000000006f78dfafd5300000000000000000908580e7000018003000000018338c000000100f1d81000000c6011f0000000000005ad8dfafd530a0f0ba889e75d23f90cc9ff8000020003000000018338f000000100f1e820000013801100000000000005ad9dfafd53162475ed6e629e83f90dbc387040028</span><br><span class="line"> ......</span><br><span class="line">000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000064c7c923292dd745; asc d  #   ;   :        )- EE            D # X       -                       p                         infimum      supremum    0    3                    ox   0                0    3                    Z    0     u ?        0    3           8        Z    1bG^  ) ?      ( 0    3                    oz   2         B l  0 0    3                    Z    2         E    8 0    3           Y        Z    32 c &#123;  ? Q s  @ 0    3      !             \D   4 F     @ |    H 0    3      !             Z    4          PY  P 0    3      $             Z    8  v  ( ? *U   X 0    3      %             \I   9   m4e @ ?Pm  ` 0    3      %             Z    9         C    h 0    3      &amp;             Z    :         Sc,  p 0    3      &#x27;    ~        dC   ;   $yi ?   b  x 0    3   </span><br><span class="line">.....</span><br><span class="line">Most likely, you have hit a bug, but this error can also be caused by malfunctioning hardware.</span><br><span class="line">Thread pointer: 0x7fb3bc3d4280</span><br><span class="line">Attempting backtrace. You can use the following information to find out</span><br><span class="line">where mysqld died. If you see no messages after this, something went</span><br><span class="line">terribly wrong...</span><br><span class="line"></span><br><span class="line">2020-11-15T09:08:52.623420Z 0 [ERROR] [MY-011937] [InnoDB] [FATAL] Apparent corruption of an index page [page id: space=580, page number=571] to be written to data file. We intentionally crash the server to prevent corrupt data from ending up in data files.</span><br><span class="line"></span><br><span class="line">2020-11-15T09:08:54.724470Z 0 [ERROR] [MY-011906] [InnoDB] Database page corruption on disk or a failed file read of page [page id: space=580, page number=571]. You may have to recover from a backup.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>翻译过来意思是：</p>
<p>​    最可能的情况是，您遇到了一个bug，但这个错误也可能是由硬件故障引起的。</p>
<p>​    mysqld去世的地方。如果在此之后您没有看到任何消息，那么一定有问题，非常错误的……</p>
<p>​    在磁盘上的数据库页损坏或失败的文件读取页[页id:空间=580，页号=571]。您可能需要从备份中进行恢复。</p>
<p>要写入数据文件的索引页[页id:空格=580，页号=571]明显损坏。我们故意使服务器崩溃，以防止损坏的数据出现在数据文件中。</p>
<p>​    百度了一阵之后，我意识到是数据库内部有表损坏导致mysql服务无法正常打开了，由于数据库中有上千张表，不可能知道到底是哪里出了问题。这就意味着我要将mysql中所有用户数据表和其他服务的元数据表全部导出到sql文件做备份，然后重新初始化mysql，在把表一个个建回来并重新导入。一想这工作量顿时头皮发麻，但是数据没法访问后果更严重。。。所以硬着头皮来了：</p>
<p>​    使用强制InnoDB恢复:</p>
<p>​    编辑<code>/etc/my.cnf</code>文件加入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">innodb_force_recovery &#x3D; 6</span><br></pre></td></tr></table></figure>

<p>​    <em>innodb_force_recovery影响整个InnoDB存储引擎的恢复状况。默认为0，表示当需要恢复时执行所有的恢复操作（即校验数据页/purge undo/insert buffer merge/rolling back&amp;forward），当不能进行有效的恢复操作时，mysql有可能无法启动，并记录错误日志；innodb_force_recovery可以设置为1-6,大的数字包含前面所有数字的影响。当设置参数值大于0后，可以对表进行select,create,drop操作,但insert,update或者delete这类操作是不允许的。</em></p>
<ul>
<li><p>1(SRV_FORCE_IGNORE_CORRUPT):忽略检查到的corrupt页。</p>
</li>
<li><p>2(SRV_FORCE_NO_BACKGROUND):阻止主线程的运行，如主线程需要执行full purge操作，会导致crash。</p>
</li>
<li><p>3(SRV_FORCE_NO_TRX_UNDO):不执行事务回滚操作。</p>
</li>
<li><p>4(SRV_FORCE_NO_IBUF_MERGE):不执行插入缓冲的合并操作。</p>
</li>
<li><p>5(SRV_FORCE_NO_UNDO_LOG_SCAN):不查看重做日志，InnoDB存储引擎会将未提交的事务视为已提交。</p>
</li>
<li><p>6(SRV_FORCE_NO_LOG_REDO):不执行前滚的操作。</p>
<p>​    </p>
<p>​    然后重启节点，发现mysql服务能启动了，但是无法执行插入更新和删除操作，但终于可以导出宝贵的数据做备份了，我使用了Navicat把mysql中所有数据进行了导出：</p>
<p><img src="/2020/11/15/%E4%B8%80%E6%AC%A1%E6%84%8F%E5%A4%96%E6%96%AD%E7%94%B5%E5%AF%BC%E8%87%B4mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8D%9F%E5%9D%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D%E8%BF%87%E7%A8%8B/2.jpg"></p>
<p>​    整个过程用了很久，但数据总算是保存下来了，接下来进入<code>/var/lib/mysql</code>备份整个目录，然后删除目录中所有内容<code>rm -rf  ./*</code>将mysql重新初始化。最后进入<code>/etc/my.cnf</code>将innodb重新置为0:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">innodb_force_recovery = 0</span><br></pre></td></tr></table></figure>

<p>成功启动mysql服务：</p>
<p><img src="/2020/11/15/%E4%B8%80%E6%AC%A1%E6%84%8F%E5%A4%96%E6%96%AD%E7%94%B5%E5%AF%BC%E8%87%B4mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8D%9F%E5%9D%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D%E8%BF%87%E7%A8%8B/4.jpg"></p>
<p>这时候数据库已经重新初始化，需要重新获取初始密码：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">grep &#x27;temporary password&#x27; /var/log/mysqld.log</span><br></pre></td></tr></table></figure>



</li>
</ul>
<p>  ​    然后进行设置mysql用户的操作，导入之前的数据，重新启动hadoop集群，成功启动hiveservice2和matestore服务。并进入datagrip查了一下，数据完美恢复！还好是测试环境，谢天谢地。</p>
<p>  <img src="/2020/11/15/%E4%B8%80%E6%AC%A1%E6%84%8F%E5%A4%96%E6%96%AD%E7%94%B5%E5%AF%BC%E8%87%B4mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8D%9F%E5%9D%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D%E8%BF%87%E7%A8%8B/3.jpg"></p>
]]></content>
      <categories>
        <category>OLTP</category>
      </categories>
  </entry>
  <entry>
    <title>Hexo搭建个人博客</title>
    <url>/2020/09/23/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<h2 id="主机配置"><a href="#主机配置" class="headerlink" title="主机配置"></a>主机配置</h2><a id="more"></a>

<p>安装node.js  git，打开git终端</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ~&#x2F;.ssh</span><br><span class="line">ssh-keygen -t rsa -C &quot;1872998728@qq.com&quot;</span><br></pre></td></tr></table></figure>

<p>上传GitHub</p>
<p>详细查看博客：<a href="https://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.html">https://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.html</a></p>
<!--more-->

<h2 id="服务器配置"><a href="#服务器配置" class="headerlink" title="服务器配置"></a>服务器配置</h2><p>安装git  Nginx</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git init --bare hexo.git</span><br><span class="line">cd hexo.git&#x2F;hooks</span><br></pre></td></tr></table></figure>

<p>vim post-receive</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">git --work-tree=/home/wwwroot/3DCEList --git-dir=/home/wwwroot/hexo.git checkout -f</span><br></pre></td></tr></table></figure>

<p>配置免密登陆：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh-copy-id -i .ssh&#x2F;id_rsa.pub root@47.242.8.122</span><br></pre></td></tr></table></figure>



]]></content>
  </entry>
  <entry>
    <title>日活需求(kafka精准一次性消费) 分别用sparkstreaming和flink实现</title>
    <url>/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/1.jpg"></p>
<a id="more"></a>

<p>指标需求：求出当日新增日活，并通过kibana按照需求做实时展示</p>
<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/kibana%E5%8F%AF%E8%A7%86%E5%8C%96.jpg"></p>
<p>实现思路1：sparkstreaming消费kafka数据，使用redis保存kafka偏移量，确保程序意外退出后能从之前的偏移量继续消费，并保存至es做去重。</p>
<p>实现思路2：flink消费kafka数据，使用状态后端(state backend)保存data source中来自kafka的偏移量，确保程序宕机后重启能从之前的偏移位置重新消费。</p>
<blockquote>
<p><em>端到端exactly-once实现：</em></p>
<p>​    <u><em>source端：kafka偏移量</em></u></p>
<p>​    <u><em>内部：sparkingstreaming：redis做去重同时保存偏移量</em></u></p>
<p>​                <u><em>flink：state backend状态后端</em></u></p>
<p>​    <u><em>sink端：es的id不可重复，做幂等性写入</em></u></p>
</blockquote>
<p>项目局部架构：</p>
<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/%E6%97%A5%E6%B4%BB.jpg"></p>
<p>sparkingstreaming流程图：</p>
<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/spark.jpg"></p>
<p>flink端到端状态一致性过程：</p>
<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/%E4%B8%80%E8%87%B4%E6%80%A71.jpg"></p>
<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/%E4%B8%80%E8%87%B4%E6%80%A72.jpg"></p>
<h2 id="demo："><a href="#demo：" class="headerlink" title="demo："></a>demo：</h2><h4 id="sparkstreaming使用scala实现："><a href="#sparkstreaming使用scala实现：" class="headerlink" title="sparkstreaming使用scala实现："></a>sparkstreaming使用scala实现：</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DauApp</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置文件对象 注意：Streaming程序至少不能设置为local，至少需要2个线程</span></span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Spark01_W&quot;</span>).setMaster(<span class="string">&quot;local[4]&quot;</span>)</span><br><span class="line">      </span><br><span class="line">    <span class="comment">//创建Spark Streaming上下文环境对象O</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> gmallstartup = <span class="string">&quot;GMALL_STARTUP_0105&quot;</span></span><br><span class="line">    <span class="keyword">val</span> daugroup = <span class="string">&quot;DAU_GROUP&quot;</span></span><br><span class="line">      </span><br><span class="line">     <span class="comment">//使用偏移量工具类从redis获取上一次的kafka偏移量</span></span><br><span class="line">    <span class="keyword">val</span> partitionToLong = util.<span class="type">OffsetManager</span>.getOffset(gmallstartup, daugroup)</span><br><span class="line">      </span><br><span class="line">      <span class="comment">//判断是否第一次消费，如果不是则从偏移量开始消费数据流</span></span><br><span class="line">    <span class="keyword">var</span> inputStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]]=<span class="literal">null</span></span><br><span class="line">    <span class="keyword">if</span> (partitionToLong!=<span class="literal">null</span>&amp;&amp;partitionToLong.size&gt;<span class="number">0</span>)&#123;</span><br><span class="line">      inputStream = util.<span class="type">MyKafkaUtil</span>.getKafkaStream(gmallstartup, ssc, partitionToLong, daugroup)</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">      inputStream=util.<span class="type">MyKafkaUtil</span>.getKafkaStream(gmallstartup,ssc)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">//得到本批次的偏移量的结束位置，用于更新redis中的偏移量</span></span><br><span class="line">    <span class="keyword">var</span>  offsetRanges: <span class="type">Array</span>[<span class="type">OffsetRange</span>] = <span class="type">Array</span>.empty[<span class="type">OffsetRange</span>]</span><br><span class="line">    <span class="keyword">val</span>  inputGetOffsetDstream: <span class="type">DStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = inputStream.transform &#123; rdd =&gt;</span><br><span class="line">      offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges  <span class="comment">//driver? executor?  //周期性的执行</span></span><br><span class="line">      rdd</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">//补充日志json时间字段</span></span><br><span class="line">    <span class="keyword">val</span> value1 = inputGetOffsetDstream.map(record =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> str = record.value()</span><br><span class="line">      <span class="keyword">val</span> nObject = <span class="type">JSON</span>.parseObject(str)</span><br><span class="line">      <span class="keyword">val</span> long = nObject.getLong(<span class="string">&quot;ts&quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> str1 = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">&quot;yyyy-MM-dd HH&quot;</span>).format(<span class="keyword">new</span> <span class="type">Date</span>(long))</span><br><span class="line">      <span class="keyword">val</span> strings = str1.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">      nObject.put(<span class="string">&quot;dt&quot;</span>, strings(<span class="number">0</span>))</span><br><span class="line">      nObject.put(<span class="string">&quot;hr&quot;</span>, strings(<span class="number">1</span>))</span><br><span class="line">      nObject</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">      <span class="comment">//写入redis以及设置保存时间为24小时，并通过是否写入redis成功判断过滤条数</span></span><br><span class="line">    <span class="keyword">val</span> value2 = value1.mapPartitions(iter =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> client = util.<span class="type">RedisUtil</span>.getJedisClient</span><br><span class="line">      <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">JSONObject</span>]()</span><br><span class="line">      <span class="keyword">val</span> list = iter.toList</span><br><span class="line">      println(<span class="string">&quot;过滤前:&quot;</span> + list.size)</span><br><span class="line">      <span class="keyword">for</span> (jsonObj &lt;- list) &#123;</span><br><span class="line">        <span class="keyword">val</span> str = jsonObj.getString(<span class="string">&quot;dt&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> str1 = jsonObj.getJSONObject(<span class="string">&quot;common&quot;</span>).getString(<span class="string">&quot;mid&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> str2 = <span class="string">&quot;dau:&quot;</span> + str</span><br><span class="line">        <span class="keyword">val</span> long = client.sadd(str2, str1)</span><br><span class="line">        client.expire(str2, <span class="number">3600</span> * <span class="number">24</span>)</span><br><span class="line">        <span class="keyword">if</span> (long == <span class="number">1</span>) &#123;</span><br><span class="line">          buffer += jsonObj</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      client.close()</span><br><span class="line">      println(<span class="string">&quot;过滤后:&quot;</span> + buffer.size)</span><br><span class="line">      list.toIterator</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">      </span><br><span class="line">      <span class="comment">//写入es</span></span><br><span class="line">    value2.foreachRDD &#123; rdd =&gt; &#123;</span><br><span class="line">      rdd.foreachPartition(rdd =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> list = rdd.toList</span><br><span class="line">        <span class="keyword">val</span> tuples = list.map(jsonObj =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> nObject = jsonObj.getJSONObject(<span class="string">&quot;common&quot;</span>)</span><br><span class="line">          <span class="keyword">val</span> info = bean.<span class="type">DauInfo</span>(nObject.getString(<span class="string">&quot;mid&quot;</span>),</span><br><span class="line">            nObject.getString(<span class="string">&quot;uid&quot;</span>),</span><br><span class="line">            nObject.getString(<span class="string">&quot;ar&quot;</span>),</span><br><span class="line">            nObject.getString(<span class="string">&quot;ch&quot;</span>),</span><br><span class="line">            nObject.getString(<span class="string">&quot;vc&quot;</span>),</span><br><span class="line">            jsonObj.getString(<span class="string">&quot;dt&quot;</span>),</span><br><span class="line">            jsonObj.getString(<span class="string">&quot;hr&quot;</span>),</span><br><span class="line">            <span class="string">&quot;00&quot;</span>,</span><br><span class="line">            jsonObj.getLong(<span class="string">&quot;ts&quot;</span>))</span><br><span class="line">          (info.mid, info)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="keyword">val</span> str = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">&quot;yyyy-MM-dd&quot;</span>).format(<span class="keyword">new</span> <span class="type">Date</span>())</span><br><span class="line">        util.<span class="type">MyEsUtil</span>.bulkDoc(tuples, <span class="string">&quot;gmall_dau_info_&quot;</span> + str)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">      util.<span class="type">OffsetManager</span>.saveOffset(gmallstartup, daugroup, offsetRanges)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//    value.map(_.value()).print()</span></span><br><span class="line">    <span class="comment">//启动采集器</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">//默认情况下，上下文对象不能关闭</span></span><br><span class="line">    <span class="comment">//ssc.stop()</span></span><br><span class="line">    <span class="comment">//等待采集结束，终止上下文环境对象</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>使用到的utils工具类可以去<a href="https://github.com/ycfn97/gmall-utils.git">GitHub</a>拉取</p>
<h4 id="flink使用java实现："><a href="#flink使用java实现：" class="headerlink" title="flink使用java实现："></a>flink使用java实现：</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> app;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.RuntimeContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch.util.RetryRejectedExecutionFailureHandler;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.util.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.http.HttpHost;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.index.IndexRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.Requests;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Copyright(c) 2020-2021 sparrow All Rights Reserved</span></span><br><span class="line"><span class="comment"> * Project: gmall2020-parent</span></span><br><span class="line"><span class="comment"> * Package: app</span></span><br><span class="line"><span class="comment"> * ClassName: DauApp01</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> 18729 created on date: 2020/12/8 11:55</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@version</span> 1.0</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> JDK 1.8</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DauApp01</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String kafkaBrokers = <span class="string">&quot;hadoop01:9092&quot;</span>;</span><br><span class="line">        String zkBrokers = <span class="string">&quot;hadoop01:2181,hadoop02:2181,hadoop03:2181&quot;</span>;</span><br><span class="line">        String topic = <span class="string">&quot;GMALL_STARTUP_0105&quot;</span>;</span><br><span class="line">        String groupId = <span class="string">&quot;DAU_GROUP&quot;</span>;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;===============》 flink任务开始  ==============》&quot;</span>);</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">//设置kafka连接参数</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, kafkaBrokers);</span><br><span class="line">        properties.setProperty(<span class="string">&quot;zookeeper.connect&quot;</span>, zkBrokers);</span><br><span class="line">        properties.setProperty(<span class="string">&quot;group.id&quot;</span>, groupId);</span><br><span class="line">        <span class="comment">//设置时间类型</span></span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">        <span class="comment">//设置检查点时间间隔</span></span><br><span class="line">        env.enableCheckpointing(<span class="number">5000</span>);</span><br><span class="line">        env.setStateBackend( <span class="keyword">new</span> MemoryStateBackend());</span><br><span class="line">        <span class="comment">//创建kafak消费者，获取kafak中的数据</span></span><br><span class="line">        FlinkKafkaConsumer010&lt;String&gt; kafkaConsumer010 = <span class="keyword">new</span> FlinkKafkaConsumer010&lt;&gt;(topic, <span class="keyword">new</span> SimpleStringSchema(), properties);</span><br><span class="line">        DataStreamSource&lt;String&gt; kafkaData = env.addSource(kafkaConsumer010);</span><br><span class="line">        DataStream&lt;String&gt; userData = kafkaData.map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;&gt;&gt;&gt;&gt;&gt;&gt;接收topic报文:&quot;</span>+s+<span class="string">&quot;&lt;&lt;&lt;&lt;&lt;&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> s;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        List&lt;HttpHost&gt; httpHosts = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        httpHosts.add(<span class="keyword">new</span> HttpHost(<span class="string">&quot;hadoop01&quot;</span>, <span class="number">9200</span>, <span class="string">&quot;http&quot;</span>));</span><br><span class="line">        ElasticsearchSink.Builder&lt;String&gt; esSinkBuilder = <span class="keyword">new</span> ElasticsearchSink.Builder&lt;&gt;(</span><br><span class="line">                httpHosts,</span><br><span class="line">                <span class="keyword">new</span> ElasticsearchSinkFunction&lt;String&gt;() &#123;</span><br><span class="line">                    <span class="function"><span class="keyword">public</span> IndexRequest <span class="title">createIndexRequest</span><span class="params">(String element)</span> </span>&#123;</span><br><span class="line">                        Map&lt;String, Object&gt; json = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">                        JSONObject jsonObject = JSON.parseObject(element);</span><br><span class="line"></span><br><span class="line">                        Long ts = jsonObject.getLong(<span class="string">&quot;ts&quot;</span>);</span><br><span class="line">                        String format = <span class="keyword">new</span> SimpleDateFormat(<span class="string">&quot;yyyy-MM-dd HH&quot;</span>).format(<span class="keyword">new</span> Date(ts));</span><br><span class="line">                        String[] s = format.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">                        jsonObject.put(<span class="string">&quot;dt&quot;</span>,s[<span class="number">0</span>]);</span><br><span class="line">                        jsonObject.put(<span class="string">&quot;hr&quot;</span>,s[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">                        String common = jsonObject.getString(<span class="string">&quot;common&quot;</span>);</span><br><span class="line">                        JSONObject jsonObject1 = JSON.parseObject(common);</span><br><span class="line"></span><br><span class="line">                        json.put(<span class="string">&quot;mid&quot;</span>,jsonObject1.getString(<span class="string">&quot;mid&quot;</span>));</span><br><span class="line">                        json.put(<span class="string">&quot;uid&quot;</span>,jsonObject1.getString(<span class="string">&quot;uid&quot;</span>));</span><br><span class="line">                        json.put(<span class="string">&quot;ar&quot;</span>,jsonObject1.getString(<span class="string">&quot;ar&quot;</span>));</span><br><span class="line">                        json.put(<span class="string">&quot;ch&quot;</span>,jsonObject1.getString(<span class="string">&quot;ch&quot;</span>));</span><br><span class="line">                        json.put(<span class="string">&quot;vc&quot;</span>,jsonObject1.getString(<span class="string">&quot;vc&quot;</span>));</span><br><span class="line">                        json.put(<span class="string">&quot;dt&quot;</span>,jsonObject.getString(<span class="string">&quot;dt&quot;</span>));</span><br><span class="line">                        json.put(<span class="string">&quot;hr&quot;</span>,jsonObject.getString(<span class="string">&quot;hr&quot;</span>));</span><br><span class="line">                        json.put(<span class="string">&quot;mi&quot;</span>,<span class="string">&quot;00&quot;</span>);</span><br><span class="line">                        json.put(<span class="string">&quot;ts&quot;</span>,jsonObject.getLong(<span class="string">&quot;ts&quot;</span>));</span><br><span class="line">                        System.out.println(<span class="string">&quot;data:&quot;</span>+element);</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">return</span> Requests.indexRequest()</span><br><span class="line">                                .index(<span class="string">&quot;gmall_dau_info_&quot;</span> + jsonObject.getString(<span class="string">&quot;dt&quot;</span>))</span><br><span class="line">                                .type(<span class="string">&quot;_doc&quot;</span>)</span><br><span class="line">                                .id(jsonObject1.getString(<span class="string">&quot;mid&quot;</span>))</span><br><span class="line">                                .source(json);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String element, RuntimeContext ctx, RequestIndexer indexer)</span> </span>&#123;</span><br><span class="line">                        indexer.add(createIndexRequest(element));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        esSinkBuilder.setBulkFlushMaxActions(<span class="number">1</span>);</span><br><span class="line"><span class="comment">//        esSinkBuilder.setRestClientFactory(</span></span><br><span class="line"><span class="comment">//                restClientBuilder -&gt; &#123;</span></span><br><span class="line"><span class="comment">//                    restClientBuilder.setDefaultHeaders()</span></span><br><span class="line"><span class="comment">//                &#125;</span></span><br><span class="line"><span class="comment">//        );</span></span><br><span class="line">        esSinkBuilder.setRestClientFactory(<span class="keyword">new</span> util.RestClientFactoryImpl());</span><br><span class="line">        esSinkBuilder.setFailureHandler(<span class="keyword">new</span> RetryRejectedExecutionFailureHandler());</span><br><span class="line"></span><br><span class="line">        userData.addSink(esSinkBuilder.build());</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">&quot;flink-task&quot;</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="测试："><a href="#测试：" class="headerlink" title="测试："></a>测试：</h4><p>将sparkstreaming任务宕机，然后在打开，从redis读取检查点位置继续消费：</p>
<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/redis1.jpg"></p>
<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/redis2.jpg"></p>
<p>将flink任务宕机，然后在打开，能够继续从检查点消费：</p>
<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/%E6%A3%80%E6%9F%A5%E7%82%B9.jpg"></p>
<p>通过对比可以发现，使用flink代码比使用sparkstreaming简洁很多，原因在于flink内部有保存状态的状态后端，同时sparkstreaming基于微批次处理，flink基于流式处理在数据处理速度上页更加流畅。</p>
]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>/2021/05/30/spark-sql%E5%B8%B8%E7%94%A8%E8%BF%90%E8%A1%8C%E6%97%B6%E8%B0%83%E4%BC%98/</url>
    <content><![CDATA[<hr>
<h2 id="spark-sql-常用运行时调优"><a href="#spark-sql-常用运行时调优" class="headerlink" title="spark-sql 常用运行时调优"></a>spark-sql 常用运行时调优</h2><p>进入spark-sql客户端：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark-sql</span><br></pre></td></tr></table></figure>

<p>运行命令获取帮助：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">         &gt; set -v;</span><br><span class="line">spark.sql.adaptive.allowAdditionalShuffle	false	When true, additional shuffles are allowed during plan optimizations in adaptive execution</span><br><span class="line">spark.sql.adaptive.enabled	false	When true, enable adaptive query execution.</span><br><span class="line">spark.sql.adaptive.join.enabled	true	When true and adaptive execution is enabled, a better join strategy is determined at runtime.</span><br><span class="line">spark.sql.adaptive.maxNumPostShufflePartitions	500	The advisory maximum number of post-shuffle partitions used in adaptive execution.</span><br><span class="line">spark.sql.adaptive.minNumPostShufflePartitions	1	The advisory minimum number of post-shuffle partitions used in adaptive execution.</span><br><span class="line">spark.sql.adaptive.shuffle.targetPostShuffleInputSize	67108864b	The target post-shuffle input size in bytes of a task.</span><br><span class="line">spark.sql.adaptive.shuffle.targetPostShuffleRowCount	20000000	The target post-shuffle row count of a task.</span><br><span class="line">spark.sql.adaptive.skewedJoin.enabled	false	When true and adaptive execution is enabled, a skewed join is automatically handled at runtime.</span><br><span class="line">spark.sql.adaptive.skewedPartitionFactor	10	A partition is considered as a skewed partition if its size is larger than this factor multiple the median partition size and also larger than spark.sql.adaptive.skewedPartitionSizeThreshold, or if its row count is larger than this factor multiple the median row count and also larger than spark.sql.adaptive.skewedPartitionRowCountThreshold.</span><br><span class="line">spark.sql.adaptive.skewedPartitionMaxSplits	5	Configures the maximum number of task to handle a skewed partition in adaptive skewedjoin.</span><br><span class="line">spark.sql.adaptive.skewedPartitionRowCountThreshold	10000000	Configures the minimum row count for a partition that is considered as a skewed partition in adaptive skewed join.</span><br><span class="line">spark.sql.adaptive.skewedPartitionSizeThreshold	67108864b	Configures the minimum size in bytes for a partition that is considered as a skewed partition in adaptive skewed join.</span><br><span class="line">spark.sql.adaptiveBroadcastJoinThreshold	&lt;undefined&gt;	Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join in adaptive exeuction mode. If not set, it equals to spark.sql.autoBroadcastJoinThreshold.</span><br><span class="line">spark.sql.autoBroadcastJoinThreshold	10485760	Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join.  By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command &lt;code&gt;ANALYZE TABLE &amp;lt;tableName&amp;gt; COMPUTE STATISTICS noscan&lt;/code&gt; has been run, and file-based data source tables where the statistics are computed directly on the files of data.</span><br><span class="line">spark.sql.avro.compression.codec	snappy	Compression codec used in writing of AVRO files. Supported codecs: uncompressed, deflate, snappy, bzip2 and xz. Default codec is snappy.</span><br><span class="line">spark.sql.avro.deflate.level	-1	Compression level for the deflate codec used in writing of AVRO files. Valid value must be in the range of from 1 to 9 inclusive or -1. The default value is -1 which corresponds to 6 level in the current implementation.</span><br><span class="line">spark.sql.broadcastTimeout	300000ms	Timeout in seconds for the broadcast wait time in broadcast joins.</span><br><span class="line">spark.sql.cache.meta.storage.path	/opt/apps/ecm/service/spark/2.4.5-hadoop3.1-1.0.0/package/spark-2.4.5-hadoop3.1-1.0.0	Local level db location to store some Cube Management metadata.</span><br><span class="line">spark.sql.cache.queryRewrite	true	When true, spark try to use previous built Cube Management to optimized input queries</span><br><span class="line">spark.sql.cbo.enabled	false	Enables CBO for estimation of plan statistics when set true.</span><br><span class="line">spark.sql.cbo.joinReorder.dp.star.filter	false	Applies star-join filter heuristics to cost based join enumeration.</span><br><span class="line">spark.sql.cbo.joinReorder.dp.threshold	12	The maximum number of joined nodes allowed in the dynamic programming algorithm.</span><br><span class="line">spark.sql.cbo.joinReorder.enabled	false	Enables join reorder in CBO.</span><br><span class="line">spark.sql.cbo.joinReorder.ga.num.generations	0	The maximum generations for the evolution to get a solution.</span><br><span class="line">spark.sql.cbo.joinReorder.ga.pool.size	0	The pool size (population) of the chromosomes in the Genetic algorithm.</span><br><span class="line">spark.sql.cbo.joinReorder.ga.selection.bias	2.0	The bias used for selecting the chromosome from the pool.</span><br><span class="line">spark.sql.cbo.joinReorder.ga.trade.factor	5	The trade-off factor between planning time and query plan quality. It should be an integer from 1 to 10. Larger value will increase the planing time which may generate a better plan. 5 is the default.</span><br><span class="line">spark.sql.cbo.outerJoinReorder.enabled	false	Enables outer join reorder in CBO.</span><br><span class="line">spark.sql.cbo.starSchemaDetection	false	When true, it enables join reordering based on star schema detection. </span><br><span class="line">spark.sql.columnNameOfCorruptRecord	_corrupt_record	The name of internal column for storing raw/un-parsed JSON and CSV records that fail to parse.</span><br><span class="line">spark.sql.crossJoin.enabled	false	When false, we will throw an error if a query contains a cartesian product without explicit CROSS JOIN syntax.</span><br><span class="line">spark.sql.delta.merge.retries	3	The max number of retries for merging into delta lake in case of `ConcurrentWriteException`.</span><br><span class="line">spark.sql.delta.merge.sourceQuery.executionMode	batch	The execution mode of source query. Supported mode: batch and stream. Default mode is batch.</span><br><span class="line">spark.sql.dynamic.runtime.filter.bbf.enabled	false	When true, enable dynamic runtime filter using bbf</span><br><span class="line">spark.sql.dynamic.runtime.filter.enabled	false	When true, enable dynamic runtime filter</span><br><span class="line">spark.sql.dynamic.runtime.filter.table.size.lower.limit	53687091200	big table side should be greater than this value</span><br><span class="line">spark.sql.dynamic.runtime.filter.table.size.upper.limit	5368709120	small table side should be smaller than this value</span><br><span class="line">spark.sql.execution.arrow.enabled	false	When true, make use of Apache Arrow for columnar data transfers. Currently available for use with pyspark.sql.DataFrame.toPandas, and pyspark.sql.SparkSession.createDataFrame when its input is a Pandas DataFrame. The following data types are unsupported: BinaryType, MapType, ArrayType of TimestampType, and nested StructType.</span><br><span class="line">spark.sql.execution.arrow.fallback.enabled	true	When true, optimizations enabled by &#x27;spark.sql.execution.arrow.enabled&#x27; will fallback automatically to non-optimized implementations if an error occurs.</span><br><span class="line">spark.sql.execution.arrow.maxRecordsPerBatch	10000	When using Apache Arrow, limit the maximum number of records that can be written to a single ArrowRecordBatch in memory. If set to zero or negative there is no limit.</span><br><span class="line">spark.sql.extensions	io.delta.sql.DeltaSparkSessionExtension,org.apache.spark.sql.emr.EmrSparkSessionExtension	A comma-separated list of classes that implement Function1[SparkSessionExtension, Unit] used to configure Spark Session extensions. The classes must have a no-args constructor. If multiple extensions are specified, they are applied in the specified order. For the case of rules and planner strategies, they are applied in the specified order. For the case of parsers, the last parser is used and each parser can delegate to its predecessor.</span><br><span class="line">spark.sql.extract.common.conjunct.filter	false	When true, common conjunct in filter conditition will be extracted, which will be pushed down later</span><br><span class="line">spark.sql.files.binPackingStrategy	efficiency-oriented	THIS IS AN INTERNAL CONFIG WHICH SHOULD NOT BE EXPOSED TO CUSTOMER. The strategy for bin-packing when scanning files, which can be &#x27;efficiency-oriented&#x27; or &#x27;size-oriented&#x27;, and &#x27;efficiency-oriented&#x27; is the default. &#x27;efficiency-oriented&#x27; tries to pack files into partitions via an algorithm which aims to read the files as soon as possible, while &#x27;size-oriented&#x27; will try to pack as many files as possible into a single partition, but the size of the partition should be less than &#x27;spark.sql.files.maxPartitionBytes&#x27; defines.</span><br><span class="line">spark.sql.files.ignoreCorruptFiles	false	Whether to ignore corrupt files. If true, the Spark jobs will continue to run when encountering corrupted files and the contents that have been read will still be returned.</span><br><span class="line">spark.sql.files.ignoreMissingFiles	false	Whether to ignore missing files. If true, the Spark jobs will continue to run when encountering missing files and the contents that have been read will still be returned.</span><br><span class="line">spark.sql.files.maxPartitionBytes	134217728	The maximum number of bytes to pack into a single partition when reading files.</span><br><span class="line">spark.sql.files.maxRecordsPerFile	0	Maximum number of records to write out to a single file. If this value is zero or negative, there is no limit.</span><br><span class="line">spark.sql.function.concatBinaryAsString	false	When this option is set to false and all inputs are binary, `functions.concat` returns an output as binary. Otherwise, it returns as a string. </span><br><span class="line">spark.sql.function.eltOutputAsString	false	When this option is set to false and all inputs are binary, `elt` returns an output as binary. Otherwise, it returns as a string. </span><br><span class="line">spark.sql.groupByAliases	true	When true, aliases in a select list can be used in group by clauses. When false, an analysis exception is thrown in the case.</span><br><span class="line">spark.sql.groupByOrdinal	true	When true, the ordinal numbers in group by clauses are treated as the position in the select list. When false, the ordinal numbers are ignored.</span><br><span class="line">spark.sql.hive.caseSensitiveInferenceMode	INFER_AND_SAVE	Sets the action to take when a case-sensitive schema cannot be read from a Hive table&#x27;s properties. Although Spark SQL itself is not case-sensitive, Hive compatible file formats such as Parquet are. Spark SQL must use a case-preserving schema when querying any table backed by files containing case-sensitive field names or queries may not return accurate results. Valid options include INFER_AND_SAVE (the default mode-- infer the case-sensitive schema from the underlying data files and write it back to the table properties), INFER_ONLY (infer the schema but don&#x27;t attempt to write it to the table properties) and NEVER_INFER (fallback to using the case-insensitive metastore schema instead of inferring).</span><br><span class="line">spark.sql.hive.convertMetastoreOrc	true	When set to true, the built-in ORC reader and writer are used to process ORC tables created by using the HiveQL syntax, instead of Hive serde.</span><br><span class="line">spark.sql.hive.convertMetastoreParquet	true	When set to true, the built-in Parquet reader and writer are used to process parquet tables created by using the HiveQL syntax, instead of Hive serde.</span><br><span class="line">spark.sql.hive.convertMetastoreParquet.mergeSchema	false	When true, also tries to merge possibly different but compatible Parquet schemas in different Parquet data files. This configuration is only effective when &quot;spark.sql.hive.convertMetastoreParquet&quot; is true.</span><br><span class="line">spark.sql.hive.filesourcePartitionFileCacheSize	262144000	When nonzero, enable caching of partition file metadata in memory. All tables share a cache that can use up to specified num bytes for file metadata. This conf only has an effect when hive filesource partition management is enabled.</span><br><span class="line">spark.sql.hive.manageFilesourcePartitions	true	When true, enable metastore partition management for file source tables as well. This includes both datasource and converted Hive tables. When partition management is enabled, datasource tables store partition in the Hive metastore, and use the metastore to prune partitions during query planning.</span><br><span class="line">spark.sql.hive.metastore.barrierPrefixes		A comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e. &lt;code&gt;org.apache.spark.*&lt;/code&gt;).</span><br><span class="line">spark.sql.hive.metastore.jars	builtin	</span><br><span class="line"> Location of the jars that should be used to instantiate the HiveMetastoreClient.</span><br><span class="line"> This property can be one of three options: </span><br><span class="line"> 1. &quot;builtin&quot;</span><br><span class="line">   Use Hive 2.3.5, which is bundled with the Spark assembly when</span><br><span class="line">   &lt;code&gt;-Phive&lt;/code&gt; is enabled. When this option is chosen,</span><br><span class="line">   &lt;code&gt;spark.sql.hive.metastore.version&lt;/code&gt; must be either</span><br><span class="line">   &lt;code&gt;2.3.5&lt;/code&gt; or not defined.</span><br><span class="line"> 2. &quot;maven&quot;</span><br><span class="line">   Use Hive jars of specified version downloaded from Maven repositories.</span><br><span class="line"> 3. A classpath in the standard format for both Hive and Hadoop.</span><br><span class="line">      </span><br><span class="line">spark.sql.hive.metastore.sharedPrefixes	com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc	A comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log4j.</span><br><span class="line">spark.sql.hive.metastore.version	2.3.5	Version of the Hive metastore. Available options are &lt;code&gt;0.12.0&lt;/code&gt; through &lt;code&gt;2.3.3&lt;/code&gt;.</span><br><span class="line">spark.sql.hive.metastorePartitionPruning	true	When true, some predicates will be pushed down into the Hive metastore so that unmatching partitions can be eliminated earlier. This only affects Hive tables not converted to filesource relations (see HiveUtils.CONVERT_METASTORE_PARQUET and HiveUtils.CONVERT_METASTORE_ORC for more information).</span><br><span class="line">spark.sql.hive.thriftServer.async	true	When set to true, Hive Thrift server executes SQL queries in an asynchronous way.</span><br><span class="line">spark.sql.hive.thriftServer.singleSession	false	When set to true, Hive Thrift server is running in a single session mode. All the JDBC/ODBC connections share the temporary views, function registries, SQL configuration and the current database.</span><br><span class="line">spark.sql.hive.verifyPartitionPath	false	When true, check all the partition paths under the table&#x27;s root directory when reading data stored in HDFS. This configuration will be deprecated in the future releases and replaced by spark.files.ignoreMissingFiles.</span><br><span class="line">spark.sql.hive.version	2.3.5	deprecated, please use spark.sql.hive.metastore.version to get the Hive version in Spark.</span><br><span class="line">spark.sql.inMemoryColumnarStorage.batchSize	10000	Controls the size of batches for columnar caching.  Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data.</span><br><span class="line">spark.sql.inMemoryColumnarStorage.compressed	true	When set to true Spark SQL will automatically select a compression codec for each column based on statistics of the data.</span><br><span class="line">spark.sql.inMemoryColumnarStorage.enableVectorizedReader	true	Enables vectorized reader for columnar caching.</span><br><span class="line">spark.sql.infer.filter.from.joincondition	false	When true, infer filter from join, which could be pushed down later</span><br><span class="line">spark.sql.intersect.groupby.placement	false	When true, distinct(join) will be placement by join(group by)</span><br><span class="line">spark.sql.legacy.replaceDatabricksSparkAvro.enabled	true	If it is set to true, the data source provider com.databricks.spark.avro is mapped to the built-in but external Avro data source module for backward compatibility.</span><br><span class="line">spark.sql.legacy.sizeOfNull	true	If it is set to true, size of null returns -1. This behavior was inherited from Hive. The size function returns null for null input if the flag is disabled.</span><br><span class="line">spark.sql.optimizer.excludedRules	&lt;undefined&gt;	Configures a list of rules to be disabled in the optimizer, in which the rules are specified by their rule names and separated by comma. It is not guaranteed that all the rules in this configuration will eventually be excluded, as some rules are necessary for correctness. The optimizer will log the rules that have indeed been excluded.</span><br><span class="line">spark.sql.orc.columnarReaderBatchSize	4096	The number of rows to include in a orc vectorized reader batch. The number should be carefully chosen to minimize overhead and avoid OOMs in reading data.</span><br><span class="line">spark.sql.orc.compression.codec	snappy	Sets the compression codec used when writing ORC files. If either `compression` or `orc.compress` is specified in the table-specific options/properties, the precedence would be `compression`, `orc.compress`, `spark.sql.orc.compression.codec`.Acceptable values include: none, uncompressed, snappy, zlib, lzo.</span><br><span class="line">spark.sql.orc.enableVectorizedReader	true	Enables vectorized orc decoding.</span><br><span class="line">spark.sql.orc.filterPushdown	true	When true, enable filter pushdown for ORC files.</span><br><span class="line">spark.sql.orderByOrdinal	true	When true, the ordinal numbers are treated as the position in the select list. When false, the ordinal numbers in order/sort by clause are ignored.</span><br><span class="line">spark.sql.parquet.binaryAsString	false	Some other Parquet-producing systems, in particular Impala and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.</span><br><span class="line">spark.sql.parquet.columnarReaderBatchSize	4096	The number of rows to include in a parquet vectorized reader batch. The number should be carefully chosen to minimize overhead and avoid OOMs in reading data.</span><br><span class="line">spark.sql.parquet.compression.codec	snappy	Sets the compression codec used when writing Parquet files. If either `compression` or `parquet.compression` is specified in the table-specific options/properties, the precedence would be `compression`, `parquet.compression`, `spark.sql.parquet.compression.codec`. Acceptable values include: none, uncompressed, snappy, gzip, lzo, brotli, lz4, zstd.</span><br><span class="line">spark.sql.parquet.enableVectorizedReader	true	Enables vectorized parquet decoding.</span><br><span class="line">spark.sql.parquet.filterPushdown	true	Enables Parquet filter push-down optimization when set to true.</span><br><span class="line">spark.sql.parquet.int64AsTimestampMillis	false	(Deprecated since Spark 2.3, please set spark.sql.parquet.outputTimestampType.) When true, timestamp values will be stored as INT64 with TIMESTAMP_MILLIS as the extended type. In this mode, the microsecond portion of the timestamp value will betruncated.</span><br><span class="line">spark.sql.parquet.int96AsTimestamp	true	Some Parquet-producing systems, in particular Impala, store Timestamp into INT96. Spark would also store Timestamp as INT96 because we need to avoid precision lost of the nanoseconds field. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.</span><br><span class="line">spark.sql.parquet.int96TimestampConversion	false	This controls whether timestamp adjustments should be applied to INT96 data when converting to timestamps, for data written by Impala.  This is necessary because Impala stores INT96 data with a different timezone offset than Hive &amp; Spark.</span><br><span class="line">spark.sql.parquet.mergeSchema	false	When true, the Parquet data source merges schemas collected from all data files, otherwise the schema is picked from the summary file or a random data file if no summary file is available.</span><br><span class="line">spark.sql.parquet.outputTimestampType	INT96	Sets which Parquet timestamp type to use when Spark writes data to Parquet files. INT96 is a non-standard but commonly used timestamp type in Parquet. TIMESTAMP_MICROS is a standard timestamp type in Parquet, which stores number of microseconds from the Unix epoch. TIMESTAMP_MILLIS is also standard, but with millisecond precision, which means Spark has to truncate the microsecond portion of its timestamp value.</span><br><span class="line">spark.sql.parquet.recordLevelFilter.enabled	false	If true, enables Parquet&#x27;s native record-level filtering using the pushed down filters. This configuration only has an effect when &#x27;spark.sql.parquet.filterPushdown&#x27; is enabled and the vectorized reader is not used. You can ensure the vectorized reader is not used by setting &#x27;spark.sql.parquet.enableVectorizedReader&#x27; to false.</span><br><span class="line">spark.sql.parquet.respectSummaryFiles	false	When true, we make assumption that all part-files of Parquet are consistent with summary files and we will ignore them when merging schema. Otherwise, if this is false, which is the default, we will merge all part-files. This should be considered as expert-only option, and shouldn&#x27;t be enabled before knowing what it means exactly.</span><br><span class="line">spark.sql.parquet.writeLegacyFormat	false	If true, data will be written in a way of Spark 1.4 and earlier. For example, decimal values will be written in Apache Parquet&#x27;s fixed-length byte array format, which other systems such as Apache Hive and Apache Impala use. If false, the newer format in Parquet will be used. For example, decimals will be written in int-based format. If Parquet output is intended for use with systems that do not support this newer format, set to true.</span><br><span class="line">spark.sql.parser.quotedRegexColumnNames	false	When true, quoted Identifiers (using backticks) in SELECT statement are interpreted as regular expressions.</span><br><span class="line">spark.sql.pivotMaxValues	10000	When doing a pivot without specifying values for the pivot column this is the maximum number of (distinct) values that will be collected without error.</span><br><span class="line">spark.sql.queryExecutionListeners	&lt;undefined&gt;	List of class names implementing QueryExecutionListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.</span><br><span class="line">spark.sql.redaction.options.regex	(?i)url	Regex to decide which keys in a Spark SQL command&#x27;s options map contain sensitive information. The values of options whose names that match this regex will be redacted in the explain output. This redaction is applied on top of the global redaction configuration defined by spark.redaction.regex.</span><br><span class="line">spark.sql.redaction.string.regex	&lt;value of spark.redaction.string.regex&gt;	Regex to decide which parts of strings produced by Spark contain sensitive information. When this regex matches a string part, that string part is replaced by a dummy value. This is currently used to redact the output of SQL explain commands. When this conf is not set, the value from `spark.redaction.string.regex` is used.</span><br><span class="line">spark.sql.repl.eagerEval.enabled	false	Enables eager evaluation or not. When true, the top K rows of Dataset will be displayed if and only if the REPL supports the eager evaluation. Currently, the eager evaluation is only supported in PySpark. For the notebooks like Jupyter, the HTML table (generated by _repr_html_) will be returned. For plain Python REPL, the returned outputs are formatted like dataframe.show().</span><br><span class="line">spark.sql.repl.eagerEval.maxNumRows	20	The max number of rows that are returned by eager evaluation. This only takes effect when spark.sql.repl.eagerEval.enabled is set to true. The valid range of this config is from 0 to (Int.MaxValue - 1), so the invalid config like negative and greater than (Int.MaxValue - 1) will be normalized to 0 and (Int.MaxValue - 1).</span><br><span class="line">spark.sql.repl.eagerEval.truncate	20	The max number of characters for each cell that is returned by eager evaluation. This only takes effect when spark.sql.repl.eagerEval.enabled is set to true.</span><br><span class="line">spark.sql.select.object.enabled	true	When true, enable select object for certain schemas files.</span><br><span class="line">spark.sql.session.timeZone	Asia/Shanghai	The ID of session local timezone, e.g. &quot;GMT&quot;, &quot;America/Los_Angeles&quot;, etc.</span><br><span class="line">spark.sql.shuffle.partitions	200	The default number of partitions to use when shuffling data for joins or aggregations.</span><br><span class="line">spark.sql.sources.bucketing.enabled	true	When false, we will treat bucketed table as normal table</span><br><span class="line">spark.sql.sources.bucketing.maxBuckets	100000	The maximum number of buckets allowed. Defaults to 100000</span><br><span class="line">spark.sql.sources.default	parquet	The default data source to use in input/output.</span><br><span class="line">spark.sql.sources.parallelPartitionDiscovery.threshold	32	The maximum number of paths allowed for listing files at driver side. If the number of detected paths exceeds this value during partition discovery, it tries to list the files with another Spark distributed job. This applies to Parquet, ORC, CSV, JSON and LibSVM data sources.</span><br><span class="line">spark.sql.sources.partitionColumnTypeInference.enabled	true	When true, automatically infer the data types for partitioned columns.</span><br><span class="line">spark.sql.sources.partitionOverwriteMode	STATIC	When INSERT OVERWRITE a partitioned data source table, we currently support 2 modes: static and dynamic. In static mode, Spark deletes all the partitions that match the partition specification(e.g. PARTITION(a=1,b)) in the INSERT statement, before overwriting. In dynamic mode, Spark doesn&#x27;t delete partitions ahead, and only overwrite those partitions that have data written into it at runtime. By default we use static mode to keep the same behavior of Spark prior to 2.3. Note that this config doesn&#x27;t affect Hive serde tables, as they are always overwritten with dynamic mode. This can also be set as an output option for a data source using key partitionOverwriteMode (which takes precedence over this setting), e.g. dataframe.write.option(&quot;partitionOverwriteMode&quot;, &quot;dynamic&quot;).save(path).</span><br><span class="line">spark.sql.statistics.fallBackToHdfs	false	If the table statistics are not available from table metadata enable fall back to hdfs. This is useful in determining if a table is small enough to use auto broadcast joins.</span><br><span class="line">spark.sql.statistics.histogram.enabled	false	Generates histograms when computing column statistics if enabled. Histograms can provide better estimation accuracy. Currently, Spark only supports equi-height histogram. Note that collecting histograms takes extra cost. For example, collecting column statistics usually takes only one table scan, but generating equi-height histogram will cause an extra table scan.</span><br><span class="line">spark.sql.statistics.size.autoUpdate.enabled	false	Enables automatic update for table size once table&#x27;s data is changed. Note that if the total number of files of the table is very large, this can be expensive and slow down data change commands.</span><br><span class="line">spark.sql.streaming.checkpointLocation	&lt;undefined&gt;	The default location for storing checkpoint data for streaming queries.</span><br><span class="line">spark.sql.streaming.metricsEnabled	false	Whether Dropwizard/Codahale metrics will be reported for active streaming queries.</span><br><span class="line">spark.sql.streaming.multipleWatermarkPolicy	min	Policy to calculate the global watermark value when there are multiple watermark operators in a streaming query. The default value is &#x27;min&#x27; which chooses the minimum watermark reported across multiple operators. Other alternative value is&#x27;max&#x27; which chooses the maximum across multiple operators.Note: This configuration cannot be changed between query restarts from the same checkpoint location.</span><br><span class="line">spark.sql.streaming.noDataMicroBatches.enabled	true	Whether streaming micro-batch engine will execute batches without data for eager state management for stateful streaming queries.</span><br><span class="line">spark.sql.streaming.numRecentProgressUpdates	100	The number of progress updates to retain for a streaming query</span><br><span class="line">spark.sql.streaming.state.ttl.enable	false	Enable state ttl or not when the output mode is Complete</span><br><span class="line">spark.sql.streaming.streamingQueryListeners	&lt;undefined&gt;	List of class names implementing StreamingQueryListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.</span><br><span class="line">spark.sql.streaming.watermarkPushdown	false	Enables Watermark push-down optimization when set to true.</span><br><span class="line">spark.sql.thriftserver.scheduler.pool	&lt;undefined&gt;	Set a Fair Scheduler pool for a JDBC client session.</span><br><span class="line">spark.sql.thriftserver.ui.retainedSessions	200	The number of SQL client sessions kept in the JDBC/ODBC web UI history.</span><br><span class="line">spark.sql.thriftserver.ui.retainedStatements	200	The number of SQL statements kept in the JDBC/ODBC web UI history.</span><br><span class="line">spark.sql.ui.retainedExecutions	1000	Number of executions to retain in the Spark UI.</span><br><span class="line">spark.sql.uncorrelated.scalar.subquery.preexecution.enabled	false	When true, uncorrelated scalar subquery will be executed before optimizer</span><br><span class="line">spark.sql.uncorrelated.scalar.subquery.preexecution.threshold	53687091200	the output size of the logical plan, which can be pre executed</span><br><span class="line">spark.sql.variable.substitute	true	This enables substitution using syntax like $&#123;var&#125; $&#123;system:var&#125; and $&#123;env:var&#125;.</span><br><span class="line">spark.sql.warehouse.dir	/user/hive/warehouse	The default location for managed databases and tables.</span><br><span class="line">Time taken: 0.035 seconds, Fetched 128 row(s)</span><br><span class="line">21/05/30 02:58:57 INFO [main] SparkSQLCLIDriver: Time taken: 0.035 seconds, Fetched 128 row(s)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>以上是spark-sql全部的调优参数，下面列举一些生产环境中常用的优化参数来介绍：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> spark.sql.crossJoin.enabled=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>

<p>当为false时，如果查询包含没有显式CROSS JOIN语法的笛卡尔积，我们将抛出错误。</p>
<p>有些需求，需要和日期做笛卡尔积以求得每一天的具体情况，spark-sql默认是不开启笛卡尔积的，此处需要开启</p>
]]></content>
  </entry>
</search>

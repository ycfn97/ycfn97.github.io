<!DOCTYPE html>


<html lang="zh_CN">


<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
     the Atlantic
  </title>
  <meta name="generator" content="hexo-theme-ayer">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/dist/main.css">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css">
  
<link rel="stylesheet" href="/css/custom.css">

  
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
  
        <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?029e85d94060b2a4054a349dc92bef7f";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
  

  

</head>

</html>

<body>
  <div id="app">
    
      
      <canvas width="1777" height="841"
        style="position: fixed; left: 0px; top: 0px; z-index: 99999; pointer-events: none;"></canvas>
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-spark-sql常用运行时调优"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
     
    <div class="article-meta">
      <a href="/2021/05/30/spark-sql%E5%B8%B8%E7%94%A8%E8%BF%90%E8%A1%8C%E6%97%B6%E8%B0%83%E4%BC%98/" class="article-date">
  <time datetime="2021-05-29T18:54:56.016Z" itemprop="datePublished">2021-05-30</time>
</a>   
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">4.5k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">27 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <hr>
<h2 id="spark-sql-常用运行时调优"><a href="#spark-sql-常用运行时调优" class="headerlink" title="spark-sql 常用运行时调优"></a>spark-sql 常用运行时调优</h2><p>进入spark-sql客户端：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-sql</span><br></pre></td></tr></table></figure>

<p>运行命令获取帮助：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line">         &gt; set -v;</span><br><span class="line">spark.sql.adaptive.allowAdditionalShuffle	false	When true, additional shuffles are allowed during plan optimizations in adaptive execution</span><br><span class="line">spark.sql.adaptive.enabled	false	When true, enable adaptive query execution.</span><br><span class="line">spark.sql.adaptive.join.enabled	true	When true and adaptive execution is enabled, a better join strategy is determined at runtime.</span><br><span class="line">spark.sql.adaptive.maxNumPostShufflePartitions	500	The advisory maximum number of post-shuffle partitions used in adaptive execution.</span><br><span class="line">spark.sql.adaptive.minNumPostShufflePartitions	1	The advisory minimum number of post-shuffle partitions used in adaptive execution.</span><br><span class="line">spark.sql.adaptive.shuffle.targetPostShuffleInputSize	67108864b	The target post-shuffle input size in bytes of a task.</span><br><span class="line">spark.sql.adaptive.shuffle.targetPostShuffleRowCount	20000000	The target post-shuffle row count of a task.</span><br><span class="line">spark.sql.adaptive.skewedJoin.enabled	false	When true and adaptive execution is enabled, a skewed join is automatically handled at runtime.</span><br><span class="line">spark.sql.adaptive.skewedPartitionFactor	10	A partition is considered as a skewed partition if its size is larger than this factor multiple the median partition size and also larger than spark.sql.adaptive.skewedPartitionSizeThreshold, or if its row count is larger than this factor multiple the median row count and also larger than spark.sql.adaptive.skewedPartitionRowCountThreshold.</span><br><span class="line">spark.sql.adaptive.skewedPartitionMaxSplits	5	Configures the maximum number of task to handle a skewed partition in adaptive skewedjoin.</span><br><span class="line">spark.sql.adaptive.skewedPartitionRowCountThreshold	10000000	Configures the minimum row count for a partition that is considered as a skewed partition in adaptive skewed join.</span><br><span class="line">spark.sql.adaptive.skewedPartitionSizeThreshold	67108864b	Configures the minimum size in bytes for a partition that is considered as a skewed partition in adaptive skewed join.</span><br><span class="line">spark.sql.adaptiveBroadcastJoinThreshold	&lt;undefined&gt;	Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join in adaptive exeuction mode. If not set, it equals to spark.sql.autoBroadcastJoinThreshold.</span><br><span class="line">spark.sql.autoBroadcastJoinThreshold	10485760	Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join.  By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command &lt;code&gt;ANALYZE TABLE &amp;lt;tableName&amp;gt; COMPUTE STATISTICS noscan&lt;/code&gt; has been run, and file-based data source tables where the statistics are computed directly on the files of data.</span><br><span class="line">spark.sql.avro.compression.codec	snappy	Compression codec used in writing of AVRO files. Supported codecs: uncompressed, deflate, snappy, bzip2 and xz. Default codec is snappy.</span><br><span class="line">spark.sql.avro.deflate.level	-1	Compression level for the deflate codec used in writing of AVRO files. Valid value must be in the range of from 1 to 9 inclusive or -1. The default value is -1 which corresponds to 6 level in the current implementation.</span><br><span class="line">spark.sql.broadcastTimeout	300000ms	Timeout in seconds for the broadcast wait time in broadcast joins.</span><br><span class="line">spark.sql.cache.meta.storage.path	/opt/apps/ecm/service/spark/2.4.5-hadoop3.1-1.0.0/package/spark-2.4.5-hadoop3.1-1.0.0	Local level db location to store some Cube Management metadata.</span><br><span class="line">spark.sql.cache.queryRewrite	true	When true, spark try to use previous built Cube Management to optimized input queries</span><br><span class="line">spark.sql.cbo.enabled	false	Enables CBO for estimation of plan statistics when set true.</span><br><span class="line">spark.sql.cbo.joinReorder.dp.star.filter	false	Applies star-join filter heuristics to cost based join enumeration.</span><br><span class="line">spark.sql.cbo.joinReorder.dp.threshold	12	The maximum number of joined nodes allowed in the dynamic programming algorithm.</span><br><span class="line">spark.sql.cbo.joinReorder.enabled	false	Enables join reorder in CBO.</span><br><span class="line">spark.sql.cbo.joinReorder.ga.num.generations	0	The maximum generations for the evolution to get a solution.</span><br><span class="line">spark.sql.cbo.joinReorder.ga.pool.size	0	The pool size (population) of the chromosomes in the Genetic algorithm.</span><br><span class="line">spark.sql.cbo.joinReorder.ga.selection.bias	2.0	The bias used for selecting the chromosome from the pool.</span><br><span class="line">spark.sql.cbo.joinReorder.ga.trade.factor	5	The trade-off factor between planning time and query plan quality. It should be an integer from 1 to 10. Larger value will increase the planing time which may generate a better plan. 5 is the default.</span><br><span class="line">spark.sql.cbo.outerJoinReorder.enabled	false	Enables outer join reorder in CBO.</span><br><span class="line">spark.sql.cbo.starSchemaDetection	false	When true, it enables join reordering based on star schema detection. </span><br><span class="line">spark.sql.columnNameOfCorruptRecord	_corrupt_record	The name of internal column for storing raw/un-parsed JSON and CSV records that fail to parse.</span><br><span class="line">spark.sql.crossJoin.enabled	false	When false, we will throw an error if a query contains a cartesian product without explicit CROSS JOIN syntax.</span><br><span class="line">spark.sql.delta.merge.retries	3	The max number of retries for merging into delta lake in case of `ConcurrentWriteException`.</span><br><span class="line">spark.sql.delta.merge.sourceQuery.executionMode	batch	The execution mode of source query. Supported mode: batch and stream. Default mode is batch.</span><br><span class="line">spark.sql.dynamic.runtime.filter.bbf.enabled	false	When true, enable dynamic runtime filter using bbf</span><br><span class="line">spark.sql.dynamic.runtime.filter.enabled	false	When true, enable dynamic runtime filter</span><br><span class="line">spark.sql.dynamic.runtime.filter.table.size.lower.limit	53687091200	big table side should be greater than this value</span><br><span class="line">spark.sql.dynamic.runtime.filter.table.size.upper.limit	5368709120	small table side should be smaller than this value</span><br><span class="line">spark.sql.execution.arrow.enabled	false	When true, make use of Apache Arrow for columnar data transfers. Currently available for use with pyspark.sql.DataFrame.toPandas, and pyspark.sql.SparkSession.createDataFrame when its input is a Pandas DataFrame. The following data types are unsupported: BinaryType, MapType, ArrayType of TimestampType, and nested StructType.</span><br><span class="line">spark.sql.execution.arrow.fallback.enabled	true	When true, optimizations enabled by &#x27;spark.sql.execution.arrow.enabled&#x27; will fallback automatically to non-optimized implementations if an error occurs.</span><br><span class="line">spark.sql.execution.arrow.maxRecordsPerBatch	10000	When using Apache Arrow, limit the maximum number of records that can be written to a single ArrowRecordBatch in memory. If set to zero or negative there is no limit.</span><br><span class="line">spark.sql.extensions	io.delta.sql.DeltaSparkSessionExtension,org.apache.spark.sql.emr.EmrSparkSessionExtension	A comma-separated list of classes that implement Function1[SparkSessionExtension, Unit] used to configure Spark Session extensions. The classes must have a no-args constructor. If multiple extensions are specified, they are applied in the specified order. For the case of rules and planner strategies, they are applied in the specified order. For the case of parsers, the last parser is used and each parser can delegate to its predecessor.</span><br><span class="line">spark.sql.extract.common.conjunct.filter	false	When true, common conjunct in filter conditition will be extracted, which will be pushed down later</span><br><span class="line">spark.sql.files.binPackingStrategy	efficiency-oriented	THIS IS AN INTERNAL CONFIG WHICH SHOULD NOT BE EXPOSED TO CUSTOMER. The strategy for bin-packing when scanning files, which can be &#x27;efficiency-oriented&#x27; or &#x27;size-oriented&#x27;, and &#x27;efficiency-oriented&#x27; is the default. &#x27;efficiency-oriented&#x27; tries to pack files into partitions via an algorithm which aims to read the files as soon as possible, while &#x27;size-oriented&#x27; will try to pack as many files as possible into a single partition, but the size of the partition should be less than &#x27;spark.sql.files.maxPartitionBytes&#x27; defines.</span><br><span class="line">spark.sql.files.ignoreCorruptFiles	false	Whether to ignore corrupt files. If true, the Spark jobs will continue to run when encountering corrupted files and the contents that have been read will still be returned.</span><br><span class="line">spark.sql.files.ignoreMissingFiles	false	Whether to ignore missing files. If true, the Spark jobs will continue to run when encountering missing files and the contents that have been read will still be returned.</span><br><span class="line">spark.sql.files.maxPartitionBytes	134217728	The maximum number of bytes to pack into a single partition when reading files.</span><br><span class="line">spark.sql.files.maxRecordsPerFile	0	Maximum number of records to write out to a single file. If this value is zero or negative, there is no limit.</span><br><span class="line">spark.sql.function.concatBinaryAsString	false	When this option is set to false and all inputs are binary, `functions.concat` returns an output as binary. Otherwise, it returns as a string. </span><br><span class="line">spark.sql.function.eltOutputAsString	false	When this option is set to false and all inputs are binary, `elt` returns an output as binary. Otherwise, it returns as a string. </span><br><span class="line">spark.sql.groupByAliases	true	When true, aliases in a select list can be used in group by clauses. When false, an analysis exception is thrown in the case.</span><br><span class="line">spark.sql.groupByOrdinal	true	When true, the ordinal numbers in group by clauses are treated as the position in the select list. When false, the ordinal numbers are ignored.</span><br><span class="line">spark.sql.hive.caseSensitiveInferenceMode	INFER_AND_SAVE	Sets the action to take when a case-sensitive schema cannot be read from a Hive table&#x27;s properties. Although Spark SQL itself is not case-sensitive, Hive compatible file formats such as Parquet are. Spark SQL must use a case-preserving schema when querying any table backed by files containing case-sensitive field names or queries may not return accurate results. Valid options include INFER_AND_SAVE (the default mode-- infer the case-sensitive schema from the underlying data files and write it back to the table properties), INFER_ONLY (infer the schema but don&#x27;t attempt to write it to the table properties) and NEVER_INFER (fallback to using the case-insensitive metastore schema instead of inferring).</span><br><span class="line">spark.sql.hive.convertMetastoreOrc	true	When set to true, the built-in ORC reader and writer are used to process ORC tables created by using the HiveQL syntax, instead of Hive serde.</span><br><span class="line">spark.sql.hive.convertMetastoreParquet	true	When set to true, the built-in Parquet reader and writer are used to process parquet tables created by using the HiveQL syntax, instead of Hive serde.</span><br><span class="line">spark.sql.hive.convertMetastoreParquet.mergeSchema	false	When true, also tries to merge possibly different but compatible Parquet schemas in different Parquet data files. This configuration is only effective when &quot;spark.sql.hive.convertMetastoreParquet&quot; is true.</span><br><span class="line">spark.sql.hive.filesourcePartitionFileCacheSize	262144000	When nonzero, enable caching of partition file metadata in memory. All tables share a cache that can use up to specified num bytes for file metadata. This conf only has an effect when hive filesource partition management is enabled.</span><br><span class="line">spark.sql.hive.manageFilesourcePartitions	true	When true, enable metastore partition management for file source tables as well. This includes both datasource and converted Hive tables. When partition management is enabled, datasource tables store partition in the Hive metastore, and use the metastore to prune partitions during query planning.</span><br><span class="line">spark.sql.hive.metastore.barrierPrefixes		A comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e. &lt;code&gt;org.apache.spark.*&lt;/code&gt;).</span><br><span class="line">spark.sql.hive.metastore.jars	builtin	</span><br><span class="line"> Location of the jars that should be used to instantiate the HiveMetastoreClient.</span><br><span class="line"> This property can be one of three options: </span><br><span class="line"> 1. &quot;builtin&quot;</span><br><span class="line">   Use Hive 2.3.5, which is bundled with the Spark assembly when</span><br><span class="line">   &lt;code&gt;-Phive&lt;/code&gt; is enabled. When this option is chosen,</span><br><span class="line">   &lt;code&gt;spark.sql.hive.metastore.version&lt;/code&gt; must be either</span><br><span class="line">   &lt;code&gt;2.3.5&lt;/code&gt; or not defined.</span><br><span class="line"> 2. &quot;maven&quot;</span><br><span class="line">   Use Hive jars of specified version downloaded from Maven repositories.</span><br><span class="line"> 3. A classpath in the standard format for both Hive and Hadoop.</span><br><span class="line">      </span><br><span class="line">spark.sql.hive.metastore.sharedPrefixes	com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc	A comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log4j.</span><br><span class="line">spark.sql.hive.metastore.version	2.3.5	Version of the Hive metastore. Available options are &lt;code&gt;0.12.0&lt;/code&gt; through &lt;code&gt;2.3.3&lt;/code&gt;.</span><br><span class="line">spark.sql.hive.metastorePartitionPruning	true	When true, some predicates will be pushed down into the Hive metastore so that unmatching partitions can be eliminated earlier. This only affects Hive tables not converted to filesource relations (see HiveUtils.CONVERT_METASTORE_PARQUET and HiveUtils.CONVERT_METASTORE_ORC for more information).</span><br><span class="line">spark.sql.hive.thriftServer.async	true	When set to true, Hive Thrift server executes SQL queries in an asynchronous way.</span><br><span class="line">spark.sql.hive.thriftServer.singleSession	false	When set to true, Hive Thrift server is running in a single session mode. All the JDBC/ODBC connections share the temporary views, function registries, SQL configuration and the current database.</span><br><span class="line">spark.sql.hive.verifyPartitionPath	false	When true, check all the partition paths under the table&#x27;s root directory when reading data stored in HDFS. This configuration will be deprecated in the future releases and replaced by spark.files.ignoreMissingFiles.</span><br><span class="line">spark.sql.hive.version	2.3.5	deprecated, please use spark.sql.hive.metastore.version to get the Hive version in Spark.</span><br><span class="line">spark.sql.inMemoryColumnarStorage.batchSize	10000	Controls the size of batches for columnar caching.  Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data.</span><br><span class="line">spark.sql.inMemoryColumnarStorage.compressed	true	When set to true Spark SQL will automatically select a compression codec for each column based on statistics of the data.</span><br><span class="line">spark.sql.inMemoryColumnarStorage.enableVectorizedReader	true	Enables vectorized reader for columnar caching.</span><br><span class="line">spark.sql.infer.filter.from.joincondition	false	When true, infer filter from join, which could be pushed down later</span><br><span class="line">spark.sql.intersect.groupby.placement	false	When true, distinct(join) will be placement by join(group by)</span><br><span class="line">spark.sql.legacy.replaceDatabricksSparkAvro.enabled	true	If it is set to true, the data source provider com.databricks.spark.avro is mapped to the built-in but external Avro data source module for backward compatibility.</span><br><span class="line">spark.sql.legacy.sizeOfNull	true	If it is set to true, size of null returns -1. This behavior was inherited from Hive. The size function returns null for null input if the flag is disabled.</span><br><span class="line">spark.sql.optimizer.excludedRules	&lt;undefined&gt;	Configures a list of rules to be disabled in the optimizer, in which the rules are specified by their rule names and separated by comma. It is not guaranteed that all the rules in this configuration will eventually be excluded, as some rules are necessary for correctness. The optimizer will log the rules that have indeed been excluded.</span><br><span class="line">spark.sql.orc.columnarReaderBatchSize	4096	The number of rows to include in a orc vectorized reader batch. The number should be carefully chosen to minimize overhead and avoid OOMs in reading data.</span><br><span class="line">spark.sql.orc.compression.codec	snappy	Sets the compression codec used when writing ORC files. If either `compression` or `orc.compress` is specified in the table-specific options/properties, the precedence would be `compression`, `orc.compress`, `spark.sql.orc.compression.codec`.Acceptable values include: none, uncompressed, snappy, zlib, lzo.</span><br><span class="line">spark.sql.orc.enableVectorizedReader	true	Enables vectorized orc decoding.</span><br><span class="line">spark.sql.orc.filterPushdown	true	When true, enable filter pushdown for ORC files.</span><br><span class="line">spark.sql.orderByOrdinal	true	When true, the ordinal numbers are treated as the position in the select list. When false, the ordinal numbers in order/sort by clause are ignored.</span><br><span class="line">spark.sql.parquet.binaryAsString	false	Some other Parquet-producing systems, in particular Impala and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.</span><br><span class="line">spark.sql.parquet.columnarReaderBatchSize	4096	The number of rows to include in a parquet vectorized reader batch. The number should be carefully chosen to minimize overhead and avoid OOMs in reading data.</span><br><span class="line">spark.sql.parquet.compression.codec	snappy	Sets the compression codec used when writing Parquet files. If either `compression` or `parquet.compression` is specified in the table-specific options/properties, the precedence would be `compression`, `parquet.compression`, `spark.sql.parquet.compression.codec`. Acceptable values include: none, uncompressed, snappy, gzip, lzo, brotli, lz4, zstd.</span><br><span class="line">spark.sql.parquet.enableVectorizedReader	true	Enables vectorized parquet decoding.</span><br><span class="line">spark.sql.parquet.filterPushdown	true	Enables Parquet filter push-down optimization when set to true.</span><br><span class="line">spark.sql.parquet.int64AsTimestampMillis	false	(Deprecated since Spark 2.3, please set spark.sql.parquet.outputTimestampType.) When true, timestamp values will be stored as INT64 with TIMESTAMP_MILLIS as the extended type. In this mode, the microsecond portion of the timestamp value will betruncated.</span><br><span class="line">spark.sql.parquet.int96AsTimestamp	true	Some Parquet-producing systems, in particular Impala, store Timestamp into INT96. Spark would also store Timestamp as INT96 because we need to avoid precision lost of the nanoseconds field. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.</span><br><span class="line">spark.sql.parquet.int96TimestampConversion	false	This controls whether timestamp adjustments should be applied to INT96 data when converting to timestamps, for data written by Impala.  This is necessary because Impala stores INT96 data with a different timezone offset than Hive &amp; Spark.</span><br><span class="line">spark.sql.parquet.mergeSchema	false	When true, the Parquet data source merges schemas collected from all data files, otherwise the schema is picked from the summary file or a random data file if no summary file is available.</span><br><span class="line">spark.sql.parquet.outputTimestampType	INT96	Sets which Parquet timestamp type to use when Spark writes data to Parquet files. INT96 is a non-standard but commonly used timestamp type in Parquet. TIMESTAMP_MICROS is a standard timestamp type in Parquet, which stores number of microseconds from the Unix epoch. TIMESTAMP_MILLIS is also standard, but with millisecond precision, which means Spark has to truncate the microsecond portion of its timestamp value.</span><br><span class="line">spark.sql.parquet.recordLevelFilter.enabled	false	If true, enables Parquet&#x27;s native record-level filtering using the pushed down filters. This configuration only has an effect when &#x27;spark.sql.parquet.filterPushdown&#x27; is enabled and the vectorized reader is not used. You can ensure the vectorized reader is not used by setting &#x27;spark.sql.parquet.enableVectorizedReader&#x27; to false.</span><br><span class="line">spark.sql.parquet.respectSummaryFiles	false	When true, we make assumption that all part-files of Parquet are consistent with summary files and we will ignore them when merging schema. Otherwise, if this is false, which is the default, we will merge all part-files. This should be considered as expert-only option, and shouldn&#x27;t be enabled before knowing what it means exactly.</span><br><span class="line">spark.sql.parquet.writeLegacyFormat	false	If true, data will be written in a way of Spark 1.4 and earlier. For example, decimal values will be written in Apache Parquet&#x27;s fixed-length byte array format, which other systems such as Apache Hive and Apache Impala use. If false, the newer format in Parquet will be used. For example, decimals will be written in int-based format. If Parquet output is intended for use with systems that do not support this newer format, set to true.</span><br><span class="line">spark.sql.parser.quotedRegexColumnNames	false	When true, quoted Identifiers (using backticks) in SELECT statement are interpreted as regular expressions.</span><br><span class="line">spark.sql.pivotMaxValues	10000	When doing a pivot without specifying values for the pivot column this is the maximum number of (distinct) values that will be collected without error.</span><br><span class="line">spark.sql.queryExecutionListeners	&lt;undefined&gt;	List of class names implementing QueryExecutionListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.</span><br><span class="line">spark.sql.redaction.options.regex	(?i)url	Regex to decide which keys in a Spark SQL command&#x27;s options map contain sensitive information. The values of options whose names that match this regex will be redacted in the explain output. This redaction is applied on top of the global redaction configuration defined by spark.redaction.regex.</span><br><span class="line">spark.sql.redaction.string.regex	&lt;value of spark.redaction.string.regex&gt;	Regex to decide which parts of strings produced by Spark contain sensitive information. When this regex matches a string part, that string part is replaced by a dummy value. This is currently used to redact the output of SQL explain commands. When this conf is not set, the value from `spark.redaction.string.regex` is used.</span><br><span class="line">spark.sql.repl.eagerEval.enabled	false	Enables eager evaluation or not. When true, the top K rows of Dataset will be displayed if and only if the REPL supports the eager evaluation. Currently, the eager evaluation is only supported in PySpark. For the notebooks like Jupyter, the HTML table (generated by _repr_html_) will be returned. For plain Python REPL, the returned outputs are formatted like dataframe.show().</span><br><span class="line">spark.sql.repl.eagerEval.maxNumRows	20	The max number of rows that are returned by eager evaluation. This only takes effect when spark.sql.repl.eagerEval.enabled is set to true. The valid range of this config is from 0 to (Int.MaxValue - 1), so the invalid config like negative and greater than (Int.MaxValue - 1) will be normalized to 0 and (Int.MaxValue - 1).</span><br><span class="line">spark.sql.repl.eagerEval.truncate	20	The max number of characters for each cell that is returned by eager evaluation. This only takes effect when spark.sql.repl.eagerEval.enabled is set to true.</span><br><span class="line">spark.sql.select.object.enabled	true	When true, enable select object for certain schemas files.</span><br><span class="line">spark.sql.session.timeZone	Asia/Shanghai	The ID of session local timezone, e.g. &quot;GMT&quot;, &quot;America/Los_Angeles&quot;, etc.</span><br><span class="line">spark.sql.shuffle.partitions	200	The default number of partitions to use when shuffling data for joins or aggregations.</span><br><span class="line">spark.sql.sources.bucketing.enabled	true	When false, we will treat bucketed table as normal table</span><br><span class="line">spark.sql.sources.bucketing.maxBuckets	100000	The maximum number of buckets allowed. Defaults to 100000</span><br><span class="line">spark.sql.sources.default	parquet	The default data source to use in input/output.</span><br><span class="line">spark.sql.sources.parallelPartitionDiscovery.threshold	32	The maximum number of paths allowed for listing files at driver side. If the number of detected paths exceeds this value during partition discovery, it tries to list the files with another Spark distributed job. This applies to Parquet, ORC, CSV, JSON and LibSVM data sources.</span><br><span class="line">spark.sql.sources.partitionColumnTypeInference.enabled	true	When true, automatically infer the data types for partitioned columns.</span><br><span class="line">spark.sql.sources.partitionOverwriteMode	STATIC	When INSERT OVERWRITE a partitioned data source table, we currently support 2 modes: static and dynamic. In static mode, Spark deletes all the partitions that match the partition specification(e.g. PARTITION(a=1,b)) in the INSERT statement, before overwriting. In dynamic mode, Spark doesn&#x27;t delete partitions ahead, and only overwrite those partitions that have data written into it at runtime. By default we use static mode to keep the same behavior of Spark prior to 2.3. Note that this config doesn&#x27;t affect Hive serde tables, as they are always overwritten with dynamic mode. This can also be set as an output option for a data source using key partitionOverwriteMode (which takes precedence over this setting), e.g. dataframe.write.option(&quot;partitionOverwriteMode&quot;, &quot;dynamic&quot;).save(path).</span><br><span class="line">spark.sql.statistics.fallBackToHdfs	false	If the table statistics are not available from table metadata enable fall back to hdfs. This is useful in determining if a table is small enough to use auto broadcast joins.</span><br><span class="line">spark.sql.statistics.histogram.enabled	false	Generates histograms when computing column statistics if enabled. Histograms can provide better estimation accuracy. Currently, Spark only supports equi-height histogram. Note that collecting histograms takes extra cost. For example, collecting column statistics usually takes only one table scan, but generating equi-height histogram will cause an extra table scan.</span><br><span class="line">spark.sql.statistics.size.autoUpdate.enabled	false	Enables automatic update for table size once table&#x27;s data is changed. Note that if the total number of files of the table is very large, this can be expensive and slow down data change commands.</span><br><span class="line">spark.sql.streaming.checkpointLocation	&lt;undefined&gt;	The default location for storing checkpoint data for streaming queries.</span><br><span class="line">spark.sql.streaming.metricsEnabled	false	Whether Dropwizard/Codahale metrics will be reported for active streaming queries.</span><br><span class="line">spark.sql.streaming.multipleWatermarkPolicy	min	Policy to calculate the global watermark value when there are multiple watermark operators in a streaming query. The default value is &#x27;min&#x27; which chooses the minimum watermark reported across multiple operators. Other alternative value is&#x27;max&#x27; which chooses the maximum across multiple operators.Note: This configuration cannot be changed between query restarts from the same checkpoint location.</span><br><span class="line">spark.sql.streaming.noDataMicroBatches.enabled	true	Whether streaming micro-batch engine will execute batches without data for eager state management for stateful streaming queries.</span><br><span class="line">spark.sql.streaming.numRecentProgressUpdates	100	The number of progress updates to retain for a streaming query</span><br><span class="line">spark.sql.streaming.state.ttl.enable	false	Enable state ttl or not when the output mode is Complete</span><br><span class="line">spark.sql.streaming.streamingQueryListeners	&lt;undefined&gt;	List of class names implementing StreamingQueryListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.</span><br><span class="line">spark.sql.streaming.watermarkPushdown	false	Enables Watermark push-down optimization when set to true.</span><br><span class="line">spark.sql.thriftserver.scheduler.pool	&lt;undefined&gt;	Set a Fair Scheduler pool for a JDBC client session.</span><br><span class="line">spark.sql.thriftserver.ui.retainedSessions	200	The number of SQL client sessions kept in the JDBC/ODBC web UI history.</span><br><span class="line">spark.sql.thriftserver.ui.retainedStatements	200	The number of SQL statements kept in the JDBC/ODBC web UI history.</span><br><span class="line">spark.sql.ui.retainedExecutions	1000	Number of executions to retain in the Spark UI.</span><br><span class="line">spark.sql.uncorrelated.scalar.subquery.preexecution.enabled	false	When true, uncorrelated scalar subquery will be executed before optimizer</span><br><span class="line">spark.sql.uncorrelated.scalar.subquery.preexecution.threshold	53687091200	the output size of the logical plan, which can be pre executed</span><br><span class="line">spark.sql.variable.substitute	true	This enables substitution using syntax like $&#123;var&#125; $&#123;system:var&#125; and $&#123;env:var&#125;.</span><br><span class="line">spark.sql.warehouse.dir	/user/hive/warehouse	The default location for managed databases and tables.</span><br><span class="line">Time taken: 0.035 seconds, Fetched 128 row(s)</span><br><span class="line">21/05/30 02:58:57 INFO [main] SparkSQLCLIDriver: Time taken: 0.035 seconds, Fetched 128 row(s)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>以上是spark-sql全部的调优参数，下面列举一些生产环境中常用的优化参数来介绍：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> spark.sql.crossJoin.enabled=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>

<p>当为false时，如果查询包含没有显式CROSS JOIN语法的笛卡尔积，我们将抛出错误。</p>
<p>有些需求，需要和日期做笛卡尔积以求得每一天的具体情况，spark-sql默认是不开启笛卡尔积的，此处需要开启</p>
 
      <!-- reward -->
      
      <div id="reword-out">
        <div id="reward-btn">
          Donate
        </div>
      </div>
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>Copyright： </strong>
          
          Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://ycfn97.xyz/2021/05/30/spark-sql%E5%B8%B8%E7%94%A8%E8%BF%90%E8%A1%8C%E6%97%B6%E8%B0%83%E4%BC%98/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2021/07/08/flink%E8%AF%BB%E5%8F%96mq%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E6%B5%81%E5%BC%8F%E5%86%99%E5%85%A5hive%E5%B9%B6%E5%90%88%E5%B9%B6hdfs%E5%88%86%E5%8C%BA%E5%B0%8F%E6%96%87%E4%BB%B6/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            flink读取mq数据实时流式写入hive并合并hdfs分区小文件
          
        </div>
      </a>
    
    
      <a href="/2021/05/26/sqoop%E5%9C%A8%E5%AE%9E%E9%99%85%E7%94%9F%E4%BA%A7%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">sqoop在实际生产中的使用经验总结</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "",
    app_key: "",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020-2021
        <i class="ri-heart-fill heart_icon"></i> ycfn97
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        Powered by <a href="https://hexo.io" target="_blank">Hexo</a>
        <span class="division">|</span>
        Theme - <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></s>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="the Atlantic"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E6%97%85%E8%A1%8C/">旅行</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">摄影</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2019/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->


<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: 'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto'
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>

<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->


<script src="/js/clickBoom2.js"></script>


<!-- CodeCopy -->


<link rel="stylesheet" href="/css/clipboard.css">

<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>


<!-- CanvasBackground -->


<script src="/js/dz.js"></script>



    
    <div id="music">
    
    
    
    <iframe frameborder="no" border="1" marginwidth="0" marginheight="0" width="200" height="86"
        src="//music.163.com/outchain/player?type=2&id=507109202&auto=1&height=66"></iframe>
</div>

<style>
    #music {
        position: fixed;
        right: 15px;
        bottom: 0;
        z-index: 998;
    }
</style>
    
  </div>
</body>

</html>
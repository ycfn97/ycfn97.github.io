<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>clickhouse作为实时和离线计算引擎从kafka消费数据</title>
    <url>/2020/12/04/clickhouse%E4%BD%9C%E4%B8%BA%E5%AE%9E%E6%97%B6%E5%92%8C%E7%A6%BB%E7%BA%BF%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E%E4%BB%8Ekafka%E6%B6%88%E8%B4%B9%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<p><img src="https://clickhouse.tech/images/index/intro.svg" alt="ClickHouse is capable of generating analytical data reports in real time, with sub-second latencies">    </p>
<p>​    由于clickhouse超高的查询性能，和使用sql开发的低成本，我们不禁想到让clickhouse作为计算引擎和存储介质直接分析存储在其中的海量数据，并配合其MergeTree表引擎特有的TTL功能做实时数据分析。</p>
<a id="more"></a>



<blockquote>
<p>本文使用到的表引擎：</p>
<p>kafka</p>
<p>Distributed</p>
<p>ReplicatedMergeTree</p>
</blockquote>
<p>使用kafka-eagle创建topic：</p>
<p><img src="/2020/12/04/clickhouse%E4%BD%9C%E4%B8%BA%E5%AE%9E%E6%97%B6%E5%92%8C%E7%A6%BB%E7%BA%BF%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E%E4%BB%8Ekafka%E6%B6%88%E8%B4%B9%E6%95%B0%E6%8D%AE/kafka-eagle.jpg"></p>
<p>在控制台创建kafka消费者并指定消费者组与kafka表中的<code>kafka_group_name</code>保持一致：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --topic clickhouse_test01 --bootstrap-server hadoop01:9092,hadoop02:9092,hadoop03:9092 --group clickhouse_group</span><br></pre></td></tr></table></figure>



<p>clickhouse集群副本及其分片配置如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">┌─cluster──────────────────────────────────────┬─shard_num─┬─shard_weight─┬─replica_num─┬─host_name─┬─host_address──┬─port─┬─is_local─┬─user────┬─default_database─┬─errors_count─┬─estimated_recovery_time─┐</span><br><span class="line">│ gmall_cluster                                │         1 │            1 │           1 │ hadoop01  │ 192.168.150.4 │ 9000 │        0 │ default │                  │            0 │                       0 │</span><br><span class="line">│ gmall_cluster                                │         1 │            1 │           2 │ hadoop02  │ 192.168.150.5 │ 9000 │        1 │ default │                  │            0 │                       0 │</span><br><span class="line">│ gmall_cluster                                │         2 │            1 │           1 │ hadoop03  │ 192.168.150.6 │ 9000 │        0 │ default │                  │            0 │                       0 │</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>首先使用kafka表引擎创建集群表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> default.kafka_src_table <span class="keyword">ON</span> CLUSTER gmall_cluster</span><br><span class="line">(</span><br><span class="line">    <span class="string">`id`</span> Int32,</span><br><span class="line">    <span class="string">`age`</span> Int32,</span><br><span class="line">    <span class="string">`msg`</span> <span class="keyword">String</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ENGINE</span> = Kafka()</span><br><span class="line"><span class="keyword">SETTINGS</span> kafka_broker_list = <span class="string">&#x27;hadoop01:9092,hadoop02:9092,hadoop03:9092&#x27;</span>, kafka_topic_list = <span class="string">&#x27;clickhouse_test01&#x27;</span>, kafka_group_name = <span class="string">&#x27;clickhouse_group&#x27;</span>, kafka_format = <span class="string">&#x27;JSONEachRow&#x27;</span></span><br><span class="line"></span><br><span class="line">┌─host─────┬─port─┬─<span class="keyword">status</span>─┬─<span class="keyword">error</span>─┬─num_hosts_remaining─┬─num_hosts_active─┐</span><br><span class="line">│ hadoop02 │ <span class="number">9000</span> │      <span class="number">0</span> │       │                   <span class="number">2</span> │                <span class="number">0</span> │</span><br><span class="line">│ hadoop03 │ <span class="number">9000</span> │      <span class="number">0</span> │       │                   <span class="number">1</span> │                <span class="number">0</span> │</span><br><span class="line">│ hadoop01 │ <span class="number">9000</span> │      <span class="number">0</span> │       │                   <span class="number">0</span> │                <span class="number">0</span> │</span><br><span class="line">└──────────┴──────┴────────┴───────┴─────────────────────┴──────────────────┘</span><br><span class="line"></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> <span class="keyword">in</span> set. Elapsed: <span class="number">0.130</span> sec. </span><br></pre></td></tr></table></figure>



<p>使用ReplacingMergeTree 弱幂等性表引擎创建集群分片表:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE default.kafka_table_local ON CLUSTER gmall_cluster</span><br><span class="line">(</span><br><span class="line">    &#96;id&#96; Int32,</span><br><span class="line">    &#96;age&#96; UInt32,</span><br><span class="line">    &#96;msg&#96; String</span><br><span class="line">)</span><br><span class="line">ENGINE &#x3D; ReplicatedMergeTree(&#39;&#x2F;clickhouse&#x2F;tables&#x2F;kafka_sink_table&#x2F;&#123;shard&#125;&#39;, &#39;&#123;replica&#125;&#39;)</span><br><span class="line">ORDER BY id</span><br><span class="line"></span><br><span class="line">┌─host─────┬─port─┬─status─┬─error─┬─num_hosts_remaining─┬─num_hosts_active─┐</span><br><span class="line">│ hadoop02 │ 9000 │      0 │       │                   2 │                0 │</span><br><span class="line">│ hadoop03 │ 9000 │      0 │       │                   1 │                0 │</span><br><span class="line">│ hadoop01 │ 9000 │      0 │       │                   0 │                0 │</span><br><span class="line">└──────────┴──────┴────────┴───────┴─────────────────────┴──────────────────┘</span><br><span class="line"></span><br><span class="line">3 rows in set. Elapsed: 0.294 sec. </span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>使用distributed引擎创建分布式集群表：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE kafka_table_distributed ON CLUSTER gmall_cluster</span><br><span class="line">(</span><br><span class="line">    &#96;id&#96; Int32,</span><br><span class="line">    &#96;age&#96; UInt32,</span><br><span class="line">    &#96;msg&#96; String</span><br><span class="line">)</span><br><span class="line">ENGINE &#x3D; Distributed(gmall_cluster, default, kafka_table_local, id)</span><br><span class="line"></span><br><span class="line">┌─host─────┬─port─┬─status─┬─error─┬─num_hosts_remaining─┬─num_hosts_active─┐</span><br><span class="line">│ hadoop02 │ 9000 │      0 │       │                   2 │                0 │</span><br><span class="line">│ hadoop03 │ 9000 │      0 │       │                   1 │                0 │</span><br><span class="line">│ hadoop01 │ 9000 │      0 │       │                   0 │                0 │</span><br><span class="line">└──────────┴──────┴────────┴───────┴─────────────────────┴──────────────────┘</span><br><span class="line"></span><br><span class="line">3 rows in set. Elapsed: 0.137 sec. </span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>创建视图从kafka表转储数据到集群表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">MATERIALIZED</span> <span class="keyword">VIEW</span> consumer <span class="keyword">TO</span> kafka_table_distributed <span class="keyword">AS</span> <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> kafka_src_table;</span><br></pre></td></tr></table></figure>



<p>然后开启kafka生产者生产数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --topic clickhouse_test01 --broker-list hadoop01:9092,hadoop02:9092,hadoop03:9092</span><br></pre></td></tr></table></figure>



<p>因为上面指定的kafka表中消费数据格式为json，故这里生产json格式的测试数据：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">3</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">22</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;su&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">1</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">20</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;sunqi&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">2</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">21</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;a&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">3</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">22</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;su&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">4</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">22</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;suqi&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">5</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">203</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;nqi&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">5</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">203</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;nqi&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">5</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">203</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;nqi&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;id&quot;</span>:<span class="number">5</span>,<span class="attr">&quot;age&quot;</span>:<span class="number">203</span>,<span class="attr">&quot;msg&quot;</span>:<span class="string">&quot;nqi&quot;</span>&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>查询集群表中：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> kafka_table_distributed</span><br><span class="line"></span><br><span class="line">┌─<span class="keyword">id</span>─┬─age─┬─msg──┐</span><br><span class="line">│  <span class="number">2</span> │  <span class="number">21</span> │ a    │</span><br><span class="line">│  <span class="number">4</span> │  <span class="number">22</span> │ suqi │</span><br><span class="line">└────┴─────┴──────┘</span><br><span class="line">┌─<span class="keyword">id</span>─┬─age─┬─msg─┐</span><br><span class="line">│  <span class="number">5</span> │ <span class="number">203</span> │ nqi │</span><br><span class="line">└────┴─────┴─────┘</span><br><span class="line">┌─<span class="keyword">id</span>─┬─age─┬─msg───┐</span><br><span class="line">│  <span class="number">1</span> │  <span class="number">20</span> │ sunqi │</span><br><span class="line">│  <span class="number">3</span> │  <span class="number">22</span> │ su    │</span><br><span class="line">└────┴─────┴───────┘</span><br><span class="line"></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> <span class="keyword">in</span> set. Elapsed: <span class="number">0.012</span> sec. </span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>查询副本1表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> kafka_table_local</span><br><span class="line"></span><br><span class="line">┌─<span class="keyword">id</span>─┬─age─┬─msg──┐</span><br><span class="line">│  <span class="number">2</span> │  <span class="number">21</span> │ a    │</span><br><span class="line">│  <span class="number">4</span> │  <span class="number">22</span> │ suqi │</span><br><span class="line">└────┴─────┴──────┘</span><br><span class="line"></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> <span class="keyword">in</span> set. Elapsed: <span class="number">0.002</span> sec. </span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>OLAP</category>
      </categories>
  </entry>
  <entry>
    <title>clickhouse单机及集群快速部署</title>
    <url>/2020/11/10/clickhouse%E5%8D%95%E6%9C%BA%E5%8F%8A%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p><img src="/2020/11/10/clickhouse%E5%8D%95%E6%9C%BA%E5%8F%8A%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/clickhouse.png"></p>
<p>​    clickhouse列式存储DB由战斗民族俄罗斯程序员开发，由于其超高的查询性能，近来备受关注。</p>
<a id="more"></a>

<p>​    之前的大数据分析，例如 Hadoop 家族由很多技术和框架组合而成，犹如一头大象被拆分后其实所剩下的价值也就是 HDFS、Kafka、Spark ，其他的几乎都没有任何价值。</p>
<p><img src="/2020/11/10/clickhouse%E5%8D%95%E6%9C%BA%E5%8F%8A%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/2.jpg"></p>
<p>​    这些可以用 ClickHouse 一项技术代替。</p>
<p><img src="/2020/11/10/clickhouse%E5%8D%95%E6%9C%BA%E5%8F%8A%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/1.jpg"></p>
<p>​    下面是100M数据集的跑分结果：ClickHouse比Vertia快约5倍，比Hive快279倍，比My SQL 快801倍；虽然对不同的SQL查询，结果不完全一样，但是基本趋势是一致的。ClickHouse跑分有多快？举个例子：ClickHouse 1秒，Vertica 5.42秒，Hive 279秒</p>
<p><img src="/2020/11/10/clickhouse%E5%8D%95%E6%9C%BA%E5%8F%8A%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/3.jpg"></p>
<p>​    clickhouse架构：</p>
<p><a href="http://www.360doc.com/content/20/0719/08/22849536_925250448.shtml">相关资料1</a></p>
<p><a href="https://www.codercto.com/a/27963.html">相关资料2</a></p>
<p><img src="/2020/11/10/clickhouse%E5%8D%95%E6%9C%BA%E5%8F%8A%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/4.jpg"></p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><a href="https://clickhouse.tech/#quick-start">clickhouse官网</a></p>
<p>首先安装工具包和秘钥：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install yum-utils</span><br><span class="line">sudo rpm --import https://repo.clickhouse.tech/CLICKHOUSE-KEY.GPG</span><br></pre></td></tr></table></figure>

<p><em>针对RHEL/CentOS 用户，这里使用清华的镜像</em></p>
<p>新建 <code>/etc/yum.repos.d/clickhouse.repo</code>，内容为</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[repo.yandex.ru_clickhouse_rpm_stable_x86_64]</span><br><span class="line">name=clickhouse stable</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/clickhouse/rpm/stable/x86_64</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p>然后进入安装，保持网络畅通即可：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install clickhouse-server clickhouse-client</span><br></pre></td></tr></table></figure>

<p>出现如下，则安装完成</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">已安装:</span><br><span class="line">  clickhouse-client-20.10.3.30-2.noarch        clickhouse-common-static-20.10.3.30-2.x86_64        clickhouse-server-20.10.3.30-2.noarch       </span><br><span class="line"></span><br><span class="line">完毕！</span><br></pre></td></tr></table></figure>

<p>需要先开启clickhouse服务，然后启动clickhouse客户端，常用命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 关闭clickhouse服务</span></span><br><span class="line">systemctl stop clickhouse-server.service</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启clickhouse服务</span></span><br><span class="line">systemctl start clickhouse-server.service</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看clickhouse服务状态</span></span><br><span class="line">systemctl status clickhouse-server.service</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 登陆客户端</span></span><br><span class="line">clickhouse-client</span><br></pre></td></tr></table></figure>



<h2 id="单机部署"><a href="#单机部署" class="headerlink" title="单机部署"></a>单机部署</h2><p>直接启动clickhouse客户端即可。</p>
<p><a href="https://blog.csdn.net/renyiforever/article/details/89401448">常见错误</a></p>
<h2 id="集群部署"><a href="#集群部署" class="headerlink" title="集群部署"></a>集群部署</h2><p>在其他节点重复以上安装步骤。</p>
<p>新建编辑文件：<code>vim /etc/metrika.xml</code>，并分发至集群各个节点。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">yandex</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">clickhouse_remote_servers</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">perftest_3shards_1replicas</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop01<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop02<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop03<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">perftest_3shards_1replicas</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">clickhouse_remote_servers</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">zookeeper-servers</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">&quot;1&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop01<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">&quot;2&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop02<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">&quot;3&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">host</span>&gt;</span>hadoop03<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">zookeeper-servers</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">macros</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">replica</span>&gt;</span>hadoop01<span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">macros</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">networks</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">ip</span>&gt;</span>::/0<span class="tag">&lt;/<span class="name">ip</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">networks</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">clickhouse_compression</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">case</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">min_part_size</span>&gt;</span>10000000000<span class="tag">&lt;/<span class="name">min_part_size</span>&gt;</span></span><br><span class="line">                                             </span><br><span class="line">  <span class="tag">&lt;<span class="name">min_part_size_ratio</span>&gt;</span>0.01<span class="tag">&lt;/<span class="name">min_part_size_ratio</span>&gt;</span>                                                                                                                                       </span><br><span class="line">  <span class="tag">&lt;<span class="name">method</span>&gt;</span>lz4<span class="tag">&lt;/<span class="name">method</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">case</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">clickhouse_compression</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">yandex</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>在各个节点重新启动clickhouse服务，然后开启客户端，并验证：<code>select * from system.clusters;</code></p>
<p>出现集群各个节点说明部署成功。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> system.clusters</span><br><span class="line"></span><br><span class="line">┌─cluster──────────────────────────────────────┬─shard_num─┬─shard_weight─┬─replica_num─┬─host_name─┬─host_address──┬─port─┬─is_local─┬─<span class="keyword">user</span>────┬─default_database─┬─errors_count─┬─estimated_recovery_time─┐</span><br><span class="line">│ perftest_3shards_1replicas                   │         <span class="number">1</span> │            <span class="number">1</span> │           <span class="number">1</span> │ hadoop01  │ <span class="number">192.168</span><span class="number">.150</span><span class="number">.4</span> │ <span class="number">9000</span> │        <span class="number">1</span> │ <span class="keyword">default</span> │                  │            <span class="number">0</span> │                       <span class="number">0</span> │</span><br><span class="line">│ perftest_3shards_1replicas                   │         <span class="number">2</span> │            <span class="number">1</span> │           <span class="number">1</span> │ hadoop02  │ <span class="number">192.168</span><span class="number">.150</span><span class="number">.5</span> │ <span class="number">9000</span> │        <span class="number">0</span> │ <span class="keyword">default</span> │                  │            <span class="number">0</span> │                       <span class="number">0</span> │</span><br><span class="line">│ perftest_3shards_1replicas                   │         <span class="number">3</span> │            <span class="number">1</span> │           <span class="number">1</span> │ hadoop03  │ <span class="number">192.168</span><span class="number">.150</span><span class="number">.6</span> │ <span class="number">9000</span> │        <span class="number">0</span> │ <span class="keyword">default</span> │</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>OLAP</category>
      </categories>
  </entry>
  <entry>
    <title>flink checkpoint机制及非barrier对齐</title>
    <url>/2021/02/16/flink%20checkpoint%E6%9C%BA%E5%88%B6%E5%8F%8A%E9%9D%9Ebarrier%E5%AF%B9%E9%BD%90/</url>
    <content><![CDATA[<p><img src="/2021/02/16/flink%20checkpoint%E6%9C%BA%E5%88%B6%E5%8F%8A%E9%9D%9Ebarrier%E5%AF%B9%E9%BD%90/0.jpg"></p>
<p>注：本文部分内容为转载总结</p>
<a id="more"></a>

<p>在flink的世界观里，一切事物都可以视为数据流中的一个个珠子，在算子间不断的流动着，之前的watermark就可以看做数据流中的一个数据珠子，同样checkpoint也不例外。</p>
<p>在多个并行度的数据流经算子内联的barrier(检查点分界线)时，会将自己的位置保存进状态后端(state backend)，生产中一般将状态保存进hdfs，rocksdb中，如果数据源是kafka，那么稳定存储的就是kafka的offset，如下图所示，checkpoint流经keyby算子后保存了当前的位置，后面经过map算子时，保存了checkpoint流过算子时的数据状态，同样作为稳定存储进状态后端，当任务出现异常时，就可以从状态后端中将数据流还原到未出错前的样子：</p>
<p><img src="/2021/02/16/flink%20checkpoint%E6%9C%BA%E5%88%B6%E5%8F%8A%E9%9D%9Ebarrier%E5%AF%B9%E9%BD%90/1.jpg"></p>
<p>但是，屏障对齐是阻塞式的，在作业出现反压时可能会成为不定时炸弹。我们知道，检查点屏障是从Source端产生并源源不断地向下游流动的。如果作业出现反压（哪怕整个DAG中的一条链路反压），数据流动的速度减慢，屏障到达下游算子的延迟就会变大，进而影响到检查点完成的延时（变大甚至超时失败）。如果反压长久不能得到解决，快照数据与实际数据之间的差距就越来越明显，一旦作业failover，势必丢失较多的处理进度。另一方面，作业恢复后需要重新处理的数据又会积压，加重反压，造成恶性循环。</p>
<p>为了规避风险，Flink 1.11引入了非对齐检查点（unaligned checkpoint）的feature。</p>
<p>非对齐检查点取消了屏障对齐操作：</p>
<ol>
<li>当算子的所有输入流中的第一个屏障到达算子的输入缓冲区时，立即将这个屏障发往下游（输出缓冲区）。</li>
<li>由于第一个屏障没有被阻塞，它的步调会比较快，超过一部分缓冲区中的数据。算子会标记两部分数据：一是屏障首先到达的那条流中被超过的数据，二是其他流中位于当前检查点屏障之前的所有数据（当然也包括进入了输入缓冲区的数据），如下图中标黄的部分所示。</li>
<li>将上述两部分数据连同算子的状态一起做异步快照。</li>
</ol>
<p><img src="/2021/02/16/flink%20checkpoint%E6%9C%BA%E5%88%B6%E5%8F%8A%E9%9D%9Ebarrier%E5%AF%B9%E9%BD%90/2.png"></p>
<p>不同检查点的数据都混在一起，非对齐检查点还是能保证exactly once。当任务从非对齐检查点恢复时，除了对齐检查点也会涉及到的Source端重放和算子的计算状态恢复之外，未对齐的流数据也会被恢复到各个链路，三者合并起来就是能够保证exactly once的完整现场了。</p>
<p><img src="/2021/02/16/flink%20checkpoint%E6%9C%BA%E5%88%B6%E5%8F%8A%E9%9D%9Ebarrier%E5%AF%B9%E9%BD%90/3.png"></p>
<p>非对齐检查点主要缺点有二：</p>
<ul>
<li><p>需要额外保存数据流的现场，总的状态大小可能会有比较明显的膨胀（文档中说可能会达到a couple of GB per task），磁盘压力大。当集群本身就具有I/O bound的特点时，该缺点的影响更明显。</p>
</li>
<li><p>从状态恢复时也需要额外恢复数据流的现场，作业重新拉起的耗时可能会很长。特别地，如果第一次恢复失败，有可能触发death spiral（死亡螺旋）使得作业永远无法恢复。</p>
</li>
</ul>
<p>官方当前推荐仅将它应用于那些容易产生反压且I/O压力较小（比如原始状态不太大）的作业中。随着后续版本的打磨，非对齐检查点肯定会更加好用。</p>
]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title>flink watermark传递机制</title>
    <url>/2021/02/15/flink%20watermark%E4%BC%A0%E9%80%92%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.12/fig/parallel_kafka_watermarks.svg" alt="Generating Watermarks with awareness for Kafka-partitions"></p>
<a id="more"></a>

<p>watermark是衡量eventtime进展机制的时间，经常用来处理乱序数据，常常结合window使用</p>
<p>生产环境一般使用周期性生成watermark，系统默认的周期是200ms，可以在代码中自定义：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.getConfig.setAutoWatermarkInterval(<span class="number">5000</span>);</span><br></pre></td></tr></table></figure>



<p>对于有eventtime的数据流，可以分为两大类：时间单调递增，乱序时间数据流，一般在生产环境中不能保证数据的准确性，所以使用BoundedOutOfOrdernessTimestampExtractor允许2s内的数据迟到：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;String&gt; input=env.socketTextStream(<span class="string">&quot;hadoop01&quot;</span>,<span class="number">7777</span>).assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;String&gt;(Time.seconds(<span class="number">2</span>)) &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(String element)</span> </span>&#123;</span><br><span class="line">                String[] fields = element.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> Long.parseLong(fields[<span class="number">1</span>]) * <span class="number">1000L</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure>



<p>在这之后要对数据进行开窗口计算，对于窗口时间和2s允许迟到的时间，以及allowedLateness内的时间也就是加起来4s的时间，会在主流输出。</p>
<p>对于超出允许迟到4s的数据，则会被送往侧输出流sideOutputLateData每来一条就进行侧输出。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">WindowedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple, TimeWindow&gt; windowedStream = keyedStream</span><br><span class="line">                .timeWindow(Time.seconds(<span class="number">5</span>))</span><br><span class="line">                .allowedLateness(Time.seconds(<span class="number">2</span>))</span><br><span class="line">                .sideOutputLateData(<span class="keyword">new</span> OutputTag&lt;Tuple2&lt;String, Integer&gt;&gt;(<span class="string">&quot;sideOutPut&quot;</span>) &#123;</span><br><span class="line">                &#125;);</span><br></pre></td></tr></table></figure>

<p><img src="/2021/02/15/flink%20watermark%E4%BC%A0%E9%80%92%E6%9C%BA%E5%88%B6/4.jpg"></p>
<p>在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/event_timestamps_watermarks.html#%E5%8F%AF%E4%BB%A5%E5%BC%83%E7%94%A8-assignerwithperiodicwatermarks-%E5%92%8C-assignerwithpunctuatedwatermarks-%E4%BA%86">新发布的flink 1.12</a>中，官方对生成watermark的api进行了升级，旧版本的api预计在不久的未来会被淘汰，但是原理是通用的。</p>
<p><img src="/2021/02/15/flink%20watermark%E4%BC%A0%E9%80%92%E6%9C%BA%E5%88%B6/1.jpg"></p>
<p>在多并行度时，算子向下游发送watermark是同时发送的，且同一个算子如果有多个watermark取最小。在生成watermark之前则是轮询发送。</p>
<p>下图是在map算子之前生成watermark：</p>
<p><img src="/2021/02/15/flink%20watermark%E4%BC%A0%E9%80%92%E6%9C%BA%E5%88%B6/2.jpg"></p>
<p>在map算子之后生成watermark：</p>
<p><img src="/2021/02/15/flink%20watermark%E4%BC%A0%E9%80%92%E6%9C%BA%E5%88%B6/3.jpg"></p>
]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title>flink yarn模式下的 HA 和 故障重启</title>
    <url>/2020/12/07/flink-yarn%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84-HA-%E5%92%8C-%E6%95%85%E9%9A%9C%E9%87%8D%E5%90%AF/</url>
    <content><![CDATA[<p><img src="https://flink.apache.org/img/flink-header-logo.svg" alt="Apache Flink"></p>
<p>​    </p>
<a id="more"></a>

<h1 id="HA"><a href="#HA" class="headerlink" title="HA"></a>HA</h1><p>​        JobManager协调每个Flink部署。它负责调度和资源管理。默认情况下，每个Flink集群只有一个JobManager实例。这会创建一个单点故障(SPOF):如果JobManager崩溃，就不能提交新的程序，运行中的程序也会失败。使用JobManager高可用性，您可以从JobManager故障中恢复，从而消除SPOF。您可以为独立集群和纱线集群配置高可用性。</p>
<p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.11/fig/jobmanager_ha_overview.png" alt="img"></p>
<p>​    </p>
<p>​    由于生产环境下通常使用yarn模式，所以这里介绍yarn模式下的高可用配置，并测试杀死jobmanager进程后重启的过程。通常高可用需要配置zookeeper参数，所以这里分别对<code>yarn-site.xml</code>和<code>flink-conf.yaml</code>进行配置：</p>
<p>yarn-site.xml增加jobmanageer失败后最大的重试次数</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.am.max-attempts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    The maximum number of application master execution attempts.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p>flink-conf.yaml中设置zookeeper参数：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">high-availability:</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.quorum:</span> <span class="string">hadoop01:2181,hadoop02:2181,hadoop03:2181</span></span><br><span class="line"><span class="attr">high-availability.storageDir:</span> <span class="string">hdfs:///flink/ha</span></span><br><span class="line"><span class="attr">high-availability.zookeeper.path.root:</span> <span class="string">/flink</span></span><br><span class="line"><span class="attr">yarn.application-attempts:</span> <span class="number">10</span></span><br></pre></td></tr></table></figure>



<blockquote>
<p>注意<code>yarn.resourcemanager.am.max-attempts</code>应用程序重新启动的上限。因此，Flink中设置的应用程序尝试次数不能超过启动YARN的YARN群集设置。(yarn.resourcemanager.am.max-attemps)。</p>
<p>yarn.application-attempts  &lt;=  yarn.resourcemanager.am.max-attempts</p>
</blockquote>
<p>配置成功并分发至各节点，然后分别启动zookeeper，yarn，并开启session cluster：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/yarn-session.sh -n 2 -s 2 -jm 1024 -tm 1024 -nm test -d</span><br></pre></td></tr></table></figure>

<p>​    查看控制台：</p>
<p><img src="/2020/12/07/flink-yarn%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84-HA-%E5%92%8C-%E6%95%85%E9%9A%9C%E9%87%8D%E5%90%AF/be.jpg"></p>
<p>​    查看job manager信息，可以看到zookeeper中注册了flink节点信息：</p>
<p><img src="/2020/12/07/flink-yarn%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84-HA-%E5%92%8C-%E6%95%85%E9%9A%9C%E9%87%8D%E5%90%AF/ha.jpg"></p>
<p>在控制台杀死<code>YarnSessionClusterEntrypoint</code>进程，flink UI界面无法访问</p>
<p><img src="/2020/12/07/flink-yarn%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84-HA-%E5%92%8C-%E6%95%85%E9%9A%9C%E9%87%8D%E5%90%AF/404.jpg"></p>
<p>查看控制台发现在hadoop03又出现一个<code>YarnSessionClusterEntrypoint</code>进程：</p>
<p><img src="/2020/12/07/flink-yarn%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84-HA-%E5%92%8C-%E6%95%85%E9%9A%9C%E9%87%8D%E5%90%AF/la.jpg"></p>
<p>通过yarn application发现又出现一个application，打开：</p>
<p><img src="/2020/12/07/flink-yarn%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84-HA-%E5%92%8C-%E6%95%85%E9%9A%9C%E9%87%8D%E5%90%AF/ha02.jpg"></p>
<h1 id="故障重启"><a href="#故障重启" class="headerlink" title="故障重启"></a>故障重启</h1><p>flink在消费kafka源源不断产生的消息时，难免会遇到程序挂掉的情况，这时候如果无人值守，那么kafka生产的消息会一直积压无法消费，所以需要开启故障重启来自动恢复任务。</p>
<p>flink重启策略有四种：下面是在<code>flink-conf.yaml</code>配置和代码中进行配置：</p>
<h4 id="固定延迟重启策略"><a href="#固定延迟重启策略" class="headerlink" title="固定延迟重启策略"></a>固定延迟重启策略</h4><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fixed-delay:固定延迟策略</span></span><br><span class="line"><span class="attr">restart-strategy:</span> <span class="string">fixed-delay</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 尝试5次，默认Integer.MAX_VALUE</span></span><br><span class="line"><span class="attr">restart-strategy.fixed-delay.attempts:</span> <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置延迟时间10s，默认为 akka.ask.timeout时间</span></span><br><span class="line"><span class="attr">restart-strategy.fixed-delay.delay:</span> <span class="string">10s</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5表示最大重试次数为5次，10s为延迟时间</span></span><br><span class="line">env.setRestartStrategy(RestartStrategies.fixedDelayRestart(<span class="number">5</span>,Time.of(<span class="number">10</span>, TimeUnit.SECONDS)));</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="故障率重启策略"><a href="#故障率重启策略" class="headerlink" title="故障率重启策略"></a>故障率重启策略</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置重启策略为failure-rate</span></span><br><span class="line"><span class="attr">restart-strategy:</span> <span class="string">failure-rate</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 失败作业之前的给定时间间隔内的最大重启次数，默认1</span></span><br><span class="line"><span class="attr">restart-strategy.failure-rate.max-failures-per-interval:</span> <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测量故障率的时间间隔。默认1min</span></span><br><span class="line"><span class="attr">restart-strategy.failure-rate.failure-rate-interval:</span> <span class="string">5min</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 两次连续重启尝试之间的延迟，默认akka.ask.timeout时间</span></span><br><span class="line"><span class="attr">restart-strategy.failure-rate.delay:</span> <span class="string">10s</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3为最大失败次数；5min为测量的故障时间；10s为2次间的延迟时间</span></span><br><span class="line">env.setRestartStrategy(RestartStrategies.failureRateRestart(<span class="number">3</span>,Time.of(<span class="number">5</span>, TimeUnit.MINUTES),Time.of(<span class="number">10</span>, TimeUnit.SECONDS)));</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="后备重启策略"><a href="#后备重启策略" class="headerlink" title="后备重启策略"></a>后备重启策略</h4><p>​    使用群集定义的重新启动策略。这对于启用检查点的流式传输程序很有帮助。默认情况下，如果没有定义其他重启策略，则选择固定延迟重启策略。</p>
<h4 id="无重启策略"><a href="#无重启策略" class="headerlink" title="无重启策略"></a>无重启策略</h4><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">restart-strategy:</span> <span class="string">none</span></span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setRestartStrategy(RestartStrategies.noRestart());</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title>flink实时数仓-dwd层维表关联</title>
    <url>/2020/12/14/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dwd%E5%B1%82%E7%BB%B4%E8%A1%A8%E5%85%B3%E8%81%94/</url>
    <content><![CDATA[<p><img src="https://flink.apache.org/img/flink-home-graphic.png" alt="img"></p>
<a id="more"></a>

<p>​        在实时业务场景中，mysql中的业务数据通过maxwell传到kafka后，我们可以将来自mysql的不同表传来的数据通过flink回传到不同的topic，然后再开启另外flink消费这些来自不同topic的数据并存储到hbase中，这样就可以方便维度表和事实表的关联。OLAP分析引擎往往不善于关联不同表的数据，即使是clickhouse的join也会损耗性能，所以在flink可以将表进行关联，也可以拓宽下游即席查询的广度。</p>
<p><img src="/2020/12/14/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dwd%E5%B1%82%E7%BB%B4%E8%A1%A8%E5%85%B3%E8%81%94/2.jpg"></p>
<p>​        在监控mysqlbinlog过程中，maxwell具有canal所没有的优势，那就是bootstrap功能，可以将mysql中的历史数据传输到kafka，在需要历史数据关联的时候就比canal有优势。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/maxwell-bootstrap --user root  --password 143382 --host hadoop01  --database gmall --table base_province --client_id maxwell_1</span><br></pre></td></tr></table></figure>

<p><img src="/2020/12/14/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dwd%E5%B1%82%E7%BB%B4%E8%A1%A8%E5%85%B3%E8%81%94/3.jpg"></p>
<p>需求整体架构：</p>
<p><img src="/2020/12/14/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dwd%E5%B1%82%E7%BB%B4%E8%A1%A8%E5%85%B3%E8%81%94/4.jpg"></p>
<p>flink-java实现：<a href="https://github.com/ycfn97/flink-utils.git">GitHub</a></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> dwd;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> bean.OrderInfo01;</span><br><span class="line"><span class="keyword">import</span> bean.dim.ProvinceInfo;</span><br><span class="line"><span class="keyword">import</span> bean.dim.UserInfo;</span><br><span class="line"><span class="keyword">import</span> bean.dim.UserState;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FilterFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.RichMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.RuntimeContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.ConfigConstants;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.RestOptions;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.memory.MemoryStateBackend;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch.util.RetryRejectedExecutionFailureHandler;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.util.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.http.HttpHost;</span><br><span class="line"><span class="keyword">import</span> org.apache.jute.Index;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.ActionRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.index.IndexRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.Requests;</span><br><span class="line"><span class="keyword">import</span> util.MyKafkaSink02;</span><br><span class="line"><span class="keyword">import</span> util.PhoenixUtil;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Copyright(c) 2020-2021 sparrow All Rights Reserved</span></span><br><span class="line"><span class="comment"> * Project: gmall2020-parent</span></span><br><span class="line"><span class="comment"> * Package: dwd</span></span><br><span class="line"><span class="comment"> * ClassName: OrderInfoApp01</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> 18729 created on date: 2020/12/10 12:22</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@version</span> 1.0</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> JDK 1.8</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderInfoApp01</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String kafkaBrokers = <span class="string">&quot;hadoop01:9092&quot;</span>;</span><br><span class="line">        String zkBrokers = <span class="string">&quot;hadoop01:2181,hadoop02:2181,hadoop03:2181&quot;</span>;</span><br><span class="line">        String topic = <span class="string">&quot;ODS_ORDER_INFO&quot;</span>;</span><br><span class="line">        String groupId = <span class="string">&quot;order_info_group&quot;</span>;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;===============》 flink任务开始  ==============》&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.setBoolean(ConfigConstants.LOCAL_START_WEBSERVER, <span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">//自定义端口</span></span><br><span class="line">        conf.setInteger(RestOptions.PORT, <span class="number">8060</span>);</span><br><span class="line">        <span class="comment">//本地env</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(conf);</span><br><span class="line">        env.setParallelism(<span class="number">4</span>);</span><br><span class="line">        <span class="comment">//生产env</span></span><br><span class="line">        <span class="comment">//val env = StreamExecutionEnvironment.getExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span></span><br><span class="line">        <span class="comment">//设置kafka连接参数</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, kafkaBrokers);</span><br><span class="line">        properties.setProperty(<span class="string">&quot;zookeeper.connect&quot;</span>, zkBrokers);</span><br><span class="line">        properties.setProperty(<span class="string">&quot;group.id&quot;</span>, groupId);</span><br><span class="line">        <span class="comment">//设置时间类型</span></span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">        <span class="comment">//设置检查点时间间隔</span></span><br><span class="line"><span class="comment">//        env.enableCheckpointing(5000);</span></span><br><span class="line"><span class="comment">//        env.setStateBackend( new MemoryStateBackend());</span></span><br><span class="line">        <span class="comment">//创建kafak消费者，获取kafak中的数据</span></span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; kafkaConsumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(topic, <span class="keyword">new</span> SimpleStringSchema(), properties);</span><br><span class="line">        DataStreamSource&lt;String&gt; kafkaData = env.addSource(kafkaConsumer);</span><br><span class="line">        DataStream&lt;String&gt; userData = kafkaData.map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line"><span class="comment">//                System.out.println(&quot;&gt;&gt;&gt;&gt;&gt;&gt;接收topic报文:&quot;+s+&quot;&lt;&lt;&lt;&lt;&lt;&quot;);</span></span><br><span class="line">                <span class="keyword">return</span> s;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;OrderInfo01&gt; map = userData.map(<span class="keyword">new</span> RichMapFunction&lt;String, OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> OrderInfo01 <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                OrderInfo01 o = (OrderInfo01) JSON.parseObject(value, Class.forName(<span class="string">&quot;bean.OrderInfo01&quot;</span>));</span><br><span class="line">                String[] s = o.getCreate_time().split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">                o.setCreate_date(s[<span class="number">0</span>]);</span><br><span class="line">                String[] split = s[<span class="number">1</span>].split(<span class="string">&quot;:&quot;</span>);</span><br><span class="line">                o.setCreate_hour(split[<span class="number">0</span>]);</span><br><span class="line">                <span class="keyword">return</span> o;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;OrderInfo01&gt; map1 = map.map(<span class="keyword">new</span> RichMapFunction&lt;OrderInfo01, OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> OrderInfo01 <span class="title">map</span><span class="params">(OrderInfo01 value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                value.setIf_first_order(<span class="string">&quot;1&quot;</span>);</span><br><span class="line">                Long user_id = value.getUser_id();</span><br><span class="line">                String s = <span class="string">&quot;select user_id,if_consumed from USER_STATE2020 where user_id = &#x27;&quot;</span>+user_id+<span class="string">&quot;&#x27;&quot;</span>;</span><br><span class="line">                List&lt;JSONObject&gt; jsonObjects = <span class="keyword">new</span> PhoenixUtil().queryList(s);</span><br><span class="line"><span class="comment">//                System.out.println(jsonObjects);</span></span><br><span class="line">                <span class="keyword">for</span> (JSONObject jsonObject:jsonObjects) &#123;</span><br><span class="line"><span class="comment">//                    System.out.println(jsonObject.getString(&quot;USER_ID&quot;));</span></span><br><span class="line"><span class="comment">//                    System.out.println(jsonObject.getString(&quot;IF_CONSUMED&quot;));</span></span><br><span class="line">                    <span class="keyword">if</span> (jsonObject.getString(<span class="string">&quot;USER_ID&quot;</span>).equals(value.getUser_id().toString())&amp;&amp;<span class="string">&quot;1&quot;</span>.equals(jsonObject.getString(<span class="string">&quot;IF_CONSUMED&quot;</span>)))&#123;</span><br><span class="line"><span class="comment">//                        System.out.println(111111111);</span></span><br><span class="line">                        value.setIf_first_order(<span class="string">&quot;0&quot;</span>);</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"><span class="comment">//                System.out.println(value);</span></span><br><span class="line">                <span class="keyword">return</span> value;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;OrderInfo01&gt; map2 = map1.map(<span class="keyword">new</span> RichMapFunction&lt;OrderInfo01, OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> OrderInfo01 <span class="title">map</span><span class="params">(OrderInfo01 value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String sql = <span class="string">&quot;select  id,name,region_id,area_code,iso_code,iso_3166_2 from GMALL2020_PROVINCE_INFO &quot;</span>;</span><br><span class="line">                List&lt;JSONObject&gt; jsonObjects = <span class="keyword">new</span> PhoenixUtil().queryList(sql);</span><br><span class="line">                HashMap&lt;String, ProvinceInfo&gt; provinceInfoHashMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">                <span class="keyword">for</span> (JSONObject object : jsonObjects) &#123;</span><br><span class="line">                    String id = object.getString(<span class="string">&quot;ID&quot;</span>);</span><br><span class="line">                    ProvinceInfo provinceInfo = <span class="keyword">new</span> ProvinceInfo(object.getString(<span class="string">&quot;ID&quot;</span>), object.getString(<span class="string">&quot;NAME&quot;</span>), object.getString(<span class="string">&quot;REGION_ID&quot;</span>), object.getString(<span class="string">&quot;AREA_CODE&quot;</span>), object.getString(<span class="string">&quot;ISO_CODE&quot;</span>), object.getString(<span class="string">&quot;ISO_3166_2&quot;</span>));</span><br><span class="line">                    provinceInfoHashMap.put(id, provinceInfo);</span><br><span class="line">                &#125;</span><br><span class="line"><span class="comment">//                System.out.println(provinceInfoHashMap.get(value.getProvince_id().toString()));</span></span><br><span class="line">                value.setProvince_name(provinceInfoHashMap.get(value.getProvince_id().toString()).getName());</span><br><span class="line">                value.setProvince_area_code(provinceInfoHashMap.get(value.getProvince_id().toString()).getArea_code());</span><br><span class="line">                value.setProvince_iso_code(provinceInfoHashMap.get(value.getProvince_id().toString()).getIso_code());</span><br><span class="line">                value.setProvince_iso_3166_2(provinceInfoHashMap.get(value.getProvince_id().toString()).getIso_3166_2());</span><br><span class="line">                <span class="keyword">return</span> value;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        map2.filter(<span class="keyword">new</span> FilterFunction&lt;OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(OrderInfo01 value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value!=<span class="keyword">null</span>&amp;&amp;value.getUser_id()!=<span class="keyword">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;OrderInfo01&gt; map3 = map2.map(<span class="keyword">new</span> RichMapFunction&lt;OrderInfo01, OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> OrderInfo01 <span class="title">map</span><span class="params">(OrderInfo01 value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String sql = <span class="string">&quot;select id,user_level,birthday,gender,age_group,gender_name from GMALL_USER_INFO &quot;</span>;</span><br><span class="line">                List&lt;JSONObject&gt; jsonObjects = <span class="keyword">new</span> PhoenixUtil().queryList(sql);</span><br><span class="line">                HashMap&lt;String, UserInfo&gt; userInfoHashMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">                <span class="keyword">for</span> (JSONObject object : jsonObjects) &#123;</span><br><span class="line">                    String id = object.getString(<span class="string">&quot;ID&quot;</span>);</span><br><span class="line">                    UserInfo userInfo = <span class="keyword">new</span> UserInfo(object.getString(<span class="string">&quot;ID&quot;</span>), object.getString(<span class="string">&quot;USER_LEVEL&quot;</span>), object.getString(<span class="string">&quot;BIRTHDAY&quot;</span>), object.getString(<span class="string">&quot;GENDER&quot;</span>), object.getString(<span class="string">&quot;AGE_GROUP&quot;</span>), object.getString(<span class="string">&quot;GENDER_NAME&quot;</span>));</span><br><span class="line">                    userInfoHashMap.put(id, userInfo);</span><br><span class="line">                &#125;</span><br><span class="line"><span class="comment">//                System.out.println(value.getUser_id());</span></span><br><span class="line"><span class="comment">//                System.out.println(userInfoHashMap.get(value.getUser_id().toString()));</span></span><br><span class="line"><span class="comment">//                System.out.println(userInfoHashMap.get(value.getUser_id().toString()).getAge_group());</span></span><br><span class="line">                value.setUser_age_group(userInfoHashMap.get(value.getUser_id().toString()).getAge_group());</span><br><span class="line">                value.setUser_gender(userInfoHashMap.get(value.getUser_id().toString()).getGender_name());</span><br><span class="line">                <span class="keyword">return</span> value;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        map3.filter(<span class="keyword">new</span> FilterFunction&lt;OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(OrderInfo01 value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value.getIf_first_order()==<span class="string">&quot;1&quot;</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).map(<span class="keyword">new</span> RichMapFunction&lt;OrderInfo01, OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> OrderInfo01 <span class="title">map</span><span class="params">(OrderInfo01 value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">new</span> PhoenixUtil().update(<span class="string">&quot;upsert into USER_STATE2020 values(&#x27;&quot;</span> +</span><br><span class="line">                        value.getUser_id()+<span class="string">&quot;&#x27;,&#x27;&quot;</span>+value.getIf_first_order()+<span class="string">&quot;&#x27;)&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> value;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        map3.addSink(EsUtils().build());</span><br><span class="line">        map3.addSink(<span class="keyword">new</span> MyKafkaSink02());</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> ElasticsearchSink.<span class="function">Builder&lt;OrderInfo01&gt; <span class="title">EsUtils</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        String format = <span class="keyword">new</span> SimpleDateFormat(<span class="string">&quot;yyyy-MM-dd&quot;</span>).format(<span class="keyword">new</span> Date());</span><br><span class="line">        List&lt;HttpHost&gt; httpHosts = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        httpHosts.add(<span class="keyword">new</span> HttpHost(<span class="string">&quot;hadoop01&quot;</span>, <span class="number">9200</span>, <span class="string">&quot;http&quot;</span>));</span><br><span class="line"></span><br><span class="line">        ElasticsearchSink.Builder&lt;OrderInfo01&gt; esBuilder = <span class="keyword">new</span> ElasticsearchSink.Builder&lt;&gt;(httpHosts, <span class="keyword">new</span> ElasticsearchSinkFunction&lt;OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(OrderInfo01 orderInfo01, RuntimeContext runtimeContext, RequestIndexer requestIndexer)</span> </span>&#123;</span><br><span class="line">                requestIndexer.add(createIndexRequest(orderInfo01));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="function"><span class="keyword">private</span> IndexRequest <span class="title">createIndexRequest</span><span class="params">(OrderInfo01 orderInfo01)</span> </span>&#123;</span><br><span class="line">                HashMap&lt;String, Object&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">                map.put(<span class="string">&quot;id&quot;</span>, orderInfo01.getId());</span><br><span class="line">                map.put(<span class="string">&quot;province_id&quot;</span>, orderInfo01.getProvince_id());</span><br><span class="line">                map.put(<span class="string">&quot;order_status&quot;</span>, orderInfo01.getOrder_status());</span><br><span class="line">                map.put(<span class="string">&quot;user_id&quot;</span>, orderInfo01.getUser_id());</span><br><span class="line">                map.put(<span class="string">&quot;final_total_amount&quot;</span>, orderInfo01.getFinal_total_amount());</span><br><span class="line">                map.put(<span class="string">&quot;benefit_reduce_amount&quot;</span>, orderInfo01.getBenefit_reduce_amount());</span><br><span class="line">                map.put(<span class="string">&quot;original_total_amount&quot;</span>, orderInfo01.getOriginal_total_amount());</span><br><span class="line">                map.put(<span class="string">&quot;feight_fee&quot;</span>, orderInfo01.getFeight_fee());</span><br><span class="line">                map.put(<span class="string">&quot;expire_time&quot;</span>, orderInfo01.getExpire_time());</span><br><span class="line">                map.put(<span class="string">&quot;create_time&quot;</span>, orderInfo01.getCreate_time());</span><br><span class="line">                map.put(<span class="string">&quot;create_hour&quot;</span>, orderInfo01.getCreate_hour());</span><br><span class="line">                map.put(<span class="string">&quot;if_first_order&quot;</span>, orderInfo01.getIf_first_order());</span><br><span class="line">                map.put(<span class="string">&quot;province_name&quot;</span>, orderInfo01.getProvince_name());</span><br><span class="line">                map.put(<span class="string">&quot;province_area_code&quot;</span>, orderInfo01.getProvince_iso_3166_2());</span><br><span class="line">                map.put(<span class="string">&quot;user_age_group&quot;</span>, orderInfo01.getUser_age_group());</span><br><span class="line">                map.put(<span class="string">&quot;user_gender&quot;</span>, orderInfo01.getUser_gender());</span><br><span class="line">                System.out.println(<span class="string">&quot;data:&quot;</span> + orderInfo01);</span><br><span class="line">                <span class="keyword">return</span> Requests.indexRequest()</span><br><span class="line">                        .index(<span class="string">&quot;gmall2020_order_info_&quot;</span> + format)</span><br><span class="line">                        .type(<span class="string">&quot;_doc&quot;</span>)</span><br><span class="line">                        .id(orderInfo01.getUser_id().toString())</span><br><span class="line">                        .source(map);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        esBuilder.setBulkFlushMaxActions(<span class="number">1</span>);</span><br><span class="line">        esBuilder.setRestClientFactory(<span class="keyword">new</span> util.RestClientFactoryImpl());</span><br><span class="line">        esBuilder.setFailureHandler(<span class="keyword">new</span> RetryRejectedExecutionFailureHandler());</span><br><span class="line">        <span class="keyword">return</span> esBuilder;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>​        在sparkstreaming中，要保持精准一次性消费需要依靠redis来读取和保存kafka偏移量，而flink-kafka端到端exactly once的优势，不言自明。</p>
]]></content>
      <categories>
        <category>实时数仓</category>
      </categories>
  </entry>
  <entry>
    <title>flink实时数仓-dws层双流join</title>
    <url>/2020/12/15/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dws%E5%B1%82%E5%8F%8C%E6%B5%81join/</url>
    <content><![CDATA[<p><img src="/2020/12/15/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dws%E5%B1%82%E5%8F%8C%E6%B5%81join/0.jpg"></p>
<a id="more"></a>

<p>​        在ods进行维度表和事实表关联形成dwd层宽表后，分别可以得到orderinfo和orderdetail两张宽表，这时可以将订单信息和订单详情表从kafka消费并使用flink intervalJoin和ProcessJoinFunction再次进行合流并去重形成订单总表，这样就可以将聚合好的数据写入OLAP例如clickhouse中，方便后续其他实时需求的实现SQL化，同时也可以将合流后的数据再次写入kafka形成dws层宽表方便数据的再次加工，最后实现ads层实时需求。</p>
<p>整体架构：</p>
<p><img src="/2020/12/15/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dws%E5%B1%82%E5%8F%8C%E6%B5%81join/1.jpg"></p>
<p>程序流图：</p>
<p><img src="/2020/12/15/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dws%E5%B1%82%E5%8F%8C%E6%B5%81join/2.jpg"></p>
<p>代码实现  java版：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> dws;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> bean.OrderDetail;</span><br><span class="line"><span class="keyword">import</span> bean.OrderDetailWide;</span><br><span class="line"><span class="keyword">import</span> bean.OrderInfo01;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> lombok.SneakyThrows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FilterFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.RichMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.ConfigConstants;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.RestOptions;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.co.ProcessJoinFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.util.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Copyright(c) 2020-2021 sparrow All Rights Reserved</span></span><br><span class="line"><span class="comment"> * Project: gmall2020-parent</span></span><br><span class="line"><span class="comment"> * Package: dws</span></span><br><span class="line"><span class="comment"> * ClassName: OrderDetailWideApp</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> 18729 created on date: 2020/12/14 16:29</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@version</span> 1.0</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> JDK 1.8</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderDetailWideApp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String kafkaBrokers = <span class="string">&quot;hadoop01:9092&quot;</span>;</span><br><span class="line">        String zkBrokers = <span class="string">&quot;hadoop01:2181,hadoop02:2181,hadoop03:2181&quot;</span>;</span><br><span class="line">        String topic = <span class="string">&quot;DWD_ORDER_INFO&quot;</span>;</span><br><span class="line">        String groupId = <span class="string">&quot;dws_order_info_group&quot;</span>;</span><br><span class="line"></span><br><span class="line">        String topic01=<span class="string">&quot;DWD_ORDER_DETAIL&quot;</span>;</span><br><span class="line">        String groupId01=<span class="string">&quot;dws_order_detail_group&quot;</span>;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;===============》 flink任务开始  ==============》&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.setBoolean(ConfigConstants.LOCAL_START_WEBSERVER, <span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">//自定义端口</span></span><br><span class="line">        conf.setInteger(RestOptions.PORT, <span class="number">4550</span>);</span><br><span class="line">        <span class="comment">//本地env</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(conf);</span><br><span class="line">        env.setParallelism(<span class="number">4</span>);</span><br><span class="line">        <span class="comment">//生产env</span></span><br><span class="line">        <span class="comment">//val env = StreamExecutionEnvironment.getExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span></span><br><span class="line">        <span class="comment">//设置kafka连接参数</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, kafkaBrokers);</span><br><span class="line">        properties.setProperty(<span class="string">&quot;zookeeper.connect&quot;</span>, zkBrokers);</span><br><span class="line">        properties.setProperty(<span class="string">&quot;group.id&quot;</span>, groupId);</span><br><span class="line"></span><br><span class="line">        Properties properties01 = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties01.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, kafkaBrokers);</span><br><span class="line">        properties01.setProperty(<span class="string">&quot;zookeeper.connect&quot;</span>, zkBrokers);</span><br><span class="line">        properties01.setProperty(<span class="string">&quot;group.id&quot;</span>, groupId01);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置时间类型</span></span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">        <span class="comment">//设置检查点时间间隔</span></span><br><span class="line"><span class="comment">//        env.enableCheckpointing(5000);</span></span><br><span class="line"><span class="comment">//        env.setStateBackend( new MemoryStateBackend());</span></span><br><span class="line">        <span class="comment">//创建kafak消费者，获取kafak中的数据</span></span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; kafkaConsumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(topic, <span class="keyword">new</span> SimpleStringSchema(), properties);</span><br><span class="line">        DataStreamSource&lt;String&gt; kafkaData = env.addSource(kafkaConsumer);</span><br><span class="line">        DataStream&lt;String&gt; orderInfo = kafkaData.map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line"><span class="comment">//                System.out.println(&quot;&gt;&gt;&gt;&gt;&gt;&gt;接收topic报文:&quot;+s+&quot;&lt;&lt;&lt;&lt;&lt;&quot;);</span></span><br><span class="line">                <span class="keyword">return</span> s;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; kafkaConsumer01 = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(topic01, <span class="keyword">new</span> SimpleStringSchema(), properties01);</span><br><span class="line">        DataStreamSource&lt;String&gt; kafkaData01 = env.addSource(kafkaConsumer01);</span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; orderDetail = kafkaData01.map(<span class="keyword">new</span> RichMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">//                System.out.println(&quot;&gt;&gt;&gt;&gt;&gt;&gt;接收topic报文:&quot; + value + &quot;&lt;&lt;&lt;&lt;&lt;&quot;);</span></span><br><span class="line">                <span class="keyword">return</span> value;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//        双流join</span></span><br><span class="line">        SingleOutputStreamOperator&lt;OrderInfo01&gt; orderInfo01 = orderInfo.map(<span class="keyword">new</span> RichMapFunction&lt;String, OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> OrderInfo01 <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                OrderInfo01 orderInfo = (OrderInfo01) JSON.parseObject(value, Class.forName(<span class="string">&quot;bean.OrderInfo01&quot;</span>));</span><br><span class="line">                <span class="keyword">return</span> orderInfo;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@SneakyThrows</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(OrderInfo01 element)</span> </span>&#123;</span><br><span class="line">                SimpleDateFormat simpleDateFormat = <span class="keyword">new</span> SimpleDateFormat(<span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>);</span><br><span class="line">                Date date = simpleDateFormat.parse(element.getCreate_time());</span><br><span class="line">                <span class="keyword">long</span> time = date.getTime();</span><br><span class="line">                <span class="keyword">return</span> time * <span class="number">1000L</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).filter(<span class="keyword">new</span> FilterFunction&lt;OrderInfo01&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(OrderInfo01 value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value.getOrder_status().equals(<span class="string">&quot;1004&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;OrderDetail&gt; orderDetail01 = (SingleOutputStreamOperator&lt;OrderDetail&gt;) orderDetail.map(<span class="keyword">new</span> RichMapFunction&lt;String, OrderDetail&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> OrderDetail <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                OrderDetail orderDetail = (OrderDetail) JSON.parseObject(value, Class.forName(<span class="string">&quot;bean.OrderDetail&quot;</span>));</span><br><span class="line">                <span class="keyword">return</span> orderDetail;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;OrderDetail&gt;() &#123;</span><br><span class="line">            <span class="meta">@SneakyThrows</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(OrderDetail element)</span> </span>&#123;</span><br><span class="line">                SimpleDateFormat simpleDateFormat = <span class="keyword">new</span> SimpleDateFormat(<span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>);</span><br><span class="line">                Date date = simpleDateFormat.parse(element.getCreate_time());</span><br><span class="line">                <span class="keyword">long</span> time = date.getTime();</span><br><span class="line">                <span class="keyword">return</span> time * <span class="number">1000L</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        orderInfo01</span><br><span class="line">                .keyBy(OrderInfo01::getId)</span><br><span class="line">                .intervalJoin(orderDetail01.keyBy(OrderDetail::getOrder_id))</span><br><span class="line">                .between(Time.minutes(-<span class="number">5</span>), Time.minutes(<span class="number">5</span>))</span><br><span class="line">                .process(<span class="keyword">new</span> TimeJoinFunction())</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="comment">//        订单金额分摊 略</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//        写入clickhouse 略</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//        .print();</span></span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeJoinFunction</span> <span class="keyword">extends</span> <span class="title">ProcessJoinFunction</span>&lt;<span class="title">OrderInfo01</span>,<span class="title">OrderDetail</span>,<span class="title">OrderDetailWide</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(OrderInfo01 left, OrderDetail right, Context ctx, Collector&lt;OrderDetailWide&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            out.collect(<span class="keyword">new</span> OrderDetailWide(left,right));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>webUI反压监控正常：</p>
<p><img src="/2020/12/15/flink%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-dws%E5%B1%82%E5%8F%8C%E6%B5%81join/3.jpg"></p>
<p>dws层写入clickhouse</p>
]]></content>
      <categories>
        <category>实时数仓</category>
      </categories>
  </entry>
  <entry>
    <title>hdfs故障恢复及原理</title>
    <url>/2021/02/21/hdfs%E6%95%85%E9%9A%9C%E6%81%A2%E5%A4%8D%E5%8F%8A%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p><img src="http://hadoop.apache.org/docs/r1.0.4/cn/images/hadoop-logo.jpg" alt="Hadoop"></p>
<a id="more"></a>

<h3 id="2NN工作机制"><a href="#2NN工作机制" class="headerlink" title="2NN工作机制"></a>2NN工作机制</h3><p>Fsimage：hdfs文件系统元数据的一个永久性检查点。</p>
<p>Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。</p>
<p><img src="/2021/02/21/hdfs%E6%95%85%E9%9A%9C%E6%81%A2%E5%A4%8D%E5%8F%8A%E5%8E%9F%E7%90%86/1.jpg"></p>
<ol>
<li>NameNode启动，滚动Edits并生成一个空的edits.inprogress，并加载Edits和Fsimage到内存中，持有最新的元数据信息。</li>
<li>Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询操作不会被记录，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息并在内存中执行元数据的增删改的操作。</li>
<li>SecondaryNameNode询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。</li>
<li>SecondaryNameNode执行CheckPoint，NameNode滚动Edits并生成一个新的edits.inprogress并将以后所有新的操作都写入edits.inprogress</li>
<li>其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint</li>
<li>然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。</li>
<li>NameNode在启动时就只需要加载新的edits.inprogress和新Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。</li>
</ol>
<h3 id="CheckPoint时间设置"><a href="#CheckPoint时间设置" class="headerlink" title="CheckPoint时间设置"></a>CheckPoint时间设置</h3><ul>
<li><p>通常情况下，SecondaryNameNode每隔一小时执行一次 [hdfs-default.xml] 。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span> &gt;</span></span><br></pre></td></tr></table></figure>



</li>
</ul>
<h3 id="NameNode故障处理"><a href="#NameNode故障处理" class="headerlink" title="NameNode故障处理"></a>NameNode故障处理</h3><p>NameNode故障后，可以采用如下两种方法恢复数据。</p>
<ul>
<li>将SecondaryNameNode中数据拷贝到NameNode存储数据的目录</li>
<li>使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。</li>
</ul>
<h3 id="Hadoop宕机"><a href="#Hadoop宕机" class="headerlink" title="Hadoop宕机"></a>Hadoop宕机</h3><ul>
<li>如果MR造成系统宕机。此时要控制Yarn同时运行的任务数，和每个任务申请的最大内存。调整参数：yarn.scheduler.maximum-allocation-mb（单个任务可申请的最多物理内存量，默认是8192MB）</li>
<li>如果写入文件过快造成NameNode宕机。那么调高Kafka的存储大小，控制从Kafka到HDFS的写入速度。例如，可以调整Flume每批次拉取数据量的大小参数batchsize。</li>
</ul>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/09/23/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<a id="more"></a>

<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>spark RDD原理</title>
    <url>/2020/09/26/spark%20RDD%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p><img src="/2020/09/26/spark%20RDD%E5%8E%9F%E7%90%86/2.png"></p>
<a id="more"></a>

<p>​            首先我觉得原理性的东西，重要的在于了解整个业务的运行流程。虽然在面试中狠问底层这是区分面试者的重要手段，但在实际工作中，最重要的还是提高业务运行的效率。当然知道框架运行的原理越细说明面试者对底层运行接触的时间就越久。然而人的精力是有限的。深入源码固然能提升能力。这里只对原理做一个简单的阐述。</p>
<p><img src="/2020/09/26/spark%20RDD%E5%8E%9F%E7%90%86/1.png"></p>
<p>​            整个spark RDD运行在资源调度系统yarn上，运行代码由spark-submit提交到rm，rm将业务交给nm上的driver，driver会把业务交由executors具体去执行，executors上进行RDD弹性分布式计算，相较于MapReduce，shufflemapRDDs之间一般省去了shuffle落盘的过程，这也是spark速度比mr引擎快的原因。    </p>
<p><img src="/2020/09/26/spark%20RDD%E5%8E%9F%E7%90%86/1.jpg"></p>
<p><img src="/2020/09/26/spark%20RDD%E5%8E%9F%E7%90%86/2.jpg">    </p>
<p>​        我们知道spark RDD算子包括transformation算子和action算子，运算过程主要发生map阶段，经过reduce聚合后action算子进行提交job任务，而transformation算子是懒加载的，也就是说，前面的运算过程并不会立即执行，application需要action算子进行触发，action算子的数量决定了job的个数，action算子中的DAGScheduler会对job任务进行切分，并寻找依赖关系，根据是否是宽依赖来确定stag个数。job的task个数由一个stage阶段最后一个RDD的partition个数决定。</p>
<p>​        spark application的运行效率取决于数据是否均衡，可以通过合理的分区来确保各个分区数据的均衡，常用的分区方案有：hash分区，range分区，自定义分区，也可以在初始化sc时指定分区。</p>
<p>​        数据均衡可以使transformation算子在各个分区完成时间相差减小以提高运行效率，常见的转换算子分为value型，双value交互，key-value型，在不同的业务场景下选择不同的运算组合以达到最佳的效率是设计者的初衷。</p>
<p>​        未完待续。。。</p>
]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title>spark core-top10热门品类</title>
    <url>/2020/09/28/spark-core-top10%E7%83%AD%E9%97%A8%E5%93%81%E7%B1%BB/</url>
    <content><![CDATA[<p>需求说明：品类是指产品的分类，大型电商网站品类分多级，咱们的项目中品类只有一级，不同的公司可能对热门的定义不一样。我们按照每个品类的点击、下单、支付的量来统计热门品类。</p>
<p>鞋            点击数 下单数  支付数</p>
<p>衣服        点击数 下单数  支付数</p>
<p>电脑        点击数 下单数  支付数</p>
<p>本项目需求优化为：先按照点击数排名，靠前的就排名高；如果点击数相同，再比较下单数；下单数再相同，就比较支付数。</p>
<a id="more"></a>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//用户访问动作表</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UserVisitAction</span>(<span class="params">date: <span class="type">String</span>,//用户点击行为的日期</span></span></span><br><span class="line"><span class="class"><span class="params">                           user_id: <span class="type">Long</span>,//用户的<span class="type">ID</span></span></span></span><br><span class="line"><span class="class"><span class="params">                        <span class="type">UserVisitAction</span>   session_id: <span class="type">String</span>,//<span class="type">Session</span>的<span class="type">ID</span></span></span></span><br><span class="line"><span class="class"><span class="params">                           page_id: <span class="type">Long</span>,//某个页面的<span class="type">ID</span></span></span></span><br><span class="line"><span class="class"><span class="params">                           action_time: <span class="type">String</span>,//动作的时间点</span></span></span><br><span class="line"><span class="class"><span class="params">                           search_keyword: <span class="type">String</span>,//用户搜索的关键词</span></span></span><br><span class="line"><span class="class"><span class="params">                           click_category_id: <span class="type">Long</span>,//某一个商品品类的<span class="type">ID</span></span></span></span><br><span class="line"><span class="class"><span class="params">                           click_product_id: <span class="type">Long</span>,//某一个商品的<span class="type">ID</span></span></span></span><br><span class="line"><span class="class"><span class="params">                           order_category_ids: <span class="type">String</span>,//一次订单中所有品类的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="class"><span class="params">                           order_product_ids: <span class="type">String</span>,//一次订单中所有商品的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="class"><span class="params">                           pay_category_ids: <span class="type">String</span>,//一次支付中所有品类的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="class"><span class="params">                           pay_product_ids: <span class="type">String</span>,//一次支付中所有商品的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="class"><span class="params">                           city_id: <span class="type">Long</span></span>)<span class="title">//城市</span> <span class="title">id</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">输出结果表</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">CategoryCountInfo</span>(<span class="params">var categoryId: <span class="type">String</span>,//品类id</span></span></span><br><span class="line"><span class="class"><span class="params">                             var clickCount: <span class="type">Long</span>,//点击次数</span></span></span><br><span class="line"><span class="class"><span class="params">                             var orderCount: <span class="type">Long</span>,//订单次数</span></span></span><br><span class="line"><span class="class"><span class="params">                             var payCount: <span class="type">Long</span></span>)<span class="title">//支付次数</span></span></span><br></pre></td></tr></table></figure>



<p>数据（局部）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2019-07-17_95_26070e87-1ad7-49a3-8fb3-cc741facaddf_6_2019-07-17 00:00:17_null_19_85_null_null_null_null_7</span><br><span class="line">2019-07-17_38_6502cdc9-cf95-4b08-8854-f03a25baa917_29_2019-07-17 00:00:19_null_12_36_null_null_null_null_5</span><br><span class="line">2019-07-17_38_6502cdc9-cf95-4b08-8854-f03a25baa917_22_2019-07-17 00:00:28_null_-1_-1_null_null_15,1,20,6,4_15,88,75_9</span><br><span class="line">2019-07-17_38_6502cdc9-cf95-4b08-8854-f03a25baa917_11_2019-07-17 00:00:29_苹果_-1_-1_null_null_null_null_7</span><br><span class="line">2019-07-17_38_6502cdc9-cf95-4b08-8854-f03a25baa917_24_2019-07-17 00:00:38_null_-1_-1_15,13,5,11,8_99,2_null_null_10</span><br><span class="line">2019-07-17_38_6502cdc9-cf95-4b08-8854-f03a25baa917_24_2019-07-17 00:00:48_null_19_44_null_null_null_null_4</span><br><span class="line">2019-07-17_38_6502cdc9-cf95-4b08-8854-f03a25baa917_47_2019-07-17 00:00:54_null_14_79_null_null_null_null_2</span><br></pre></td></tr></table></figure>



<p>根据上面提供的类，很显然是将各个字段封装的上面提供的两个类中，然后进行运算调用。</p>
<p>思路：</p>
<p>1，首先要将数据按照_切分，得到各个字段对应的值并封装到UserVisitAction对象中</p>
<p>2，对封装好的UserVisitAction对象进行进一步封装，得到CategoryCountInfo对象并打散分布</p>
<p>3，根据商品id对数据进行聚合，最后排序并取前十输出</p>
<p>伪代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.textFile(<span class="string">&quot;input02&quot;</span>)</span><br><span class="line">  .map(line=&gt;&#123;</span><br><span class="line">    <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">    <span class="type">UserVisitAction</span>(</span><br><span class="line">      datas(<span class="number">0</span>),</span><br><span class="line">      datas(<span class="number">1</span>).toLong,</span><br><span class="line">      datas(<span class="number">2</span>),</span><br><span class="line">      datas(<span class="number">3</span>).toLong,</span><br><span class="line">      datas(<span class="number">4</span>),</span><br><span class="line">      datas(<span class="number">5</span>),</span><br><span class="line">      datas(<span class="number">6</span>).toLong,</span><br><span class="line">      datas(<span class="number">7</span>).toLong,</span><br><span class="line">      datas(<span class="number">8</span>),</span><br><span class="line">      datas(<span class="number">9</span>),</span><br><span class="line">      datas(<span class="number">10</span>),</span><br><span class="line">      datas(<span class="number">11</span>),</span><br><span class="line">      datas(<span class="number">12</span>).toLong</span><br><span class="line">    )</span><br><span class="line">  &#125;)</span><br><span class="line">  .flatMap(a=&gt;(</span><br><span class="line">    <span class="keyword">if</span> (a.click_category_id != <span class="number">-1</span>)&#123;</span><br><span class="line">      <span class="type">List</span>(<span class="type">CategoryCountInfo</span>(a.click_category_id.toString,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    &#125;<span class="keyword">else</span> <span class="keyword">if</span>(a.order_category_ids!=<span class="string">&quot;null&quot;</span>)&#123;</span><br><span class="line">      <span class="keyword">val</span> infoesToInfoes: <span class="type">ListBuffer</span>[<span class="type">CategoryCountInfo</span>] =<span class="keyword">new</span>  <span class="type">ListBuffer</span>[<span class="type">CategoryCountInfo</span>]</span><br><span class="line">      <span class="keyword">val</span> strings: <span class="type">Array</span>[<span class="type">String</span>] = a.order_category_ids.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">      <span class="keyword">for</span> (id&lt;-strings)&#123;</span><br><span class="line">        infoesToInfoes.append(<span class="type">CategoryCountInfo</span>(id,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>))</span><br><span class="line">      &#125;</span><br><span class="line">      infoesToInfoes</span><br><span class="line">    &#125;<span class="keyword">else</span> <span class="keyword">if</span> (a.pay_category_ids!=<span class="string">&quot;null&quot;</span>)&#123;</span><br><span class="line">      <span class="keyword">val</span> infoesToInfoes: <span class="type">ListBuffer</span>[<span class="type">CategoryCountInfo</span>] =<span class="keyword">new</span>  <span class="type">ListBuffer</span>[<span class="type">CategoryCountInfo</span>]</span><br><span class="line">      <span class="keyword">val</span> strings: <span class="type">Array</span>[<span class="type">String</span>] = a.pay_category_ids.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">      <span class="keyword">for</span> (id&lt;-strings)&#123;</span><br><span class="line">        infoesToInfoes.append(<span class="type">CategoryCountInfo</span>(id,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">      &#125;</span><br><span class="line">      infoesToInfoes</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">      <span class="type">Nil</span></span><br><span class="line">    &#125;</span><br><span class="line">  ))</span><br><span class="line">  .groupBy(info=&gt;info.categoryId)</span><br><span class="line">  .mapValues(a=&gt;a.reduce(</span><br><span class="line">    (a,b)=&gt;&#123;</span><br><span class="line">      a.orderCount=a.orderCount+b.orderCount</span><br><span class="line">      a.clickCount=a.clickCount+b.clickCount</span><br><span class="line">      a.payCount=a.payCount+b.payCount</span><br><span class="line">      a</span><br><span class="line">    &#125;</span><br><span class="line">  ))</span><br><span class="line">  .map(_._2)</span><br><span class="line">  .sortBy(a=&gt;(a.clickCount,a.orderCount,a.payCount),<span class="literal">false</span>)</span><br><span class="line">  .take(<span class="number">10</span>).foreach(println)</span><br></pre></td></tr></table></figure>



<p>输出结果：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">6</span>,<span class="number">5912</span>,<span class="number">1768</span>,<span class="number">1197</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">16</span>,<span class="number">5928</span>,<span class="number">1782</span>,<span class="number">1233</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">4</span>,<span class="number">5961</span>,<span class="number">1760</span>,<span class="number">1271</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">14</span>,<span class="number">5964</span>,<span class="number">1773</span>,<span class="number">1171</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">8</span>,<span class="number">5974</span>,<span class="number">1736</span>,<span class="number">1238</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">3</span>,<span class="number">5975</span>,<span class="number">1749</span>,<span class="number">1192</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">1</span>,<span class="number">5976</span>,<span class="number">1766</span>,<span class="number">1191</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">10</span>,<span class="number">5991</span>,<span class="number">1757</span>,<span class="number">1174</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">5</span>,<span class="number">6011</span>,<span class="number">1820</span>,<span class="number">1132</span>)</span><br><span class="line"><span class="type">CategoryCountInfo</span>(<span class="number">18</span>,<span class="number">6024</span>,<span class="number">1754</span>,<span class="number">1197</span>)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/09/28/spark-core-top10%E7%83%AD%E9%97%A8%E5%93%81%E7%B1%BB/1.png" alt="DAG Visualization"></p>
]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title>spark核心函数实例-广告点击Top3</title>
    <url>/2020/09/24/spark-core-%E5%B9%BF%E5%91%8A%E7%82%B9%E5%87%BBTop3/</url>
    <content><![CDATA[<p>案例需求：统计每个省份广告被点击次数的top3</p>
<a id="more"></a>

<p><img src="/2020/09/24/spark-core-%E5%B9%BF%E5%91%8A%E7%82%B9%E5%87%BBTop3/1.png"></p>
<p>最终希望输出效果：</p>
<p><img src="/2020/09/24/spark-core-%E5%B9%BF%E5%91%8A%E7%82%B9%E5%87%BBTop3/2.jpg"></p>
<p>思考：样本中需要提取出省份，广告，还有统计出每个省份每个广告种类点击的次数。然后聚合出广告出现的次数并做排名截取前三名。</p>
<p>代码实现1：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.textFile(<span class="string">&quot;input/agent.log&quot;</span>)</span><br><span class="line">  .map(line=&gt;(line.split(<span class="string">&quot; &quot;</span>)(<span class="number">1</span>)+<span class="string">&quot;-&quot;</span>+line.split(<span class="string">&quot; &quot;</span>)(<span class="number">4</span>),<span class="number">1</span>))</span><br><span class="line">  .reduceByKey(_+_)</span><br><span class="line">  .map(a=&gt;(a._1.split(<span class="string">&quot;-&quot;</span>)(<span class="number">0</span>),(a._1.split(<span class="string">&quot;-&quot;</span>)(<span class="number">1</span>),a._2)))</span><br><span class="line">  .groupByKey()</span><br><span class="line">  .mapValues(data=&gt;data.toList.sortWith((a,b)=&gt;(a._2&gt;b._2)).take(<span class="number">3</span>))</span><br><span class="line">  .collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p><img src="/2020/09/24/spark-core-%E5%B9%BF%E5%91%8A%E7%82%B9%E5%87%BBTop3/4.jpg" alt="DAG Visualization"></p>
<p>代码实现2：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.textFile(<span class="string">&quot;input/agent.log&quot;</span>)</span><br><span class="line">  .map(line=&gt;((line.split(<span class="string">&quot; &quot;</span>)(<span class="number">1</span>),line.split(<span class="string">&quot; &quot;</span>)(<span class="number">4</span>)),<span class="number">1</span>))</span><br><span class="line">  .reduceByKey((a,b)=&gt;(a+b))</span><br><span class="line">  .groupBy(_._1._1)</span><br><span class="line">  .mapValues(a=&gt;a.toList.sortWith(_._2&gt;_._2).take(<span class="number">3</span>))</span><br><span class="line">  .collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p><img src="/2020/09/24/spark-core-%E5%B9%BF%E5%91%8A%E7%82%B9%E5%87%BBTop3/5.jpg" alt="DAG Visualization"></p>
<p>分析实现：</p>
<p><img src="/2020/09/24/spark-core-%E5%B9%BF%E5%91%8A%E7%82%B9%E5%87%BBTop3/3.jpg"></p>
]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title>一次意外断电导致mysql数据库损坏的数据恢复过程</title>
    <url>/2020/11/15/%E4%B8%80%E6%AC%A1%E6%84%8F%E5%A4%96%E6%96%AD%E7%94%B5%E5%AF%BC%E8%87%B4mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8D%9F%E5%9D%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<p>​    <img src="/2020/11/15/%E4%B8%80%E6%AC%A1%E6%84%8F%E5%A4%96%E6%96%AD%E7%94%B5%E5%AF%BC%E8%87%B4mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8D%9F%E5%9D%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D%E8%BF%87%E7%A8%8B/1.jpg"></p>
<p>​    mysql在企业开发中不仅存放着大量用户数据表，还存放着其他服务的元数据，这些数据一但无法访问，不仅影响其他业务的进行，同时用户也无法查询自己的数据信息，众所周知，数据是无价的，弄丢了数据的后果极其严重。</p>
<a id="more"></a>

<p>​    昨天就遇到了一次意外断电mysql索引损坏导致mysqld服务无法正常启动的麻烦，断电后重启hadoop集群，开启hiveservice2和matestore元数据服务时发现无法启动，打开hive.log发现是mysql连接失败：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Caused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure</span><br><span class="line">......</span><br><span class="line">Caused by: java.net.ConnectException: 拒绝连接 (Connection refused)</span><br></pre></td></tr></table></figure>

<p>随即检查了一下mysql服务，发现mysql服务被意外关闭了，于是又重新启动mysql服务:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart mysqld;</span><br></pre></td></tr></table></figure>

<p>但是服务并没有顺利启动成功，而是卡住不动了，尝试了几次都是一样卡住，我有点不淡定了，平时mysql服务断开重新开启一下就可以了，于是进入<code>/etc/my.cnf</code>中找到mysql的错误日志并监控日志输出：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tail -f /var/log/mysql.log</span><br></pre></td></tr></table></figure>

<p>在另外一个控制台又启动一次服务，发现了从来没有遇到过得报错：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2020-11-15T09:08:52.532504Z 0 [ERROR] [MY-012712] [InnoDB] Dir slot does not point to right rec 99</span><br><span class="line"> len 16384; hex 64c7c9230000023b0000023affffffff00000000292dd74545bf00000000000000000000024400231a58808c000000001a2d00020089008a0000000000000000000000000000000004700000000000000000000000000000000000000000010002001a696e66696d756d0007000b000073757072656d756d000010003000000018338b000000100f1d81000000c601100000000000006f78dfafd5300000000000000000908580e7000018003000000018338c000000100f1d81000000c6011f0000000000005ad8dfafd530a0f0ba889e75d23f90cc9ff8000020003000000018338f000000100f1e820000013801100000000000005ad9dfafd53162475ed6e629e83f90dbc387040028</span><br><span class="line"> ......</span><br><span class="line">000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000064c7c923292dd745; asc d  #   ;   :        )- EE            D # X       -                       p                         infimum      supremum    0    3                    ox   0                0    3                    Z    0     u ?        0    3           8        Z    1bG^  ) ?      ( 0    3                    oz   2         B l  0 0    3                    Z    2         E    8 0    3           Y        Z    32 c &#123;  ? Q s  @ 0    3      !             \D   4 F     @ |    H 0    3      !             Z    4          PY  P 0    3      $             Z    8  v  ( ? *U   X 0    3      %             \I   9   m4e @ ?Pm  ` 0    3      %             Z    9         C    h 0    3      &amp;             Z    :         Sc,  p 0    3      &#x27;    ~        dC   ;   $yi ?   b  x 0    3   </span><br><span class="line">.....</span><br><span class="line">Most likely, you have hit a bug, but this error can also be caused by malfunctioning hardware.</span><br><span class="line">Thread pointer: 0x7fb3bc3d4280</span><br><span class="line">Attempting backtrace. You can use the following information to find out</span><br><span class="line">where mysqld died. If you see no messages after this, something went</span><br><span class="line">terribly wrong...</span><br><span class="line"></span><br><span class="line">2020-11-15T09:08:52.623420Z 0 [ERROR] [MY-011937] [InnoDB] [FATAL] Apparent corruption of an index page [page id: space=580, page number=571] to be written to data file. We intentionally crash the server to prevent corrupt data from ending up in data files.</span><br><span class="line"></span><br><span class="line">2020-11-15T09:08:54.724470Z 0 [ERROR] [MY-011906] [InnoDB] Database page corruption on disk or a failed file read of page [page id: space=580, page number=571]. You may have to recover from a backup.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>翻译过来意思是：</p>
<p>​    最可能的情况是，您遇到了一个bug，但这个错误也可能是由硬件故障引起的。</p>
<p>​    mysqld去世的地方。如果在此之后您没有看到任何消息，那么一定有问题，非常错误的……</p>
<p>​    在磁盘上的数据库页损坏或失败的文件读取页[页id:空间=580，页号=571]。您可能需要从备份中进行恢复。</p>
<p>要写入数据文件的索引页[页id:空格=580，页号=571]明显损坏。我们故意使服务器崩溃，以防止损坏的数据出现在数据文件中。</p>
<p>​    百度了一阵之后，我意识到是数据库内部有表损坏导致mysql服务无法正常打开了，由于数据库中有上千张表，不可能知道到底是哪里出了问题。这就意味着我要将mysql中所有用户数据表和其他服务的元数据表全部导出到sql文件做备份，然后重新初始化mysql，在把表一个个建回来并重新导入。一想这工作量顿时头皮发麻，但是数据没法访问后果更严重。。。所以硬着头皮来了：</p>
<p>​    使用强制InnoDB恢复:</p>
<p>​    编辑<code>/etc/my.cnf</code>文件加入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">innodb_force_recovery &#x3D; 6</span><br></pre></td></tr></table></figure>

<p>​    <em>innodb_force_recovery影响整个InnoDB存储引擎的恢复状况。默认为0，表示当需要恢复时执行所有的恢复操作（即校验数据页/purge undo/insert buffer merge/rolling back&amp;forward），当不能进行有效的恢复操作时，mysql有可能无法启动，并记录错误日志；innodb_force_recovery可以设置为1-6,大的数字包含前面所有数字的影响。当设置参数值大于0后，可以对表进行select,create,drop操作,但insert,update或者delete这类操作是不允许的。</em></p>
<ul>
<li><p>1(SRV_FORCE_IGNORE_CORRUPT):忽略检查到的corrupt页。</p>
</li>
<li><p>2(SRV_FORCE_NO_BACKGROUND):阻止主线程的运行，如主线程需要执行full purge操作，会导致crash。</p>
</li>
<li><p>3(SRV_FORCE_NO_TRX_UNDO):不执行事务回滚操作。</p>
</li>
<li><p>4(SRV_FORCE_NO_IBUF_MERGE):不执行插入缓冲的合并操作。</p>
</li>
<li><p>5(SRV_FORCE_NO_UNDO_LOG_SCAN):不查看重做日志，InnoDB存储引擎会将未提交的事务视为已提交。</p>
</li>
<li><p>6(SRV_FORCE_NO_LOG_REDO):不执行前滚的操作。</p>
<p>​    </p>
<p>​    然后重启节点，发现mysql服务能启动了，但是无法执行插入更新和删除操作，但终于可以导出宝贵的数据做备份了，我使用了Navicat把mysql中所有数据进行了导出：</p>
<p><img src="/2020/11/15/%E4%B8%80%E6%AC%A1%E6%84%8F%E5%A4%96%E6%96%AD%E7%94%B5%E5%AF%BC%E8%87%B4mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8D%9F%E5%9D%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D%E8%BF%87%E7%A8%8B/2.jpg"></p>
<p>​    整个过程用了很久，但数据总算是保存下来了，接下来进入<code>/var/lib/mysql</code>备份整个目录，然后删除目录中所有内容<code>rm -rf  ./*</code>将mysql重新初始化。最后进入<code>/etc/my.cnf</code>将innodb重新置为0:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">innodb_force_recovery = 0</span><br></pre></td></tr></table></figure>

<p>成功启动mysql服务：</p>
<p><img src="/2020/11/15/%E4%B8%80%E6%AC%A1%E6%84%8F%E5%A4%96%E6%96%AD%E7%94%B5%E5%AF%BC%E8%87%B4mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8D%9F%E5%9D%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D%E8%BF%87%E7%A8%8B/4.jpg"></p>
<p>这时候数据库已经重新初始化，需要重新获取初始密码：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">grep &#x27;temporary password&#x27; /var/log/mysqld.log</span><br></pre></td></tr></table></figure>



</li>
</ul>
<p>  ​    然后进行设置mysql用户的操作，导入之前的数据，重新启动hadoop集群，成功启动hiveservice2和matestore服务。并进入datagrip查了一下，数据完美恢复！还好是测试环境，谢天谢地。</p>
<p>  <img src="/2020/11/15/%E4%B8%80%E6%AC%A1%E6%84%8F%E5%A4%96%E6%96%AD%E7%94%B5%E5%AF%BC%E8%87%B4mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8D%9F%E5%9D%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D%E8%BF%87%E7%A8%8B/3.jpg"></p>
]]></content>
      <categories>
        <category>OLTP</category>
      </categories>
  </entry>
  <entry>
    <title>Hexo搭建个人博客</title>
    <url>/2020/09/23/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<h2 id="主机配置"><a href="#主机配置" class="headerlink" title="主机配置"></a>主机配置</h2><a id="more"></a>

<p>安装node.js  git，打开git终端</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ~&#x2F;.ssh</span><br><span class="line">ssh-keygen -t rsa -C &quot;1872998728@qq.com&quot;</span><br></pre></td></tr></table></figure>

<p>上传GitHub</p>
<p>详细查看博客：<a href="https://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.html">https://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.html</a></p>
<!--more-->

<h2 id="服务器配置"><a href="#服务器配置" class="headerlink" title="服务器配置"></a>服务器配置</h2><p>安装git  Nginx</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git init --bare hexo.git</span><br><span class="line">cd hexo.git&#x2F;hooks</span><br></pre></td></tr></table></figure>

<p>vim post-receive</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">git --work-tree=/home/wwwroot/3DCEList --git-dir=/home/wwwroot/hexo.git checkout -f</span><br></pre></td></tr></table></figure>

<p>配置免密登陆：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh-copy-id -i .ssh&#x2F;id_rsa.pub root@47.242.8.122</span><br></pre></td></tr></table></figure>



]]></content>
  </entry>
  <entry>
    <title>日活需求(kafka精准一次性消费) 分别用sparkstreaming和flink实现</title>
    <url>/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/1.jpg"></p>
<a id="more"></a>

<p>指标需求：求出当日新增日活，并通过kibana按照需求做实时展示</p>
<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/kibana%E5%8F%AF%E8%A7%86%E5%8C%96.jpg"></p>
<p>实现思路1：sparkstreaming消费kafka数据，使用redis保存kafka偏移量，确保程序意外退出后能从之前的偏移量继续消费，并保存至es做去重。</p>
<p>实现思路2：flink消费kafka数据，使用状态后端(state backend)保存data source中来自kafka的偏移量，确保程序宕机后重启能从之前的偏移位置重新消费。</p>
<blockquote>
<p><em>端到端exactly-once实现：</em></p>
<p>​    <u><em>source端：kafka偏移量</em></u></p>
<p>​    <u><em>内部：sparkingstreaming：redis做去重同时保存偏移量</em></u></p>
<p>​                <u><em>flink：state backend状态后端</em></u></p>
<p>​    <u><em>sink端：es的id不可重复，做幂等性写入</em></u></p>
</blockquote>
<p>项目局部架构：</p>
<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/%E6%97%A5%E6%B4%BB.jpg"></p>
<p>sparkingstreaming流程图：</p>
<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/spark.jpg"></p>
<p>flink端到端状态一致性过程：</p>
<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/%E4%B8%80%E8%87%B4%E6%80%A71.jpg"></p>
<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/%E4%B8%80%E8%87%B4%E6%80%A72.jpg"></p>
<h2 id="demo："><a href="#demo：" class="headerlink" title="demo："></a>demo：</h2><h4 id="sparkstreaming使用scala实现："><a href="#sparkstreaming使用scala实现：" class="headerlink" title="sparkstreaming使用scala实现："></a>sparkstreaming使用scala实现：</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DauApp</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置文件对象 注意：Streaming程序至少不能设置为local，至少需要2个线程</span></span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Spark01_W&quot;</span>).setMaster(<span class="string">&quot;local[4]&quot;</span>)</span><br><span class="line">      </span><br><span class="line">    <span class="comment">//创建Spark Streaming上下文环境对象O</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> gmallstartup = <span class="string">&quot;GMALL_STARTUP_0105&quot;</span></span><br><span class="line">    <span class="keyword">val</span> daugroup = <span class="string">&quot;DAU_GROUP&quot;</span></span><br><span class="line">      </span><br><span class="line">     <span class="comment">//使用偏移量工具类从redis获取上一次的kafka偏移量</span></span><br><span class="line">    <span class="keyword">val</span> partitionToLong = util.<span class="type">OffsetManager</span>.getOffset(gmallstartup, daugroup)</span><br><span class="line">      </span><br><span class="line">      <span class="comment">//判断是否第一次消费，如果不是则从偏移量开始消费数据流</span></span><br><span class="line">    <span class="keyword">var</span> inputStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]]=<span class="literal">null</span></span><br><span class="line">    <span class="keyword">if</span> (partitionToLong!=<span class="literal">null</span>&amp;&amp;partitionToLong.size&gt;<span class="number">0</span>)&#123;</span><br><span class="line">      inputStream = util.<span class="type">MyKafkaUtil</span>.getKafkaStream(gmallstartup, ssc, partitionToLong, daugroup)</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">      inputStream=util.<span class="type">MyKafkaUtil</span>.getKafkaStream(gmallstartup,ssc)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">//得到本批次的偏移量的结束位置，用于更新redis中的偏移量</span></span><br><span class="line">    <span class="keyword">var</span>  offsetRanges: <span class="type">Array</span>[<span class="type">OffsetRange</span>] = <span class="type">Array</span>.empty[<span class="type">OffsetRange</span>]</span><br><span class="line">    <span class="keyword">val</span>  inputGetOffsetDstream: <span class="type">DStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = inputStream.transform &#123; rdd =&gt;</span><br><span class="line">      offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges  <span class="comment">//driver? executor?  //周期性的执行</span></span><br><span class="line">      rdd</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">//补充日志json时间字段</span></span><br><span class="line">    <span class="keyword">val</span> value1 = inputGetOffsetDstream.map(record =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> str = record.value()</span><br><span class="line">      <span class="keyword">val</span> nObject = <span class="type">JSON</span>.parseObject(str)</span><br><span class="line">      <span class="keyword">val</span> long = nObject.getLong(<span class="string">&quot;ts&quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> str1 = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">&quot;yyyy-MM-dd HH&quot;</span>).format(<span class="keyword">new</span> <span class="type">Date</span>(long))</span><br><span class="line">      <span class="keyword">val</span> strings = str1.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">      nObject.put(<span class="string">&quot;dt&quot;</span>, strings(<span class="number">0</span>))</span><br><span class="line">      nObject.put(<span class="string">&quot;hr&quot;</span>, strings(<span class="number">1</span>))</span><br><span class="line">      nObject</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">      <span class="comment">//写入redis以及设置保存时间为24小时，并通过是否写入redis成功判断过滤条数</span></span><br><span class="line">    <span class="keyword">val</span> value2 = value1.mapPartitions(iter =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> client = util.<span class="type">RedisUtil</span>.getJedisClient</span><br><span class="line">      <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">JSONObject</span>]()</span><br><span class="line">      <span class="keyword">val</span> list = iter.toList</span><br><span class="line">      println(<span class="string">&quot;过滤前:&quot;</span> + list.size)</span><br><span class="line">      <span class="keyword">for</span> (jsonObj &lt;- list) &#123;</span><br><span class="line">        <span class="keyword">val</span> str = jsonObj.getString(<span class="string">&quot;dt&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> str1 = jsonObj.getJSONObject(<span class="string">&quot;common&quot;</span>).getString(<span class="string">&quot;mid&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> str2 = <span class="string">&quot;dau:&quot;</span> + str</span><br><span class="line">        <span class="keyword">val</span> long = client.sadd(str2, str1)</span><br><span class="line">        client.expire(str2, <span class="number">3600</span> * <span class="number">24</span>)</span><br><span class="line">        <span class="keyword">if</span> (long == <span class="number">1</span>) &#123;</span><br><span class="line">          buffer += jsonObj</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      client.close()</span><br><span class="line">      println(<span class="string">&quot;过滤后:&quot;</span> + buffer.size)</span><br><span class="line">      list.toIterator</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">      </span><br><span class="line">      <span class="comment">//写入es</span></span><br><span class="line">    value2.foreachRDD &#123; rdd =&gt; &#123;</span><br><span class="line">      rdd.foreachPartition(rdd =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> list = rdd.toList</span><br><span class="line">        <span class="keyword">val</span> tuples = list.map(jsonObj =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> nObject = jsonObj.getJSONObject(<span class="string">&quot;common&quot;</span>)</span><br><span class="line">          <span class="keyword">val</span> info = bean.<span class="type">DauInfo</span>(nObject.getString(<span class="string">&quot;mid&quot;</span>),</span><br><span class="line">            nObject.getString(<span class="string">&quot;uid&quot;</span>),</span><br><span class="line">            nObject.getString(<span class="string">&quot;ar&quot;</span>),</span><br><span class="line">            nObject.getString(<span class="string">&quot;ch&quot;</span>),</span><br><span class="line">            nObject.getString(<span class="string">&quot;vc&quot;</span>),</span><br><span class="line">            jsonObj.getString(<span class="string">&quot;dt&quot;</span>),</span><br><span class="line">            jsonObj.getString(<span class="string">&quot;hr&quot;</span>),</span><br><span class="line">            <span class="string">&quot;00&quot;</span>,</span><br><span class="line">            jsonObj.getLong(<span class="string">&quot;ts&quot;</span>))</span><br><span class="line">          (info.mid, info)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="keyword">val</span> str = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">&quot;yyyy-MM-dd&quot;</span>).format(<span class="keyword">new</span> <span class="type">Date</span>())</span><br><span class="line">        util.<span class="type">MyEsUtil</span>.bulkDoc(tuples, <span class="string">&quot;gmall_dau_info_&quot;</span> + str)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">      util.<span class="type">OffsetManager</span>.saveOffset(gmallstartup, daugroup, offsetRanges)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//    value.map(_.value()).print()</span></span><br><span class="line">    <span class="comment">//启动采集器</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">//默认情况下，上下文对象不能关闭</span></span><br><span class="line">    <span class="comment">//ssc.stop()</span></span><br><span class="line">    <span class="comment">//等待采集结束，终止上下文环境对象</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>使用到的utils工具类可以去<a href="https://github.com/ycfn97/gmall-utils.git">GitHub</a>拉取</p>
<h4 id="flink使用java实现："><a href="#flink使用java实现：" class="headerlink" title="flink使用java实现："></a>flink使用java实现：</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> app;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.RuntimeContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch.util.RetryRejectedExecutionFailureHandler;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.util.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.http.HttpHost;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.index.IndexRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.Requests;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Copyright(c) 2020-2021 sparrow All Rights Reserved</span></span><br><span class="line"><span class="comment"> * Project: gmall2020-parent</span></span><br><span class="line"><span class="comment"> * Package: app</span></span><br><span class="line"><span class="comment"> * ClassName: DauApp01</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> 18729 created on date: 2020/12/8 11:55</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@version</span> 1.0</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span> JDK 1.8</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DauApp01</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String kafkaBrokers = <span class="string">&quot;hadoop01:9092&quot;</span>;</span><br><span class="line">        String zkBrokers = <span class="string">&quot;hadoop01:2181,hadoop02:2181,hadoop03:2181&quot;</span>;</span><br><span class="line">        String topic = <span class="string">&quot;GMALL_STARTUP_0105&quot;</span>;</span><br><span class="line">        String groupId = <span class="string">&quot;DAU_GROUP&quot;</span>;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;===============》 flink任务开始  ==============》&quot;</span>);</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">//设置kafka连接参数</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, kafkaBrokers);</span><br><span class="line">        properties.setProperty(<span class="string">&quot;zookeeper.connect&quot;</span>, zkBrokers);</span><br><span class="line">        properties.setProperty(<span class="string">&quot;group.id&quot;</span>, groupId);</span><br><span class="line">        <span class="comment">//设置时间类型</span></span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">        <span class="comment">//设置检查点时间间隔</span></span><br><span class="line">        env.enableCheckpointing(<span class="number">5000</span>);</span><br><span class="line">        env.setStateBackend( <span class="keyword">new</span> MemoryStateBackend());</span><br><span class="line">        <span class="comment">//创建kafak消费者，获取kafak中的数据</span></span><br><span class="line">        FlinkKafkaConsumer010&lt;String&gt; kafkaConsumer010 = <span class="keyword">new</span> FlinkKafkaConsumer010&lt;&gt;(topic, <span class="keyword">new</span> SimpleStringSchema(), properties);</span><br><span class="line">        DataStreamSource&lt;String&gt; kafkaData = env.addSource(kafkaConsumer010);</span><br><span class="line">        DataStream&lt;String&gt; userData = kafkaData.map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;&gt;&gt;&gt;&gt;&gt;&gt;接收topic报文:&quot;</span>+s+<span class="string">&quot;&lt;&lt;&lt;&lt;&lt;&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> s;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        List&lt;HttpHost&gt; httpHosts = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        httpHosts.add(<span class="keyword">new</span> HttpHost(<span class="string">&quot;hadoop01&quot;</span>, <span class="number">9200</span>, <span class="string">&quot;http&quot;</span>));</span><br><span class="line">        ElasticsearchSink.Builder&lt;String&gt; esSinkBuilder = <span class="keyword">new</span> ElasticsearchSink.Builder&lt;&gt;(</span><br><span class="line">                httpHosts,</span><br><span class="line">                <span class="keyword">new</span> ElasticsearchSinkFunction&lt;String&gt;() &#123;</span><br><span class="line">                    <span class="function"><span class="keyword">public</span> IndexRequest <span class="title">createIndexRequest</span><span class="params">(String element)</span> </span>&#123;</span><br><span class="line">                        Map&lt;String, Object&gt; json = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">                        JSONObject jsonObject = JSON.parseObject(element);</span><br><span class="line"></span><br><span class="line">                        Long ts = jsonObject.getLong(<span class="string">&quot;ts&quot;</span>);</span><br><span class="line">                        String format = <span class="keyword">new</span> SimpleDateFormat(<span class="string">&quot;yyyy-MM-dd HH&quot;</span>).format(<span class="keyword">new</span> Date(ts));</span><br><span class="line">                        String[] s = format.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">                        jsonObject.put(<span class="string">&quot;dt&quot;</span>,s[<span class="number">0</span>]);</span><br><span class="line">                        jsonObject.put(<span class="string">&quot;hr&quot;</span>,s[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">                        String common = jsonObject.getString(<span class="string">&quot;common&quot;</span>);</span><br><span class="line">                        JSONObject jsonObject1 = JSON.parseObject(common);</span><br><span class="line"></span><br><span class="line">                        json.put(<span class="string">&quot;mid&quot;</span>,jsonObject1.getString(<span class="string">&quot;mid&quot;</span>));</span><br><span class="line">                        json.put(<span class="string">&quot;uid&quot;</span>,jsonObject1.getString(<span class="string">&quot;uid&quot;</span>));</span><br><span class="line">                        json.put(<span class="string">&quot;ar&quot;</span>,jsonObject1.getString(<span class="string">&quot;ar&quot;</span>));</span><br><span class="line">                        json.put(<span class="string">&quot;ch&quot;</span>,jsonObject1.getString(<span class="string">&quot;ch&quot;</span>));</span><br><span class="line">                        json.put(<span class="string">&quot;vc&quot;</span>,jsonObject1.getString(<span class="string">&quot;vc&quot;</span>));</span><br><span class="line">                        json.put(<span class="string">&quot;dt&quot;</span>,jsonObject.getString(<span class="string">&quot;dt&quot;</span>));</span><br><span class="line">                        json.put(<span class="string">&quot;hr&quot;</span>,jsonObject.getString(<span class="string">&quot;hr&quot;</span>));</span><br><span class="line">                        json.put(<span class="string">&quot;mi&quot;</span>,<span class="string">&quot;00&quot;</span>);</span><br><span class="line">                        json.put(<span class="string">&quot;ts&quot;</span>,jsonObject.getLong(<span class="string">&quot;ts&quot;</span>));</span><br><span class="line">                        System.out.println(<span class="string">&quot;data:&quot;</span>+element);</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">return</span> Requests.indexRequest()</span><br><span class="line">                                .index(<span class="string">&quot;gmall_dau_info_&quot;</span> + jsonObject.getString(<span class="string">&quot;dt&quot;</span>))</span><br><span class="line">                                .type(<span class="string">&quot;_doc&quot;</span>)</span><br><span class="line">                                .id(jsonObject1.getString(<span class="string">&quot;mid&quot;</span>))</span><br><span class="line">                                .source(json);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String element, RuntimeContext ctx, RequestIndexer indexer)</span> </span>&#123;</span><br><span class="line">                        indexer.add(createIndexRequest(element));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        esSinkBuilder.setBulkFlushMaxActions(<span class="number">1</span>);</span><br><span class="line"><span class="comment">//        esSinkBuilder.setRestClientFactory(</span></span><br><span class="line"><span class="comment">//                restClientBuilder -&gt; &#123;</span></span><br><span class="line"><span class="comment">//                    restClientBuilder.setDefaultHeaders()</span></span><br><span class="line"><span class="comment">//                &#125;</span></span><br><span class="line"><span class="comment">//        );</span></span><br><span class="line">        esSinkBuilder.setRestClientFactory(<span class="keyword">new</span> util.RestClientFactoryImpl());</span><br><span class="line">        esSinkBuilder.setFailureHandler(<span class="keyword">new</span> RetryRejectedExecutionFailureHandler());</span><br><span class="line"></span><br><span class="line">        userData.addSink(esSinkBuilder.build());</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">&quot;flink-task&quot;</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="测试："><a href="#测试：" class="headerlink" title="测试："></a>测试：</h4><p>将sparkstreaming任务宕机，然后在打开，从redis读取检查点位置继续消费：</p>
<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/redis1.jpg"></p>
<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/redis2.jpg"></p>
<p>将flink任务宕机，然后在打开，能够继续从检查点消费：</p>
<p><img src="/2020/12/08/%E6%97%A5%E6%B4%BB%E9%9C%80%E6%B1%82(kafka%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9)-%E5%88%86%E5%88%AB%E7%94%A8sparkstreaming%E5%92%8Cflink%E5%AE%9E%E7%8E%B0/%E6%A3%80%E6%9F%A5%E7%82%B9.jpg"></p>
<p>通过对比可以发现，使用flink代码比使用sparkstreaming简洁很多，原因在于flink内部有保存状态的状态后端，同时sparkstreaming基于微批次处理，flink基于流式处理在数据处理速度上页更加流畅。</p>
]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title>elasticsearch原理及常见面试点</title>
    <url>/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/</url>
    <content><![CDATA[<p><img src="https://images.contentstack.io/v3/assets/bltefdd0b53724fa2ce/blt280217a63b82a734/5bbdaacf63ed239936a7dd56/elastic-logo.svg" alt="Elastic logo"></p>
<a id="more"></a>

<h4 id="elasticsearch是基于Lucene-倒排索引结构的一款OLAP数据分析引擎，特点是快速，并且支持模糊查询。"><a href="#elasticsearch是基于Lucene-倒排索引结构的一款OLAP数据分析引擎，特点是快速，并且支持模糊查询。" class="headerlink" title="elasticsearch是基于Lucene 倒排索引结构的一款OLAP数据分析引擎，特点是快速，并且支持模糊查询。"></a>elasticsearch是基于Lucene 倒排索引结构的一款OLAP数据分析引擎，特点是快速，并且支持模糊查询。</h4><p>模糊查询基于倒排索引：</p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/15.jpg"></p>
<p>倒排索引原理：</p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/16.jpg"></p>
<h4 id="图解-ElasticSearch-原理"><a href="#图解-ElasticSearch-原理" class="headerlink" title="图解 ElasticSearch 原理"></a>图解 ElasticSearch 原理</h4><p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/0.jpg"></p>
<ul>
<li>集群cluster中包含很多nodes</li>
<li>一个或多个node中的shard组成index索引=lucene index</li>
<li>lucene index由许多的segments组成</li>
<li>segment内部结构由多个部分组成，最最重要的 <strong>Inverted Index</strong></li>
</ul>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/1.png"></p>
<p>Inverted Index 主要包括两部分：</p>
<ul>
<li>一个有序的数据字典 Dictionary（包括单词 Term 和它出现的频率）</li>
<li>与单词 Term 对应的 Postings（即存在这个单词的文件）</li>
</ul>
<p>当我们搜索的时候，首先将搜索的内容分解，然后在字典里找到对应 Term，从而查找到与搜索相关的文件内容。</p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/2.jpg"></p>
<p>在 Shard 中搜索</p>
<p>ElasticSearch 从 Shard 中搜索的过程与 Lucene Segment 中搜索的过程类似。</p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/1.jpg"></p>
<p>需要注意的是：1 次搜索查找 2 个 Shard＝2 次分别搜索 Shard</p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/3.jpg"></p>
<p>Shard 不会进行更进一步的拆分，但是 Shard 可能会被转移到不同节点上。</p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/4.jpg"></p>
<p>如果当集群节点压力增长到一定的程度，我们可能会考虑增加新的节点，这就会要求我们对所有数据进行重新索</p>
<p>引，这是我们不太希望看到的。所以我们需要在规划的时候就考虑清楚，如何去平衡足够多的节点与不足节点之间</p>
<p>的关系。</p>
<hr>
<p><strong>一个真实的请求</strong></p>
<p>请求分发：这个请求可能被分发到集群里的任意一个节点，如下图：<br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/5.jpg"><br>上帝节点：</p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/6.png"><br>这时这个节点就成为当前请求的协调者（Coordinator），它决定：<br>根据索引信息，判断请求会被路由到哪个核心节点。<br>以及哪个副本是可用的。<br>等等。</p>
<p>路由：<br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/7.png"><br>在真实搜索之前ElasticSearch 会将 Query 转换成 Lucene Query，如下图：<br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/8.jpg"><br>然后在所有的 Segment 中执行计算，如下图：<br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/9.jpg"><br>对于 Filter 条件本身也会有缓存，如下图：<br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/10.jpg"><br>但 Queries 不会被缓存，所以如果相同的 Query 重复执行，应用程序自己需要做缓存。<br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/17.jpg"><br>所以Filters 可以在任何时候使用。Query 只有在需要 Score 的时候才使用。</p>
<p>搜索结束之后，结果会沿着下行的路径向上逐层返回，如下图：</p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/18.jpg"></p>
<p><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/11.jpg"><br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/12.jpg"><br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/13.jpg"><br><img src="/2021/02/23/elasticsearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9/14.jpg"></p>
<h3 id="常见面试问题："><a href="#常见面试问题：" class="headerlink" title="常见面试问题："></a>常见面试问题：</h3><p>kafka-flink-es过程中，flink自定义sink可以使用基于哪两种方式发送？</p>
<p>1、http方式</p>
<p>2、jdbc方式</p>
<hr>
<p>字段全值匹配检索如果在关系型数据库Mysql中查询多字段匹配数据（字段检索）一般会执行下面的SQL语句：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> atguigu <span class="keyword">where</span> <span class="keyword">name</span> = <span class="string">&#x27;haha&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>但在Elasticsearch中需要采用特殊的方式：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET atguigu/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;bool&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;filter&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;term&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;about&quot;</span>: <span class="string">&quot;I love to go rock climbing&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>字段分词匹配检索</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET atguigu/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;match&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;about&quot;</span>: <span class="string">&quot;I&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>字段模糊匹配检索如果在关系型数据库Mysql中模糊查询多字段数据一般会执行下面的SQL语句：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> <span class="keyword">name</span> <span class="keyword">like</span> <span class="string">&#x27;%haha%&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>但在Elasticsearch中需要采用特殊的方式，查询出所有文档字段值分词后包含haha的文档：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">GET  test/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;fuzzy&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;aa&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;value&quot;</span>: <span class="string">&quot;我是程序&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
  </entry>
</search>
